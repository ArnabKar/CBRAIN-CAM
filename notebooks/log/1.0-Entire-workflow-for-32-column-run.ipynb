{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire workflow for 32 column run \n",
    "\n",
    "In this notebook, I will try to document every step currently required from the raw SP training data to a neural network online run with analysis. The hope is 1) to provide an introduction for people new to this code and (more importantly) 2) show which bottlenecks there currently are in the workflow and where things could/should be improved.\n",
    "\n",
    "Unfortunately, the code and the repository are quite messy at times. Sorry about that.\n",
    "\n",
    "We will use the 32 column SPCAM training run, which has previously crashed, which gives us a good opportunity to do some analysis for a crashed run.\n",
    "\n",
    "I will start working on my local machine in Munich, where I copied the raw data to. There I will do data preprocessing and network training. Then we will switch to Stampede2 to compile CAM, run the neural network version and analyze the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here are the commands necessary for the entire workflow.\n",
    "\n",
    "**Preprocessing**\n",
    "\n",
    "Create config file (e.g. `32col_engy_ess_ref.yml`):\n",
    "```\n",
    "inputs : [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]\n",
    "outputs : [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\n",
    "in_dir : /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/\n",
    "out_dir: /local/S.Rasp/preprocessed_data/\n",
    "```\n",
    "\n",
    "Run preprocessing scripts:\n",
    "```\n",
    "python ../../cbrain/preprocess_aqua.py \\\n",
    "--config ../../pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-0[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_train\n",
    "\n",
    "python ../../cbrain/preprocess_aqua.py \\\n",
    "--config ../../pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-1[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_valid --ext_norm Nope\n",
    "\n",
    "python ../../cbrain/shuffle_ds.py --pref /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train\n",
    "```\n",
    "\n",
    "**Neural network training**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: from raw data to neural network input\n",
    "\n",
    "The goal of the preprocessing is to convert the raw SPCAM output data to a format which can easily and quickly be used during neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The raw data\n",
    "\n",
    "The raw data is saved in daily files, each approximately 900MB large (h1 files contain the necessary data). For this 32 column version, there is 1 year and 3 months of data. For storage information of raw files, check https://github.com/raspstephan/CBRAIN-CAM/wiki/Raw-SPCAM-runs-(to-be-used-for-network-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAW_DATA = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/'\n",
    "RAW_DATA = '/local/S.Rasp/sp32fbp_andkua/'   #SSD drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! executes a command line script from within the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-01-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-02-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-03-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-04-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-05-00000.nc\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls -hl $RAW_DATA*h1* | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(RAW_DATA + 'AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-01-00000.nc', \n",
    "                     decode_times=False)   # This is necessary because of the way time is stored in SPCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (crm_x: 32, crm_y: 1, crm_z: 28, ilev: 31, isccp_prs: 7, isccp_prstau: 49, isccp_tau: 7, lat: 64, lev: 30, lon: 128, tbnd: 2, time: 48)\n",
       "Coordinates:\n",
       "  * lat           (lat) float64 -87.86 -85.1 -82.31 -79.53 -76.74 -73.95 ...\n",
       "  * lon           (lon) float64 0.0 2.812 5.625 8.438 11.25 14.06 16.88 ...\n",
       "  * crm_x         (crm_x) float64 0.0 4.0 8.0 12.0 16.0 20.0 24.0 28.0 32.0 ...\n",
       "  * crm_y         (crm_y) float64 0.0\n",
       "  * crm_z         (crm_z) float64 992.6 976.3 957.5 936.2 912.6 887.0 859.5 ...\n",
       "  * lev           (lev) float64 3.643 7.595 14.36 24.61 38.27 54.6 72.01 ...\n",
       "  * ilev          (ilev) float64 2.255 5.032 10.16 18.56 30.67 45.87 63.32 ...\n",
       "  * isccp_prs     (isccp_prs) float64 90.0 245.0 375.0 500.0 620.0 740.0 900.0\n",
       "  * isccp_tau     (isccp_tau) float64 0.15 0.8 2.45 6.5 16.2 41.5 219.5\n",
       "  * isccp_prstau  (isccp_prstau) float64 90.0 90.0 90.0 90.01 90.02 90.04 ...\n",
       "  * time          (time) float64 0.0 0.02083 0.04167 0.0625 0.08333 0.1042 ...\n",
       "Dimensions without coordinates: tbnd\n",
       "Data variables:\n",
       "    P0            float64 ...\n",
       "    time_bnds     (time, tbnd) float64 ...\n",
       "    date_written  (time) |S8 ...\n",
       "    time_written  (time) |S8 ...\n",
       "    ntrm          int32 ...\n",
       "    ntrn          int32 ...\n",
       "    ntrk          int32 ...\n",
       "    ndbase        int32 ...\n",
       "    nsbase        int32 ...\n",
       "    nbdate        int32 ...\n",
       "    nbsec         int32 ...\n",
       "    mdt           int32 ...\n",
       "    nlon          (lat) int32 ...\n",
       "    wnummax       (lat) int32 ...\n",
       "    hyai          (ilev) float64 ...\n",
       "    hybi          (ilev) float64 ...\n",
       "    hyam          (lev) float64 ...\n",
       "    hybm          (lev) float64 ...\n",
       "    gw            (lat) float64 ...\n",
       "    ndcur         (time) int32 ...\n",
       "    nscur         (time) int32 ...\n",
       "    date          (time) int32 ...\n",
       "    datesec       (time) int32 ...\n",
       "    nsteph        (time) int32 ...\n",
       "    DTV           (time, lev, lat, lon) float32 ...\n",
       "    DTVKE         (time, lev, lat, lon) float32 ...\n",
       "    FLNS          (time, lat, lon) float32 ...\n",
       "    FLNT          (time, lat, lon) float32 ...\n",
       "    FLUT          (time, lat, lon) float32 ...\n",
       "    FSNS          (time, lat, lon) float32 ...\n",
       "    FSNT          (time, lat, lon) float32 ...\n",
       "    LHFLX         (time, lat, lon) float32 ...\n",
       "    PHCLDICE      (time, lev, lat, lon) float32 ...\n",
       "    PHCLDLIQ      (time, lev, lat, lon) float32 ...\n",
       "    PHQ           (time, lev, lat, lon) float32 ...\n",
       "    PRECC         (time, lat, lon) float32 ...\n",
       "    PRECL         (time, lat, lon) float32 ...\n",
       "    PRECSC        (time, lat, lon) float32 ...\n",
       "    PRECSL        (time, lat, lon) float32 ...\n",
       "    PRECSTEN      (time, lat, lon) float32 ...\n",
       "    PRECT         (time, lat, lon) float32 ...\n",
       "    PRECTEND      (time, lat, lon) float32 ...\n",
       "    PS            (time, lat, lon) float32 ...\n",
       "    QAP           (time, lev, lat, lon) float32 ...\n",
       "    QCAP          (time, lev, lat, lon) float32 ...\n",
       "    QIAP          (time, lev, lat, lon) float32 ...\n",
       "    QRL           (time, lev, lat, lon) float32 ...\n",
       "    QRS           (time, lev, lat, lon) float32 ...\n",
       "    SHFLX         (time, lat, lon) float32 ...\n",
       "    SOLIN         (time, lat, lon) float32 ...\n",
       "    SPDQ          (time, lev, lat, lon) float32 ...\n",
       "    SPDT          (time, lev, lat, lon) float32 ...\n",
       "    T             (time, lev, lat, lon) float32 ...\n",
       "    TAP           (time, lev, lat, lon) float32 ...\n",
       "    TPHYSTND      (time, lev, lat, lon) float32 ...\n",
       "    TS            (time, lat, lon) float32 ...\n",
       "    UAP           (time, lev, lat, lon) float32 ...\n",
       "    VAP           (time, lev, lat, lon) float32 ...\n",
       "    VD01          (time, lev, lat, lon) float32 ...\n",
       "    VPHYSTND      (time, lev, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:  CF-1.0\n",
       "    source:       CAM\n",
       "    case:         AndKua_aqua_SPCAM3.0_sp_fbp32\n",
       "    title:        \n",
       "    logname:      tg847872\n",
       "    host:         \n",
       "    Version:      $Name:  $\n",
       "    revision_Id:  $Id: history.F90,v 1.26.2.38 2003/12/15 18:52:35 hender Exp $"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing script\n",
    "\n",
    "The preprocessing is currently handled by a script called `preprocess_aqua.py` in the `cbrain` directory of the repository. \n",
    "\n",
    "To train the neural network we look at the data column by column. If we want to e.g. use TAP and QAP for the input of the neural network, in the raw data these two variables would be saved at different locations of the disc, which makes reading slow. In order to quickly read the data during neural network training, this script concatenates the input and output vector and saves these vectors to disc. \n",
    "\n",
    "A second step done by the script is to compute variables, that are not directly available in the raw output. For example, `TBP` = `TAP - TPHYSTND * dt`. The exact meaning of the variables for SPCAM can be found here (...eventually): https://github.com/raspstephan/CBRAIN-CAM/wiki/What-exactly-happens-in-a-SPCAM-time-step%3F\n",
    "\n",
    "The preprocessing script takes a number of arguments from the command line or from a configuration script. Let's write a configuration script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocess_aqua.py [-h] [--config_file CONFIG_FILE]\r\n",
      "                          [--inputs INPUTS [INPUTS ...]]\r\n",
      "                          [--outputs OUTPUTS [OUTPUTS ...]]\r\n",
      "                          [--in_dir IN_DIR [IN_DIR ...]] [--out_dir OUT_DIR]\r\n",
      "                          [--aqua_names AQUA_NAMES [AQUA_NAMES ...]]\r\n",
      "                          [--out_pref OUT_PREF] [--chunk_size CHUNK_SIZE]\r\n",
      "                          [--ext_norm EXT_NORM] [--min_lev MIN_LEV]\r\n",
      "                          [--lat_range LAT_RANGE [LAT_RANGE ...]]\r\n",
      "                          [--target_factor TARGET_FACTOR]\r\n",
      "                          [--random_seed RANDOM_SEED] [--shuffle]\r\n",
      "                          [--only_norm] [--flx_same_dt]\r\n",
      "                          [--norm_features NORM_FEATURES]\r\n",
      "                          [--norm_targets NORM_TARGETS] [--verbose]\r\n",
      "\r\n",
      "Args that start with '--' (eg. --inputs) can also be set in a config file\r\n",
      "(specified via --config_file). Config file syntax allows: key=value,\r\n",
      "flag=true, stuff=[a,b,c] (for details, see syntax at https://goo.gl/R74nmi).\r\n",
      "If an arg is specified in more than one place, then commandline values\r\n",
      "override config file values which override defaults.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --config_file CONFIG_FILE\r\n",
      "                        Name of config file in this directory. Must contain\r\n",
      "                        feature and target variable lists.\r\n",
      "  --inputs INPUTS [INPUTS ...]\r\n",
      "                        Feature variables\r\n",
      "  --outputs OUTPUTS [OUTPUTS ...]\r\n",
      "                        Target variables\r\n",
      "  --in_dir IN_DIR [IN_DIR ...]\r\n",
      "                        Directory with input (aqua) files.\r\n",
      "  --out_dir OUT_DIR     Directory to write preprocessed file.\r\n",
      "  --aqua_names AQUA_NAMES [AQUA_NAMES ...]\r\n",
      "                        String with filenames to be processed.\r\n",
      "  --out_pref OUT_PREF   Prefix for all file names\r\n",
      "  --chunk_size CHUNK_SIZE\r\n",
      "                        size of chunks\r\n",
      "  --ext_norm EXT_NORM   Name of external normalization file\r\n",
      "  --min_lev MIN_LEV     Minimum level index. Default = 0\r\n",
      "  --lat_range LAT_RANGE [LAT_RANGE ...]\r\n",
      "                        Latitude range. Default = [-90, 90]\r\n",
      "  --target_factor TARGET_FACTOR\r\n",
      "                        Factor to multiply targets with. For TF comparison set\r\n",
      "                        to 1e-3. Default = 1.\r\n",
      "  --random_seed RANDOM_SEED\r\n",
      "                        Random seed for shuffling of data.\r\n",
      "  --shuffle             If given, shuffle data along sample dimension.\r\n",
      "  --only_norm           If given, only compute and save normalization file.\r\n",
      "  --flx_same_dt         If given, take surface fluxes from same time step.\r\n",
      "  --norm_features NORM_FEATURES\r\n",
      "                        by_var or by_lev\r\n",
      "  --norm_targets NORM_TARGETS\r\n",
      "                        norm or scale\r\n",
      "  --verbose             If given, print debugging information.\r\n"
     ]
    }
   ],
   "source": [
    "!python ../../cbrain/preprocess_aqua.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs : [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]\r\n",
      "outputs : [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\r\n",
      "in_dir : /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/\r\n",
      "out_dir: /local/S.Rasp/preprocessed_data/"
     ]
    }
   ],
   "source": [
    "!cat ../../pp_config/32col_engy_ess_ref.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROC_DIR = '/local/S.Rasp/preprocessed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important arguments are:\n",
    "- inputs: List of input variables, e.g. [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]. `_BP` variables are computed as: `TAP - TPHYSTND*dt`\n",
    "- outputs: List of output variables, e.g. [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\n",
    "- in_dir: Directory where SPCAM files are stored. I strongly recommend putting the files on a fast drive (e.g. /scratch/ on Greenplanet)\n",
    "- aqua_names: Here you can put a string with placeholders `*`, e.g. `'*.h1.0001-*-*-*'` for all files from year 1 or `'*.h1.0000-*-1[7-9]-*'` for day 17-19 from each month of year 0.\n",
    "- out_dir: directory where the processed files will be stored\n",
    "- out_pref: Prefix for the output files. Give them some descriptive name, e.g. `my_experiment1`\n",
    "- ext_norm: This is deprecated and ugly. If ext_norm = None (default), a normalization file will be computed from the data. This normalization file contains means, standard deviations, etc. For a full year of data this can take a long time. For this reason I chose to compute the normalization files for a sample of the data. The differences are small. If ext_norm is some string (just pick any jiberish) no normalization file will be computed but the data will still not be normalized. This is controlled by the `norm_features` and `norm_targets` options which are None and should stay so. The normalization happens later on the fly during network training.\n",
    "\n",
    "Let's start by creating a training dataset using the first three days from each month of the first year of available data. This is a total of 36 days, so roughly a tenth of the entire year. I added timestamps to the script, so that we can see how much time each step in the preprocessing takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checkpoint reading data: 13.15 s\n",
      "Number of time steps: 1728\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Time checkpoint create datasets: 13.28 s\n",
      "Time checkpoint reshape and rechunk: 13.75 s\n",
      "Compute means and stds\n",
      "Saving normalization file: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_norm.nc\n",
      "Time checkpoint normalization arrays: 214.54 s\n",
      "Time checkpoint rechunk and ds: 215.20 s\n",
      "Save features: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_features.nc\n",
      "Save targets: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_targets.nc\n",
      "Total time: 292.96 s\n"
     ]
    }
   ],
   "source": [
    "!python ../../cbrain/preprocess_aqua.py \\\n",
    "--config ../../pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-0[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the normalization file actually takes longest. This involves computing statistics over the entire sample dimension. But it's not crucial to compute the exactly right statistics for normalization. E.g. when using the entire year as training data, the normalization file computed with a subset of the data is totally fine to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_features.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_norm.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROC_DIR/32_col*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a features and a target dataset plus a normalization dataset. Let's check all of them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (feature_lev: 94, sample: 14057472)\n",
       "Coordinates:\n",
       "  * feature_lev    (feature_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...\n",
       "    time           (sample) int64 ...\n",
       "    lat            (sample) float64 ...\n",
       "    lon            (sample) float64 ...\n",
       "    feature_names  (feature_lev) object ...\n",
       "Dimensions without coordinates: sample\n",
       "Data variables:\n",
       "    features       (sample, feature_lev) float32 ...\n",
       "Attributes:\n",
       "    log:      \\n    Time: 2019-01-08T12:45:18\\n\\n    Executed command:\\n\\n   ..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_features.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (sample: 14057472, target_lev: 65)\n",
       "Coordinates:\n",
       "  * target_lev    (target_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...\n",
       "    time          (sample) int64 ...\n",
       "    lat           (sample) float64 ...\n",
       "    lon           (sample) float64 ...\n",
       "Dimensions without coordinates: sample\n",
       "Data variables:\n",
       "    targets       (sample, target_lev) float32 ...\n",
       "    target_names  (target_lev) object ...\n",
       "Attributes:\n",
       "    log:      \\n    Time: 2019-01-08T12:45:18\\n\\n    Executed command:\\n\\n   ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two dimensions, `sample` and `target_lev`, which corresponds to the stacked vectors. Additionally we have the variable `target_names`, which is handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'target_names' (target_lev: 65)>\n",
       "array(['TPHYSTND_lev00', 'TPHYSTND_lev01', 'TPHYSTND_lev02', 'TPHYSTND_lev03',\n",
       "       'TPHYSTND_lev04', 'TPHYSTND_lev05', 'TPHYSTND_lev06', 'TPHYSTND_lev07',\n",
       "       'TPHYSTND_lev08', 'TPHYSTND_lev09', 'TPHYSTND_lev10', 'TPHYSTND_lev11',\n",
       "       'TPHYSTND_lev12', 'TPHYSTND_lev13', 'TPHYSTND_lev14', 'TPHYSTND_lev15',\n",
       "       'TPHYSTND_lev16', 'TPHYSTND_lev17', 'TPHYSTND_lev18', 'TPHYSTND_lev19',\n",
       "       'TPHYSTND_lev20', 'TPHYSTND_lev21', 'TPHYSTND_lev22', 'TPHYSTND_lev23',\n",
       "       'TPHYSTND_lev24', 'TPHYSTND_lev25', 'TPHYSTND_lev26', 'TPHYSTND_lev27',\n",
       "       'TPHYSTND_lev28', 'TPHYSTND_lev29', 'PHQ_lev00', 'PHQ_lev01',\n",
       "       'PHQ_lev02', 'PHQ_lev03', 'PHQ_lev04', 'PHQ_lev05', 'PHQ_lev06',\n",
       "       'PHQ_lev07', 'PHQ_lev08', 'PHQ_lev09', 'PHQ_lev10', 'PHQ_lev11',\n",
       "       'PHQ_lev12', 'PHQ_lev13', 'PHQ_lev14', 'PHQ_lev15', 'PHQ_lev16',\n",
       "       'PHQ_lev17', 'PHQ_lev18', 'PHQ_lev19', 'PHQ_lev20', 'PHQ_lev21',\n",
       "       'PHQ_lev22', 'PHQ_lev23', 'PHQ_lev24', 'PHQ_lev25', 'PHQ_lev26',\n",
       "       'PHQ_lev27', 'PHQ_lev28', 'PHQ_lev29', 'FSNT', 'FSNS', 'FLNT', 'FLNS',\n",
       "       'PRECT'], dtype=object)\n",
       "Coordinates:\n",
       "  * target_lev  (target_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc').target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:              (feature_lev: 94, target_lev: 65)\n",
       "Coordinates:\n",
       "  * feature_lev          (feature_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 ...\n",
       "  * target_lev           (target_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 ...\n",
       "Data variables:\n",
       "    feature_means        (feature_lev) float32 ...\n",
       "    feature_stds         (feature_lev) float32 ...\n",
       "    feature_mins         (feature_lev) float32 ...\n",
       "    feature_maxs         (feature_lev) float32 ...\n",
       "    target_means         (target_lev) float32 ...\n",
       "    target_stds          (target_lev) float32 ...\n",
       "    target_mins          (target_lev) float32 ...\n",
       "    target_maxs          (target_lev) float32 ...\n",
       "    feature_names        (feature_lev) object ...\n",
       "    target_names         (target_lev) object ...\n",
       "    feature_stds_by_var  (feature_lev) float32 ...\n",
       "    target_conv          (target_lev) float32 ...\n",
       "Attributes:\n",
       "    log:      \\n    Time: 2019-01-08T12:45:18\\n\\n    Executed command:\\n\\n   ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_norm.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization file contains a number of statistics for the features and targets, which can then be used during training for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to have a separate validation dataset. For this we simply chose a different date range. For the validation dataset we do not need to compute a normalization file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checkpoint reading data: 1.19 s\n",
      "Number of time steps: 1728\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Time checkpoint create datasets: 1.30 s\n",
      "Time checkpoint reshape and rechunk: 1.77 s\n",
      "Load external normalization file\n",
      "Time checkpoint normalization arrays: 1.77 s\n",
      "Time checkpoint rechunk and ds: 2.39 s\n",
      "Save features: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_valid_features.nc\n",
      "Save targets: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_valid_targets.nc\n",
      "Total time: 148.53 s\n"
     ]
    }
   ],
   "source": [
    "!python ../../cbrain/preprocess_aqua.py \\\n",
    "--config ../../pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-1[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_valid --ext_norm Nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_features.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_norm.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_valid_features.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_valid_targets.nc\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROC_DIR/32_col*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed comparison: regular hard-drive vs. SSD\n",
    "\n",
    "I copy all the raw data to an SSD drive, in order to speed up the computation. We can load the data from a regular disc (but still save on SSD), to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checkpoint reading data: 40.51 s\n",
      "Number of time steps: 1728\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Time checkpoint create datasets: 40.64 s\n",
      "Time checkpoint reshape and rechunk: 42.67 s\n",
      "Compute means and stds\n",
      "Saving normalization file: /local/S.Rasp/preprocessed_data/test_32_col_engy_ess_3d_train_norm.nc\n",
      "Time checkpoint normalization arrays: 1352.22 s\n",
      "Time checkpoint rechunk and ds: 1352.89 s\n",
      "Save features: /local/S.Rasp/preprocessed_data/test_32_col_engy_ess_3d_train_features.nc\n",
      "Save targets: /local/S.Rasp/preprocessed_data/test_32_col_engy_ess_3d_train_targets.nc\n",
      "Total time: 2062.98 s\n"
     ]
    }
   ],
   "source": [
    "!python ../../cbrain/preprocess_aqua.py \\\n",
    "--config ../../pp_config/32col_engy_ess_ref.yml \\\n",
    "--in_dir /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/ --aqua_names '*.h1.0000-*-0[1-3]-*' \\\n",
    "--out_pref test_32_col_engy_ess_3d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to process 36 days:\n",
    "\n",
    "SSD: 5 minutes    \n",
    "Hard-drive: 36 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Entire year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the training dataset\n",
    "\n",
    "During network training it's important that the samples in each batch are random. Currently they are still in order (lat-lon-time only have been flattened). Ideally, you would randomly shuffle during training. Because the datasets exceed RAM however, shuffling means reading random locations on disc for each sample in the batch. This is slow.\n",
    "\n",
    "For this reason, I am pre-shuffling the training dataset once. Then during training, for each batch we will read a continuous slice of the shuffled dataset. Only the order of the batches is random. This should be a good enough approximation. \n",
    "\n",
    "Shuffling is done by the `shuffle_ds.py` script in the `cbrain` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: shuffle_ds.py [-h] [--method METHOD] [--pref PREF]\r\n",
      "                     [--random_seed RANDOM_SEED] [--chunk_size CHUNK_SIZE]\r\n",
      "                     [--verbose VERBOSE]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --method METHOD       [Meticulous or fast]\r\n",
      "  --pref PREF           Prefix. ie without the _features.nc\r\n",
      "  --random_seed RANDOM_SEED\r\n",
      "                        Random seed for shuffling of data.\r\n",
      "  --chunk_size CHUNK_SIZE\r\n",
      "                        Size of chunks for fast method\r\n",
      "  --verbose VERBOSE     Verbosity level\r\n"
     ]
    }
   ],
   "source": [
    "!python ../../cbrain/shuffle_ds.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_features.nc /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_targets.nc\n",
      "Creating files: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_shuffle_features.nc /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_shuffle_targets.nc\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:51<00:00, 25.74s/it]\n"
     ]
    }
   ],
   "source": [
    "!python ../../cbrain/shuffle_ds.py --pref /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Entire year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
