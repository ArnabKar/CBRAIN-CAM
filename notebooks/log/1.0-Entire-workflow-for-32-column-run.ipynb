{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire workflow for 32 column run \n",
    "\n",
    "In this notebook, I will try to document every step currently required from the raw SP training data to a neural network online run with analysis. The hope is 1) to provide an introduction for people new to this code and (more importantly) 2) show which bottlenecks there currently are in the workflow and where things could/should be improved.\n",
    "\n",
    "Unfortunately, the code and the repository are quite messy at times. Sorry about that.\n",
    "\n",
    "We will use the 32 column SPCAM training run, which has previously crashed, which gives us a good opportunity to do some analysis for a crashed run.\n",
    "\n",
    "I will start working on my local machine in Munich, where I copied the raw data to. There I will do data preprocessing and network training. Then we will switch to Stampede2 to compile CAM, run the neural network version and analyze the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s/S.Rasp/repositories/CBRAIN-CAM/notebooks/log\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/s/S.Rasp/repositories/CBRAIN-CAM'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here are the commands necessary for the entire workflow.\n",
    "\n",
    "**Preprocessing**\n",
    "\n",
    "Create config file (e.g. `32col_engy_ess_ref.yml`):\n",
    "```\n",
    "inputs : [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]\n",
    "outputs : [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\n",
    "in_dir : /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/\n",
    "out_dir: /local/S.Rasp/preprocessed_data/\n",
    "```\n",
    "\n",
    "Run preprocessing scripts:\n",
    "```\n",
    "python cbrain/preprocess_aqua.py \\\n",
    "--config pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-0[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_train\n",
    "\n",
    "python cbrain/preprocess_aqua.py \\\n",
    "--config pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-1[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_valid --ext_norm Nope\n",
    "\n",
    "python cbrain/shuffle_ds.py --pref /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train\n",
    "```\n",
    "\n",
    "**Neural network training**\n",
    "\n",
    "Create config file (e.g. `001_32cols_engy_ess_3d_pnas.yml`):\n",
    "```\n",
    "exp_name: 001_32cols_engy_ess_3d_pnas\n",
    "data_dir: /local/S.Rasp/preprocessed_data/\n",
    "train_fn: 32_col_engy_ess_3d_train_shuffle\n",
    "valid_fn: 32_col_engy_ess_3d_valid\n",
    "norm_fn: 32_col_engy_ess_3d_train_norm.nc\n",
    "fsub: feature_means\n",
    "fdiv: max_rs\n",
    "tmult: target_conv\n",
    "activation: LeakyReLU\n",
    "hidden_layers: [256,256,256,256,256,256,256,256,256]\n",
    "loss: mse\n",
    "log_dir: ./logs/\n",
    "epochs: 18\n",
    "lr_step: 3\n",
    "```\n",
    "\n",
    "Run NN script\n",
    "```\n",
    "python run_experiment.py -c nn_config/32cols/001_32cols_engy_ess_3d_pnas.yml\n",
    "```\n",
    "\n",
    "Convert to text file\n",
    "```\n",
    "python save_weights.py -c nn_config/32cols/001_32cols_engy_ess_3d_pnas.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: from raw data to neural network input\n",
    "\n",
    "The goal of the preprocessing is to convert the raw SPCAM output data to a format which can easily and quickly be used during neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The raw data\n",
    "\n",
    "The raw data is saved in daily files, each approximately 900MB large (h1 files contain the necessary data). For this 32 column version, there is 1 year and 3 months of data. For storage information of raw files, check https://github.com/raspstephan/CBRAIN-CAM/wiki/Raw-SPCAM-runs-(to-be-used-for-network-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAW_DATA = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/'\n",
    "RAW_DATA = '/local/S.Rasp/sp32fbp_andkua/'   #SSD drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! executes a command line script from within the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-01-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-02-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-03-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-04-00000.nc\r\n",
      "-rwxr-xr-x 1 S.Rasp ls-craig 881M Apr  7  2018 /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-05-00000.nc\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls -hl $RAW_DATA*h1* | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(RAW_DATA + 'AndKua_aqua_SPCAM3.0_sp_fbp32.cam2.h1.0000-01-01-00000.nc', \n",
    "                     decode_times=False)   # This is necessary because of the way time is stored in SPCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (crm_x: 32, crm_y: 1, crm_z: 28, ilev: 31, isccp_prs: 7, isccp_prstau: 49, isccp_tau: 7, lat: 64, lev: 30, lon: 128, tbnd: 2, time: 48)\n",
       "Coordinates:\n",
       "  * lat           (lat) float64 -87.86 -85.1 -82.31 -79.53 -76.74 -73.95 ...\n",
       "  * lon           (lon) float64 0.0 2.812 5.625 8.438 11.25 14.06 16.88 ...\n",
       "  * crm_x         (crm_x) float64 0.0 4.0 8.0 12.0 16.0 20.0 24.0 28.0 32.0 ...\n",
       "  * crm_y         (crm_y) float64 0.0\n",
       "  * crm_z         (crm_z) float64 992.6 976.3 957.5 936.2 912.6 887.0 859.5 ...\n",
       "  * lev           (lev) float64 3.643 7.595 14.36 24.61 38.27 54.6 72.01 ...\n",
       "  * ilev          (ilev) float64 2.255 5.032 10.16 18.56 30.67 45.87 63.32 ...\n",
       "  * isccp_prs     (isccp_prs) float64 90.0 245.0 375.0 500.0 620.0 740.0 900.0\n",
       "  * isccp_tau     (isccp_tau) float64 0.15 0.8 2.45 6.5 16.2 41.5 219.5\n",
       "  * isccp_prstau  (isccp_prstau) float64 90.0 90.0 90.0 90.01 90.02 90.04 ...\n",
       "  * time          (time) float64 0.0 0.02083 0.04167 0.0625 0.08333 0.1042 ...\n",
       "Dimensions without coordinates: tbnd\n",
       "Data variables:\n",
       "    P0            float64 ...\n",
       "    time_bnds     (time, tbnd) float64 ...\n",
       "    date_written  (time) |S8 ...\n",
       "    time_written  (time) |S8 ...\n",
       "    ntrm          int32 ...\n",
       "    ntrn          int32 ...\n",
       "    ntrk          int32 ...\n",
       "    ndbase        int32 ...\n",
       "    nsbase        int32 ...\n",
       "    nbdate        int32 ...\n",
       "    nbsec         int32 ...\n",
       "    mdt           int32 ...\n",
       "    nlon          (lat) int32 ...\n",
       "    wnummax       (lat) int32 ...\n",
       "    hyai          (ilev) float64 ...\n",
       "    hybi          (ilev) float64 ...\n",
       "    hyam          (lev) float64 ...\n",
       "    hybm          (lev) float64 ...\n",
       "    gw            (lat) float64 ...\n",
       "    ndcur         (time) int32 ...\n",
       "    nscur         (time) int32 ...\n",
       "    date          (time) int32 ...\n",
       "    datesec       (time) int32 ...\n",
       "    nsteph        (time) int32 ...\n",
       "    DTV           (time, lev, lat, lon) float32 ...\n",
       "    DTVKE         (time, lev, lat, lon) float32 ...\n",
       "    FLNS          (time, lat, lon) float32 ...\n",
       "    FLNT          (time, lat, lon) float32 ...\n",
       "    FLUT          (time, lat, lon) float32 ...\n",
       "    FSNS          (time, lat, lon) float32 ...\n",
       "    FSNT          (time, lat, lon) float32 ...\n",
       "    LHFLX         (time, lat, lon) float32 ...\n",
       "    PHCLDICE      (time, lev, lat, lon) float32 ...\n",
       "    PHCLDLIQ      (time, lev, lat, lon) float32 ...\n",
       "    PHQ           (time, lev, lat, lon) float32 ...\n",
       "    PRECC         (time, lat, lon) float32 ...\n",
       "    PRECL         (time, lat, lon) float32 ...\n",
       "    PRECSC        (time, lat, lon) float32 ...\n",
       "    PRECSL        (time, lat, lon) float32 ...\n",
       "    PRECSTEN      (time, lat, lon) float32 ...\n",
       "    PRECT         (time, lat, lon) float32 ...\n",
       "    PRECTEND      (time, lat, lon) float32 ...\n",
       "    PS            (time, lat, lon) float32 ...\n",
       "    QAP           (time, lev, lat, lon) float32 ...\n",
       "    QCAP          (time, lev, lat, lon) float32 ...\n",
       "    QIAP          (time, lev, lat, lon) float32 ...\n",
       "    QRL           (time, lev, lat, lon) float32 ...\n",
       "    QRS           (time, lev, lat, lon) float32 ...\n",
       "    SHFLX         (time, lat, lon) float32 ...\n",
       "    SOLIN         (time, lat, lon) float32 ...\n",
       "    SPDQ          (time, lev, lat, lon) float32 ...\n",
       "    SPDT          (time, lev, lat, lon) float32 ...\n",
       "    T             (time, lev, lat, lon) float32 ...\n",
       "    TAP           (time, lev, lat, lon) float32 ...\n",
       "    TPHYSTND      (time, lev, lat, lon) float32 ...\n",
       "    TS            (time, lat, lon) float32 ...\n",
       "    UAP           (time, lev, lat, lon) float32 ...\n",
       "    VAP           (time, lev, lat, lon) float32 ...\n",
       "    VD01          (time, lev, lat, lon) float32 ...\n",
       "    VPHYSTND      (time, lev, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:  CF-1.0\n",
       "    source:       CAM\n",
       "    case:         AndKua_aqua_SPCAM3.0_sp_fbp32\n",
       "    title:        \n",
       "    logname:      tg847872\n",
       "    host:         \n",
       "    Version:      $Name:  $\n",
       "    revision_Id:  $Id: history.F90,v 1.26.2.38 2003/12/15 18:52:35 hender Exp $"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing script\n",
    "\n",
    "The preprocessing is currently handled by a script called `preprocess_aqua.py` in the `cbrain` directory of the repository. \n",
    "\n",
    "To train the neural network we look at the data column by column. If we want to e.g. use TAP and QAP for the input of the neural network, in the raw data these two variables would be saved at different locations of the disc, which makes reading slow. In order to quickly read the data during neural network training, this script concatenates the input and output vector and saves these vectors to disc. \n",
    "\n",
    "A second step done by the script is to compute variables, that are not directly available in the raw output. For example, `TBP` = `TAP - TPHYSTND * dt`. The exact meaning of the variables for SPCAM can be found here (...eventually): https://github.com/raspstephan/CBRAIN-CAM/wiki/What-exactly-happens-in-a-SPCAM-time-step%3F\n",
    "\n",
    "The preprocessing script takes a number of arguments from the command line or from a configuration script. Let's write a configuration script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocess_aqua.py [-h] [--config_file CONFIG_FILE]\r\n",
      "                          [--inputs INPUTS [INPUTS ...]]\r\n",
      "                          [--outputs OUTPUTS [OUTPUTS ...]]\r\n",
      "                          [--in_dir IN_DIR [IN_DIR ...]] [--out_dir OUT_DIR]\r\n",
      "                          [--aqua_names AQUA_NAMES [AQUA_NAMES ...]]\r\n",
      "                          [--out_pref OUT_PREF] [--chunk_size CHUNK_SIZE]\r\n",
      "                          [--ext_norm EXT_NORM] [--min_lev MIN_LEV]\r\n",
      "                          [--lat_range LAT_RANGE [LAT_RANGE ...]]\r\n",
      "                          [--target_factor TARGET_FACTOR]\r\n",
      "                          [--random_seed RANDOM_SEED] [--shuffle]\r\n",
      "                          [--only_norm] [--flx_same_dt]\r\n",
      "                          [--norm_features NORM_FEATURES]\r\n",
      "                          [--norm_targets NORM_TARGETS] [--verbose]\r\n",
      "\r\n",
      "Args that start with '--' (eg. --inputs) can also be set in a config file\r\n",
      "(specified via --config_file). Config file syntax allows: key=value,\r\n",
      "flag=true, stuff=[a,b,c] (for details, see syntax at https://goo.gl/R74nmi).\r\n",
      "If an arg is specified in more than one place, then commandline values\r\n",
      "override config file values which override defaults.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --config_file CONFIG_FILE\r\n",
      "                        Name of config file in this directory. Must contain\r\n",
      "                        feature and target variable lists.\r\n",
      "  --inputs INPUTS [INPUTS ...]\r\n",
      "                        Feature variables\r\n",
      "  --outputs OUTPUTS [OUTPUTS ...]\r\n",
      "                        Target variables\r\n",
      "  --in_dir IN_DIR [IN_DIR ...]\r\n",
      "                        Directory with input (aqua) files.\r\n",
      "  --out_dir OUT_DIR     Directory to write preprocessed file.\r\n",
      "  --aqua_names AQUA_NAMES [AQUA_NAMES ...]\r\n",
      "                        String with filenames to be processed.\r\n",
      "  --out_pref OUT_PREF   Prefix for all file names\r\n",
      "  --chunk_size CHUNK_SIZE\r\n",
      "                        size of chunks\r\n",
      "  --ext_norm EXT_NORM   Name of external normalization file\r\n",
      "  --min_lev MIN_LEV     Minimum level index. Default = 0\r\n",
      "  --lat_range LAT_RANGE [LAT_RANGE ...]\r\n",
      "                        Latitude range. Default = [-90, 90]\r\n",
      "  --target_factor TARGET_FACTOR\r\n",
      "                        Factor to multiply targets with. For TF comparison set\r\n",
      "                        to 1e-3. Default = 1.\r\n",
      "  --random_seed RANDOM_SEED\r\n",
      "                        Random seed for shuffling of data.\r\n",
      "  --shuffle             If given, shuffle data along sample dimension.\r\n",
      "  --only_norm           If given, only compute and save normalization file.\r\n",
      "  --flx_same_dt         If given, take surface fluxes from same time step.\r\n",
      "  --norm_features NORM_FEATURES\r\n",
      "                        by_var or by_lev\r\n",
      "  --norm_targets NORM_TARGETS\r\n",
      "                        norm or scale\r\n",
      "  --verbose             If given, print debugging information.\r\n"
     ]
    }
   ],
   "source": [
    "!python cbrain/preprocess_aqua.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs : [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]\r\n",
      "outputs : [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\r\n",
      "in_dir : /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/\r\n",
      "out_dir: /local/S.Rasp/preprocessed_data/"
     ]
    }
   ],
   "source": [
    "!cat pp_config/32col_engy_ess_ref.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROC_DIR = '/local/S.Rasp/preprocessed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important arguments are:\n",
    "- inputs: List of input variables, e.g. [TBP, QBP, VBP, PS, SOLIN, SHFLX, LHFLX]. `_BP` variables are computed as: `TAP - TPHYSTND*dt`\n",
    "- outputs: List of output variables, e.g. [TPHYSTND, PHQ, FSNT, FSNS, FLNT, FLNS, PRECT]\n",
    "- in_dir: Directory where SPCAM files are stored. I strongly recommend putting the files on a fast drive (e.g. /scratch/ on Greenplanet)\n",
    "- aqua_names: Here you can put a string with placeholders `*`, e.g. `'*.h1.0001-*-*-*'` for all files from year 1 or `'*.h1.0000-*-1[7-9]-*'` for day 17-19 from each month of year 0.\n",
    "- out_dir: directory where the processed files will be stored\n",
    "- out_pref: Prefix for the output files. Give them some descriptive name, e.g. `my_experiment1`\n",
    "- ext_norm: This is deprecated and ugly. If ext_norm = None (default), a normalization file will be computed from the data. This normalization file contains means, standard deviations, etc. For a full year of data this can take a long time. For this reason I chose to compute the normalization files for a sample of the data. The differences are small. If ext_norm is some string (just pick any jiberish) no normalization file will be computed but the data will still not be normalized. This is controlled by the `norm_features` and `norm_targets` options which are None and should stay so. The normalization happens later on the fly during network training.\n",
    "\n",
    "Let's start by creating a training dataset using the first three days from each month of the first year of available data. This is a total of 36 days, so roughly a tenth of the entire year. I added timestamps to the script, so that we can see how much time each step in the preprocessing takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checkpoint reading data: 13.15 s\n",
      "Number of time steps: 1728\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Time checkpoint create datasets: 13.28 s\n",
      "Time checkpoint reshape and rechunk: 13.75 s\n",
      "Compute means and stds\n",
      "Saving normalization file: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_norm.nc\n",
      "Time checkpoint normalization arrays: 214.54 s\n",
      "Time checkpoint rechunk and ds: 215.20 s\n",
      "Save features: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_features.nc\n",
      "Save targets: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_targets.nc\n",
      "Total time: 292.96 s\n"
     ]
    }
   ],
   "source": [
    "!python cbrain/preprocess_aqua.py \\\n",
    "--config pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0000-*-0[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the normalization file actually takes longest. This involves computing statistics over the entire sample dimension. But it's not crucial to compute the exactly right statistics for normalization. E.g. when using the entire year as training data, the normalization file computed with a subset of the data is totally fine to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_features.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_norm.nc\r\n",
      "/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc\r\n"
     ]
    }
   ],
   "source": [
    "!ls $PREPROC_DIR/32_col*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a features and a target dataset plus a normalization dataset. Let's check all of them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (feature_lev: 94, sample: 14057472)\n",
       "Coordinates:\n",
       "  * feature_lev    (feature_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...\n",
       "    time           (sample) int64 ...\n",
       "    lat            (sample) float64 ...\n",
       "    lon            (sample) float64 ...\n",
       "    feature_names  (feature_lev) object ...\n",
       "Dimensions without coordinates: sample\n",
       "Data variables:\n",
       "    features       (sample, feature_lev) float32 ...\n",
       "Attributes:\n",
       "    log:      \\n    Time: 2019-01-08T12:45:18\\n\\n    Executed command:\\n\\n   ..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_features.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (sample: 14057472, target_lev: 65)\n",
       "Coordinates:\n",
       "  * target_lev    (target_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...\n",
       "    time          (sample) int64 ...\n",
       "    lat           (sample) float64 ...\n",
       "    lon           (sample) float64 ...\n",
       "Dimensions without coordinates: sample\n",
       "Data variables:\n",
       "    targets       (sample, target_lev) float32 ...\n",
       "    target_names  (target_lev) object ...\n",
       "Attributes:\n",
       "    log:      \\n    Time: 2019-01-08T12:45:18\\n\\n    Executed command:\\n\\n   ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two dimensions, `sample` and `target_lev`, which corresponds to the stacked vectors. Additionally we have the variable `target_names`, which is handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'target_names' (target_lev: 65)>\n",
       "array(['TPHYSTND_lev00', 'TPHYSTND_lev01', 'TPHYSTND_lev02', 'TPHYSTND_lev03',\n",
       "       'TPHYSTND_lev04', 'TPHYSTND_lev05', 'TPHYSTND_lev06', 'TPHYSTND_lev07',\n",
       "       'TPHYSTND_lev08', 'TPHYSTND_lev09', 'TPHYSTND_lev10', 'TPHYSTND_lev11',\n",
       "       'TPHYSTND_lev12', 'TPHYSTND_lev13', 'TPHYSTND_lev14', 'TPHYSTND_lev15',\n",
       "       'TPHYSTND_lev16', 'TPHYSTND_lev17', 'TPHYSTND_lev18', 'TPHYSTND_lev19',\n",
       "       'TPHYSTND_lev20', 'TPHYSTND_lev21', 'TPHYSTND_lev22', 'TPHYSTND_lev23',\n",
       "       'TPHYSTND_lev24', 'TPHYSTND_lev25', 'TPHYSTND_lev26', 'TPHYSTND_lev27',\n",
       "       'TPHYSTND_lev28', 'TPHYSTND_lev29', 'PHQ_lev00', 'PHQ_lev01',\n",
       "       'PHQ_lev02', 'PHQ_lev03', 'PHQ_lev04', 'PHQ_lev05', 'PHQ_lev06',\n",
       "       'PHQ_lev07', 'PHQ_lev08', 'PHQ_lev09', 'PHQ_lev10', 'PHQ_lev11',\n",
       "       'PHQ_lev12', 'PHQ_lev13', 'PHQ_lev14', 'PHQ_lev15', 'PHQ_lev16',\n",
       "       'PHQ_lev17', 'PHQ_lev18', 'PHQ_lev19', 'PHQ_lev20', 'PHQ_lev21',\n",
       "       'PHQ_lev22', 'PHQ_lev23', 'PHQ_lev24', 'PHQ_lev25', 'PHQ_lev26',\n",
       "       'PHQ_lev27', 'PHQ_lev28', 'PHQ_lev29', 'FSNT', 'FSNS', 'FLNT', 'FLNS',\n",
       "       'PRECT'], dtype=object)\n",
       "Coordinates:\n",
       "  * target_lev  (target_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc').target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:              (feature_lev: 94, target_lev: 65)\n",
       "Coordinates:\n",
       "  * feature_lev          (feature_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 ...\n",
       "  * target_lev           (target_lev) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 ...\n",
       "Data variables:\n",
       "    feature_means        (feature_lev) float32 ...\n",
       "    feature_stds         (feature_lev) float32 ...\n",
       "    feature_mins         (feature_lev) float32 ...\n",
       "    feature_maxs         (feature_lev) float32 ...\n",
       "    target_means         (target_lev) float32 ...\n",
       "    target_stds          (target_lev) float32 ...\n",
       "    target_mins          (target_lev) float32 ...\n",
       "    target_maxs          (target_lev) float32 ...\n",
       "    feature_names        (feature_lev) object ...\n",
       "    target_names         (target_lev) object ...\n",
       "    feature_stds_by_var  (feature_lev) float32 ...\n",
       "    target_conv          (target_lev) float32 ...\n",
       "Attributes:\n",
       "    log:      \\n    Time: 2019-01-08T12:45:18\\n\\n    Executed command:\\n\\n   ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('/local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_norm.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization file contains a number of statistics for the features and targets, which can then be used during training for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to have a separate validation dataset. For this we simply chose a different date range, the months available for the second year. For the validation dataset we do not need to compute a normalization file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checkpoint reading data: 0.81 s\n",
      "Number of time steps: 432\n",
      "Cut time steps: [143 287]\n",
      "Cut time steps: [143 287]\n",
      "Time checkpoint create datasets: 0.90 s\n",
      "Time checkpoint reshape and rechunk: 1.03 s\n",
      "Load external normalization file\n",
      "Time checkpoint normalization arrays: 1.03 s\n",
      "Time checkpoint rechunk and ds: 1.13 s\n",
      "Save features: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_valid_features.nc\n",
      "Save targets: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_valid_targets.nc\n",
      "Total time: 26.12 s\n"
     ]
    }
   ],
   "source": [
    "!python cbrain/preprocess_aqua.py \\\n",
    "--config pp_config/32col_engy_ess_ref.yml \\\n",
    "--aqua_names '*.h1.0001-*-0[1-3]-*' \\\n",
    "--out_pref 32_col_engy_ess_3d_valid --ext_norm Nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 S.Rasp ls-craig 5.3G Jan  8 12:49 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_features.nc\r\n",
      "-rw-r--r-- 1 S.Rasp ls-craig  24K Jan  8 12:48 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_norm.nc\r\n",
      "-rw-r--r-- 1 S.Rasp ls-craig 5.3G Jan  8 13:03 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_shuffle_features.nc\r\n",
      "-rw-r--r-- 1 S.Rasp ls-craig 3.8G Jan  8 13:03 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_shuffle_targets.nc\r\n",
      "-rw-r--r-- 1 S.Rasp ls-craig 3.8G Jan  8 12:50 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_train_targets.nc\r\n",
      "-rw-r--r-- 1 S.Rasp ls-craig 5.3G Jan  8 12:59 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_valid_features.nc\r\n",
      "-rw-r--r-- 1 S.Rasp ls-craig 3.8G Jan  8 12:59 /local/S.Rasp/preprocessed_data//32_col_engy_ess_3d_valid_targets.nc\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $PREPROC_DIR/32_col*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed comparison: regular hard-drive vs. SSD\n",
    "\n",
    "I copy all the raw data to an SSD drive, in order to speed up the computation. We can load the data from a regular disc (but still save on SSD), to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checkpoint reading data: 40.51 s\n",
      "Number of time steps: 1728\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Cut time steps: [ 143  287  431  575  719  863 1007 1151 1295 1439 1583]\n",
      "Time checkpoint create datasets: 40.64 s\n",
      "Time checkpoint reshape and rechunk: 42.67 s\n",
      "Compute means and stds\n",
      "Saving normalization file: /local/S.Rasp/preprocessed_data/test_32_col_engy_ess_3d_train_norm.nc\n",
      "Time checkpoint normalization arrays: 1352.22 s\n",
      "Time checkpoint rechunk and ds: 1352.89 s\n",
      "Save features: /local/S.Rasp/preprocessed_data/test_32_col_engy_ess_3d_train_features.nc\n",
      "Save targets: /local/S.Rasp/preprocessed_data/test_32_col_engy_ess_3d_train_targets.nc\n",
      "Total time: 2062.98 s\n"
     ]
    }
   ],
   "source": [
    "!python cbrain/preprocess_aqua.py \\\n",
    "--config pp_config/32col_engy_ess_ref.yml \\\n",
    "--in_dir /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/ --aqua_names '*.h1.0000-*-0[1-3]-*' \\\n",
    "--out_pref test_32_col_engy_ess_3d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to process 36 days:\n",
    "\n",
    "SSD: 5 minutes    \n",
    "Hard-drive: 36 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Entire year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from root directory:\n",
    "```\n",
    "python cbrain/preprocess_aqua.py --config pp_config/32col_engy_ess_ref.yml --in_dir /project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/ --aqua_names '*.h1.0000-*' --out_pref 32_col_engy_ess_1y_train --ext_norm Nope\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the training dataset\n",
    "\n",
    "During network training it's important that the samples in each batch are random. Currently they are still in order (lat-lon-time only have been flattened). Ideally, you would randomly shuffle during training. Because the datasets exceed RAM however, shuffling means reading random locations on disc for each sample in the batch. This is slow.\n",
    "\n",
    "For this reason, I am pre-shuffling the training dataset once. Then during training, for each batch we will read a continuous slice of the shuffled dataset. Only the order of the batches is random. This should be a good enough approximation. \n",
    "\n",
    "Shuffling is done by the `shuffle_ds.py` script in the `cbrain` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: shuffle_ds.py [-h] [--method METHOD] [--pref PREF]\r\n",
      "                     [--random_seed RANDOM_SEED] [--chunk_size CHUNK_SIZE]\r\n",
      "                     [--verbose VERBOSE]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --method METHOD       [Meticulous or fast]\r\n",
      "  --pref PREF           Prefix. ie without the _features.nc\r\n",
      "  --random_seed RANDOM_SEED\r\n",
      "                        Random seed for shuffling of data.\r\n",
      "  --chunk_size CHUNK_SIZE\r\n",
      "                        Size of chunks for fast method\r\n",
      "  --verbose VERBOSE     Verbosity level\r\n"
     ]
    }
   ],
   "source": [
    "!python cbrain/shuffle_ds.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_features.nc /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_targets.nc\n",
      "Creating files: /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_shuffle_features.nc /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train_shuffle_targets.nc\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:51<00:00, 25.74s/it]\n"
     ]
    }
   ],
   "source": [
    "!python cbrain/shuffle_ds.py --pref /local/S.Rasp/preprocessed_data/32_col_engy_ess_3d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Entire year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and convert to \n",
    "\n",
    "Now we will use the preprocessed files and train the neural network. Then we will save the parameters of the network in text format to use them inside CAM.\n",
    "\n",
    "For now I will only use the command line script `run_experiment.py` which is in the root directory of the repositories. At some point, I will go deeper and analyze what is actually going on during training. In particular, we need to look at the data_generator function, which reads and modifies the preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n",
    "\n",
    "Again, the script takes a config file. The idea of the config files is to make the runs reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_name: 001_32cols_engy_ess_3d_pnas\r\n",
      "data_dir: /local/S.Rasp/preprocessed_data/\r\n",
      "train_fn: 32_col_engy_ess_3d_train_shuffle\r\n",
      "valid_fn: 32_col_engy_ess_3d_valid\r\n",
      "norm_fn: 32_col_engy_ess_3d_train_norm.nc\r\n",
      "fsub: feature_means\r\n",
      "fdiv: max_rs\r\n",
      "tmult: target_conv\r\n",
      "activation: LeakyReLU\r\n",
      "hidden_layers: [256,256,256,256,256,256,256,256,256]\r\n",
      "loss: mse\r\n",
      "log_dir: ./logs/\r\n",
      "epochs: 18\r\n",
      "lr_step: 3\r\n"
     ]
    }
   ],
   "source": [
    "!cat nn_config/32cols/001_32cols_engy_ess_3d_pnas.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation:\n",
    "- log_dir: This simply saves a Tensorboard log. I would not worry about this for now since it doesn't really give more information that the scores.\n",
    "- lr_step: In addition to the simple example above, I aplemented a learning rate scheduler. I basically copied this: https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/ lr_step defines after how many epochs the learning rate is dropped.\n",
    "- valid_after: If this is true, the validation score is only computed for the last epoch. This is reasonable once you know that your model behaves ok. Computing the validation score actually takes some time.\n",
    "- model_dir and exp_name: The keras model is saved after training in the directory specified with the experiment name. We need the saved model to implement into CAM later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/s/S.Rasp/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "usage: run_experiment.py [-h] [-c CONFIG_FILE] [--exp_name EXP_NAME]\n",
      "                         [--model_dir MODEL_DIR] [--data_dir DATA_DIR]\n",
      "                         [--train_fn TRAIN_FN] [--valid_fn VALID_FN]\n",
      "                         [--norm_fn NORM_FN] [--log_dir LOG_DIR] [--fsub FSUB]\n",
      "                         [--fdiv FDIV] [--tsub TSUB] [--tmult TMULT]\n",
      "                         [--loss LOSS] [--mse_var_ratio MSE_VAR_RATIO]\n",
      "                         [--activation ACTIVATION] [--lr LR]\n",
      "                         [--lr_step LR_STEP] [--lr_divide LR_DIVIDE]\n",
      "                         [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                         [--kernel_size KERNEL_SIZE] [--stride STRIDE]\n",
      "                         [--hidden_layers HIDDEN_LAYERS [HIDDEN_LAYERS ...]]\n",
      "                         [--dr DR] [--noise NOISE] [--l2 L2]\n",
      "                         [--conv_layers CONV_LAYERS [CONV_LAYERS ...]]\n",
      "                         [--n_workers N_WORKERS]\n",
      "                         [--max_queue_size MAX_QUEUE_SIZE] [--convolution]\n",
      "                         [--tile] [--padding PADDING] [--batch_norm]\n",
      "                         [--partial_relu] [--eq] [--valid_after]\n",
      "                         [--locally_connected] [--verbose]\n",
      "\n",
      "Args that start with '--' (eg. --exp_name) can also be set in a config file\n",
      "(specified via -c). Config file syntax allows: key=value, flag=true,\n",
      "stuff=[a,b,c] (for details, see syntax at https://goo.gl/R74nmi). If an arg is\n",
      "specified in more than one place, then commandline values override config file\n",
      "values which override defaults.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -c CONFIG_FILE, --config_file CONFIG_FILE\n",
      "                        Name of config file.\n",
      "  --exp_name EXP_NAME   Experiment name.\n",
      "  --model_dir MODEL_DIR\n",
      "                        Directory to save model to.\n",
      "  --data_dir DATA_DIR   Full outputs file.\n",
      "  --train_fn TRAIN_FN   Training set file.\n",
      "  --valid_fn VALID_FN   Validation set file.\n",
      "  --norm_fn NORM_FN     Normalization file. Default: None -> Infer\n",
      "  --log_dir LOG_DIR     TensorBoard log dir\n",
      "  --fsub FSUB           Subtract feature array by. Default: None\n",
      "  --fdiv FDIV           Divide feature array by. Special: range. Default: None\n",
      "  --tsub TSUB           Subtract target array by. Default: None\n",
      "  --tmult TMULT         Divide target array by, e.g. target_conv. Default:\n",
      "                        None\n",
      "  --loss LOSS           Loss function.\n",
      "  --mse_var_ratio MSE_VAR_RATIO\n",
      "                        If mse_var loss function, multiplier for var.\n",
      "  --activation ACTIVATION\n",
      "                        Activation function.\n",
      "  --lr LR               Learning rate.\n",
      "  --lr_step LR_STEP     Step at which to divide learning rate by factor.\n",
      "  --lr_divide LR_DIVIDE\n",
      "                        Factor to divide learning rate.\n",
      "  --epochs EPOCHS       Number of epochs\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size\n",
      "  --kernel_size KERNEL_SIZE\n",
      "                        Size of convolution kernel.\n",
      "  --stride STRIDE       stride\n",
      "  --hidden_layers HIDDEN_LAYERS [HIDDEN_LAYERS ...]\n",
      "                        List with hidden nodes.\n",
      "  --dr DR               Dropout rate.\n",
      "  --noise NOISE         Training noise std\n",
      "  --l2 L2               L2 regularization for dense layers\n",
      "  --conv_layers CONV_LAYERS [CONV_LAYERS ...]\n",
      "                        List with feature maps\n",
      "  --n_workers N_WORKERS\n",
      "                        Workers for generator queue\n",
      "  --max_queue_size MAX_QUEUE_SIZE\n",
      "                        Generator queue size\n",
      "  --convolution         Use convolutional net.\n",
      "  --tile                tile\n",
      "  --padding PADDING     padding\n",
      "  --batch_norm          Use batch_norm.\n",
      "  --partial_relu        ...\n",
      "  --eq                  ...\n",
      "  --valid_after         Only validate after training.\n",
      "  --locally_connected   Use locally connected convolutions.\n",
      "  --verbose             Print additional information.\n"
     ]
    }
   ],
   "source": [
    "!python run_experiment.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/s/S.Rasp/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "2019-01-08 13:50:03.328552: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F FMA\n",
      "2019-01-08 13:50:03.483059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\n",
      "pciBusID: 0000:b3:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 6.47GiB\n",
      "2019-01-08 13:50:03.483092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2019-01-08 13:50:03.736182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-08 13:50:03.736232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2019-01-08 13:50:03.736238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2019-01-08 13:50:03.736462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6246 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:b3:00.0, compute capability: 6.1)\n",
      "Generator will have 14057472 samples in 13728 batches\n",
      "Features have shape 94; targets have shape 65\n",
      "Generator will have 14057472 samples in 858 batches\n",
      "Features have shape 94; targets have shape 65\n",
      "Epoch 1/18\n",
      "lr: 0.001\n",
      "13728/13728 [==============================] - 106s 8ms/step - loss: 0.0023 - rmse: 0.0356 - log_loss: -1.5703 - var_ratio: 0.7211 - mean_squared_error: 0.0023 - var_loss: 5.1812e-06 - val_loss: 0.0021 - val_rmse: 0.0349 - val_log_loss: -1.5792 - val_var_ratio: 0.7904 - val_mean_squared_error: 0.0021 - val_var_loss: 2.7357e-06\n",
      "Epoch 2/18\n",
      "lr: 0.001\n",
      "13728/13728 [==============================] - 101s 7ms/step - loss: 0.0021 - rmse: 0.0343 - log_loss: -1.5917 - var_ratio: 0.7379 - mean_squared_error: 0.0021 - var_loss: 4.5495e-06 - val_loss: 0.0021 - val_rmse: 0.0341 - val_log_loss: -1.5896 - val_var_ratio: 0.7611 - val_mean_squared_error: 0.0021 - val_var_loss: 3.5480e-06\n",
      "Epoch 3/18\n",
      "lr: 0.0002\n",
      "13728/13728 [==============================] - 100s 7ms/step - loss: 0.0020 - rmse: 0.0327 - log_loss: -1.6196 - var_ratio: 0.7547 - mean_squared_error: 0.0020 - var_loss: 3.9631e-06 - val_loss: 0.0019 - val_rmse: 0.0327 - val_log_loss: -1.6158 - val_var_ratio: 0.7247 - val_mean_squared_error: 0.0019 - val_var_loss: 4.7013e-06\n",
      "Epoch 4/18\n",
      "lr: 0.0002\n",
      "13728/13728 [==============================] - 99s 7ms/step - loss: 0.0020 - rmse: 0.0325 - log_loss: -1.6233 - var_ratio: 0.7579 - mean_squared_error: 0.0020 - var_loss: 3.8635e-06 - val_loss: 0.0019 - val_rmse: 0.0326 - val_log_loss: -1.6176 - val_var_ratio: 0.7283 - val_mean_squared_error: 0.0019 - val_var_loss: 4.5813e-06\n",
      "Epoch 5/18\n",
      "lr: 0.0002\n",
      "13728/13728 [==============================] - 99s 7ms/step - loss: 0.0019 - rmse: 0.0324 - log_loss: -1.6255 - var_ratio: 0.7596 - mean_squared_error: 0.0019 - var_loss: 3.8074e-06 - val_loss: 0.0019 - val_rmse: 0.0325 - val_log_loss: -1.6198 - val_var_ratio: 0.7301 - val_mean_squared_error: 0.0019 - val_var_loss: 4.5210e-06\n",
      "Epoch 6/18\n",
      "lr: 4.000000000000001e-05\n",
      "13728/13728 [==============================] - 100s 7ms/step - loss: 0.0019 - rmse: 0.0319 - log_loss: -1.6337 - var_ratio: 0.7645 - mean_squared_error: 0.0019 - var_loss: 3.6476e-06 - val_loss: 0.0019 - val_rmse: 0.0322 - val_log_loss: -1.6261 - val_var_ratio: 0.7597 - val_mean_squared_error: 0.0019 - val_var_loss: 3.5859e-06\n",
      "Epoch 7/18\n",
      "lr: 4.000000000000001e-05\n",
      "13728/13728 [==============================] - 100s 7ms/step - loss: 0.0019 - rmse: 0.0319 - log_loss: -1.6347 - var_ratio: 0.7654 - mean_squared_error: 0.0019 - var_loss: 3.6187e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6267 - val_var_ratio: 0.7587 - val_mean_squared_error: 0.0019 - val_var_loss: 3.6156e-06\n",
      "Epoch 8/18\n",
      "lr: 4.000000000000001e-05\n",
      "13728/13728 [==============================] - 98s 7ms/step - loss: 0.0019 - rmse: 0.0318 - log_loss: -1.6355 - var_ratio: 0.7660 - mean_squared_error: 0.0019 - var_loss: 3.6009e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6274 - val_var_ratio: 0.7584 - val_mean_squared_error: 0.0019 - val_var_loss: 3.6248e-06\n",
      "Epoch 9/18\n",
      "lr: 8.000000000000001e-06\n",
      "13728/13728 [==============================] - 94s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6374 - var_ratio: 0.7672 - mean_squared_error: 0.0019 - var_loss: 3.5634e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6275 - val_var_ratio: 0.7676 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3544e-06\n",
      "Epoch 10/18\n",
      "lr: 8.000000000000001e-06\n",
      "13728/13728 [==============================] - 95s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6376 - var_ratio: 0.7675 - mean_squared_error: 0.0019 - var_loss: 3.5536e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6276 - val_var_ratio: 0.7679 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3445e-06\n",
      "Epoch 11/18\n",
      "lr: 8.000000000000001e-06\n",
      "13728/13728 [==============================] - 98s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6378 - var_ratio: 0.7676 - mean_squared_error: 0.0019 - var_loss: 3.5487e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6277 - val_var_ratio: 0.7681 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3390e-06\n",
      "Epoch 12/18\n",
      "lr: 1.6000000000000004e-06\n",
      "13728/13728 [==============================] - 97s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6382 - var_ratio: 0.7679 - mean_squared_error: 0.0019 - var_loss: 3.5409e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6279 - val_var_ratio: 0.7685 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3289e-06\n",
      "Epoch 13/18\n",
      "lr: 1.6000000000000004e-06\n",
      "13728/13728 [==============================] - 98s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6382 - var_ratio: 0.7680 - mean_squared_error: 0.0019 - var_loss: 3.5376e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6279 - val_var_ratio: 0.7685 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3283e-06\n",
      "Epoch 14/18\n",
      "lr: 1.6000000000000004e-06\n",
      "13728/13728 [==============================] - 99s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6383 - var_ratio: 0.7680 - mean_squared_error: 0.0019 - var_loss: 3.5363e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6280 - val_var_ratio: 0.7686 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3262e-06\n",
      "Epoch 15/18\n",
      "lr: 3.200000000000001e-07\n",
      "13728/13728 [==============================] - 99s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6383 - var_ratio: 0.7681 - mean_squared_error: 0.0019 - var_loss: 3.5346e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6280 - val_var_ratio: 0.7693 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3065e-06\n",
      "Epoch 16/18\n",
      "lr: 3.200000000000001e-07\n",
      "13728/13728 [==============================] - 101s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6384 - var_ratio: 0.7681 - mean_squared_error: 0.0019 - var_loss: 3.5339e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6280 - val_var_ratio: 0.7693 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3061e-06\n",
      "Epoch 17/18\n",
      "lr: 3.200000000000001e-07\n",
      "13728/13728 [==============================] - 98s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6384 - var_ratio: 0.7681 - mean_squared_error: 0.0019 - var_loss: 3.5337e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6280 - val_var_ratio: 0.7693 - val_mean_squared_error: 0.0019 - val_var_loss: 3.3061e-06\n",
      "Epoch 18/18\n",
      "lr: 6.400000000000003e-08\n",
      "13728/13728 [==============================] - 97s 7ms/step - loss: 0.0019 - rmse: 0.0317 - log_loss: -1.6384 - var_ratio: 0.7682 - mean_squared_error: 0.0019 - var_loss: 3.5316e-06 - val_loss: 0.0019 - val_rmse: 0.0321 - val_log_loss: -1.6280 - val_var_ratio: 0.7698 - val_mean_squared_error: 0.0019 - val_var_loss: 3.2898e-06\n"
     ]
    }
   ],
   "source": [
    "!python run_experiment.py -c nn_config/32cols/001_32cols_engy_ess_3d_pnas.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we trained for way too long. How long to train depends on the diversity of the training data and the data amount. Improving this is something to work on, see: https://github.com/raspstephan/CBRAIN-CAM/issues/1\n",
    "\n",
    "We can also see that we do not get any noticeable overfitting in the traditional sense, i.e. difference between training and validation loss.\n",
    "\n",
    "The neural network matrices and architecture are saved in the `./saved_models` directory as h5 files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../saved_models/001_32cols_engy_ess_3d_pnas.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls saved_models/001_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting .h5 file to txt file\n",
    "\n",
    "The next step here is now to convert the h5 file to files that can be read by the neural network version of CAM. For this we will use another script, called `save_weights.py`, which takes the same config file as `run_experiment.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/s/S.Rasp/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "2019-01-08 14:44:47.617709: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F FMA\n",
      "2019-01-08 14:44:47.771843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\n",
      "pciBusID: 0000:b3:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 6.48GiB\n",
      "2019-01-08 14:44:47.771877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n",
      "2019-01-08 14:44:48.018103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-08 14:44:48.018135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n",
      "2019-01-08 14:44:48.018143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n",
      "2019-01-08 14:44:48.018326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6261 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:b3:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "!python save_weights.py -c nn_config/32cols/001_32cols_engy_ess_3d_pnas.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_max_rs.txt\t     layer1_bias.txt\tlayer5_bias.txt    layer9_bias.txt\r\n",
      "inp_maxs.txt\t     layer1_kernel.txt\tlayer5_kernel.txt  layer9_kernel.txt\r\n",
      "inp_means.txt\t     layer2_bias.txt\tlayer6_bias.txt    outp_maxs.txt\r\n",
      "inp_mins.txt\t     layer2_kernel.txt\tlayer6_kernel.txt  outp_mins.txt\r\n",
      "inp_stds_by_var.txt  layer3_bias.txt\tlayer7_bias.txt    weights.h5\r\n",
      "inp_stds.txt\t     layer3_kernel.txt\tlayer7_kernel.txt\r\n",
      "layer10_bias.txt     layer4_bias.txt\tlayer8_bias.txt\r\n",
      "layer10_kernel.txt   layer4_kernel.txt\tlayer8_kernel.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls saved_models/001_32cols_engy_ess_3d_pnas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and biases for all the layers are saved in separate text files, plus the relevant normalization arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.247076e-01,2.444012e-02,-6.513681e-02,1.084918e-01,-1.971805e-02,1.679797e-02,6.645612e-02,1.205308e-01,9.274448e-02,-6.410713e-02,-5.922029e-02,-8.371486e-02,-6.580098e-02,-7.378058e-02,-1.398824e-02,-1.351725e-01,-1.762785e-01,7.083420e-02,2.030234e-03,2.106156e-01,3.582359e-01,3.041640e-01,2.500775e-01,-5.012782e-02,-1.125699e-01,4.082102e-02,-1.728023e-01,-6.359089e-03,-1.227781e-01,-1.985432e-02,1.931775e-01,1.839796e-02,-7.847431e-02,-6.119738e-02,-1.337386e-01,1.668242e-01,3.829718e-02,5.141954e-02,5.827349e-01,1.015465e+00,8.116463e-01,6.864675e-01,5.228361e-01,1.373079e-01,-8.082748e-02,-4.842604e-02,8.416072e-03,7.908968e-02,-4.162578e-02,5.653955e-02,2.271484e-02,9.763756e-02,4.991648e-02,-3.098556e-02,-6.799575e-03,1.183797e-01,-1.354993e-01,9.173439e-02,2.124729e-02,-4.702663e-02,-6.526815e-02,1.570719e-01,5.800200e-02,-9.230268e-02,-6.970107e-02,-3.576361e-02,8.898021e-03,7.295761e-02,-7.373397e-02,-4.990729e-02,7.852868e-02,-1.300494e-02,-1.297906e-01,8.923770e-02,5.708418e-02,8.192351e-02,4.105149e-02,6.594840e-02,1.049470e-02,-1.153375e-01,-1.944382e-01,-1.321899e-01,2.516997e-02,4.427574e-02,1.319279e-01,2.176304e-01,1.743118e-01,1.669100e-01,-2.432619e-01,-1.371924e-01,1.126827e-01,-2.103119e-02,7.135925e-01,-3.162053e-01\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat saved_models/001_32cols_engy_ess_3d_pnas/layer1_kernel.txt | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to Stampede"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
