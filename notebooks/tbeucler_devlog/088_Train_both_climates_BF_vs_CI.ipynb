{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/13/2022 - The goal of this notebook is to train both brute-force and climate-invariant neural nets on (-4K) and (+4K) simultaneously to see whether climate-invariant nets facilitate learning both climates simultaneously. We will explore uncertainty using dropout and test different architectures by varying the depth of the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "from cbrain.climate_invariant import *\n",
    "from cbrain.climate_invariant_utils import *\n",
    "import pickle\n",
    "\n",
    "from cbrain.imports import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.normalization import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_m4K = path_data + '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "path_train_p4K = path_data + '2021_03_18_O3_TRAIN_P4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_valid_m4K = path_data + '2021_03_18_O3_VALID_M4K.nc'\n",
    "path_valid_p4K = path_data + '2021_03_18_O3_VALID_P4K.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_m4K = path_data + '2021_03_18_O3_TEST_M4K.nc'\n",
    "path_test_p4K = path_data + '2021_03_18_O3_TEST_P4K.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_norm = path_data + '2021_01_24_NORM_O3_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_norm_RH = path_data + '2021_02_01_NORM_O3_RH_small.nc'\n",
    "path_norm_BMSE = path_data + '2021_06_16_NORM_BMSE_small.nc'\n",
    "path_norm_LHF_nsDELQ = path_data + '2021_02_01_NORM_O3_LHF_nsDELQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS','SOLIN','SHFLX','LHFLX'] # We take the large-scale climate state as inputs\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS'] # and we output the response of clouds/storms to these climate conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = pickle.load(open(path_data+'009_Wm2_scaling.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the data generator so that it can load two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_2DS(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "    Data generator class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_fn1, data_fn2, input_vars, output_vars,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=8192, shuffle=True, xarray=False, var_cut_off=None):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn1, self.data_fn2, self.norm_fn = data_fn1, data_fn2, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_mfdataset([self.data_fn1,self.data_fn2],\n",
    "                                         concat_dim=\"sample\",\n",
    "                                         combine='nested')\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "\n",
    "        # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.vars.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        # Get input and output variable indices\n",
    "        self.input_idxs = return_var_idxs(self.data_ds, input_vars, var_cut_off)\n",
    "        self.output_idxs = return_var_idxs(self.data_ds, output_vars)\n",
    "        self.n_inputs, self.n_outputs = len(self.input_idxs), len(self.output_idxs)\n",
    "\n",
    "        # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer(self.norm_ds, output_vars, output_transform)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "        if not xarray:\n",
    "            self.data_ds.close()\n",
    "            self.data_ds = h5py.File(data_fn1, 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorCI_2DS(DataGenerator_2DS):\n",
    "    def __init__(self, data_fn1, data_fn2, input_vars, output_vars,\n",
    "             norm_fn=None, input_transform=None, output_transform=None,\n",
    "             batch_size=8192, shuffle=True, xarray=False, var_cut_off=None, \n",
    "             Qscaling=None,\n",
    "             Tscaling=None,\n",
    "             LHFscaling=None,\n",
    "             SHFscaling=None,\n",
    "             output_scaling=False,\n",
    "             interpolate=False,\n",
    "             hyam=None,hybm=None,\n",
    "             inp_sub_Qscaling=None,inp_div_Qscaling=None,\n",
    "             inp_sub_Tscaling=None,inp_div_Tscaling=None,\n",
    "             inp_sub_LHFscaling=None,inp_div_LHFscaling=None,\n",
    "             inp_sub_SHFscaling=None,inp_div_SHFscaling=None,\n",
    "             lev=None, interm_size=40,\n",
    "             lower_lim=6,\n",
    "             is_continous=True,Tnot=5,epsQ=1e-3,epsT=1,\n",
    "                 mode='train'):\n",
    "        self.output_scaling = output_scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.Qscaling = Qscaling\n",
    "        self.Tscaling = Tscaling\n",
    "        self.LHFscaling = LHFscaling\n",
    "        self.SHFscaling = SHFscaling\n",
    "        self.inp_shape = 64\n",
    "        self.mode=mode\n",
    "        super().__init__(data_fn1, data_fn2, input_vars,output_vars,norm_fn,input_transform,output_transform,\n",
    "                        batch_size,shuffle,xarray,var_cut_off) ## call the base data generator\n",
    "        self.inp_sub = self.input_transform.sub\n",
    "        self.inp_div = self.input_transform.div\n",
    "        if Qscaling=='RH':\n",
    "            self.QLayer = QV2RHNumpy(self.inp_sub,self.inp_div,inp_sub_Qscaling,inp_div_Qscaling,hyam,hybm)\n",
    "        elif Qscaling=='QSATdeficit':\n",
    "            self.QLayer = QV2QSATdeficitNumpy(self.inp_sub,self.inp_div,inp_sub_Qscaling,inp_div_Qscaling,hyam,hybm)\n",
    "        if Tscaling=='TfromNS':\n",
    "            self.TLayer = T2TmTNSNumpy(self.inp_sub,self.inp_div,inp_sub_Tscaling,inp_div_Tscaling,hyam,hybm)\n",
    "        elif Tscaling=='BCONS':\n",
    "            self.TLayer = T2BCONSNumpy(self.inp_sub,self.inp_div,inp_sub_Tscaling,inp_div_Tscaling,hyam,hybm)\n",
    "        elif Tscaling=='BMSE':\n",
    "            self.TLayer = T2BMSENumpy(self.inp_sub,self.inp_div,inp_sub_Tscaling,inp_div_Tscaling,hyam,hybm)\n",
    "        elif Tscaling=='T_NSto220':\n",
    "            self.TLayer = T2T_NSto220Numpy(self.inp_sub,self.inp_div,inp_sub_Tscaling,inp_div_Tscaling,hyam,hybm)\n",
    "        if LHFscaling=='LHF_nsDELQ':\n",
    "            self.LHFLayer = LHF2LHF_nsDELQNumpy(self.inp_sub,self.inp_div,inp_sub_LHFscaling,inp_div_LHFscaling,hyam,hybm,epsQ)\n",
    "        elif LHFscaling=='LHF_nsQ':\n",
    "            self.LHFLayer = LHF2LHF_nsQNumpy(self.inp_sub,self.inp_div,inp_sub_LHFscaling,inp_div_LHFscaling,hyam,hybm,epsQ)\n",
    "        if SHFscaling=='SHF_nsDELT':\n",
    "            self.SHFLayer = SHF2SHF_nsDELTNumpy(self.inp_sub,self.inp_div,inp_sub_SHFscaling,inp_div_SHFscaling,hyam,hybm,epsT)\n",
    "        if output_scaling:\n",
    "            self.scalingLayer = ScalingNumpy(hyam,hybm)\n",
    "            self.inp_shape += 1\n",
    "        if interpolate:\n",
    "            self.interpLayer = InterpolationNumpy(lev,is_continous,Tnot,lower_lim,interm_size)\n",
    "            self.inp_shape += interm_size*2 + 4 + 30 ## 4 same as 60-64 and 30 for lev_tilde.size\n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "        X_result = np.copy(X_norm)\n",
    "        if self.Qscaling:\n",
    "            X_result = self.QLayer.process(X_result)\n",
    "        if self.Tscaling:\n",
    "            # tgb - 3/21/2021 - BCONS needs qv in kg/kg as an input\n",
    "            if self.Tscaling=='BCONS' or self.Tscaling=='BMSE':\n",
    "                if self.Qscaling:\n",
    "                    X_resultT = self.TLayer.process(X_norm)\n",
    "                    X_result = np.concatenate([X_result[:,:30],X_resultT[:,30:60],X_result[:,60:]], axis=1)\n",
    "                else:\n",
    "                    X_result = self.TLayer.process(X_result)\n",
    "            else:\n",
    "                X_result = self.TLayer.process(X_result)\n",
    "        if self.SHFscaling:\n",
    "            X_result = self.SHFLayer.process(X_result)\n",
    "        if self.LHFscaling:\n",
    "            # tgb - 3/22/2021 - LHF_ns(DEL)Q needs qv in kg/kg and T in K\n",
    "            if self.Qscaling or self.Tscaling:\n",
    "                X_resultLHF = self.LHFLayer.process(X_norm)\n",
    "                X_result = np.concatenate([X_result[:,:60],X_resultLHF[:,60:]],axis=1)\n",
    "            else:\n",
    "                X_result = self.LHFLayer.process(X_result)\n",
    "        if self.output_scaling:\n",
    "            scalings = self.scalingLayer.process(X)\n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "        if self.mode=='val':\n",
    "            return xr.DataArray(X_result), xr.DataArray(Y)\n",
    "        return X_result,Y\n",
    "    ##transforms the input data into the required format, take the unnormalized dataset\n",
    "    def transform(self,X):\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        X_result = X_norm\n",
    "        if self.Qscaling:\n",
    "            X_result = self.QLayer.process(X_result)\n",
    "        if self.Tscaling:\n",
    "            X_result = self.TLayer.process(X_result)\n",
    "        if self.SHFscaling:\n",
    "            X_result = self.SHFLayer.process(X_result)\n",
    "        if self.LHFscaling:\n",
    "            X_result = self.LHFLayer.process(X_result)\n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X)\n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "        return X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the generator so that it can only use a reduced amount of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BF = DataGeneratorCI_2DS(\n",
    "    data_fn1 = path_train_m4K,\n",
    "    data_fn2 = path_train_p4K,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path_input_norm,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate-Invariant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen_rescaling(input_rescaling):\n",
    "    return DataGeneratorCI_2DS(\n",
    "        data_fn1 = path_train_m4K,\n",
    "        data_fn2 = path_train_p4K,\n",
    "        input_vars = input_rescaling,\n",
    "        output_vars = out_vars,\n",
    "        norm_fn = path_input_norm,\n",
    "        input_transform = ('mean', 'maxrs'),\n",
    "        output_transform = scale_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = train_gen_rescaling(in_vars)\n",
    "train_gen_BMSE = train_gen_rescaling(in_vars)\n",
    "train_gen_LHF_nsDELQ = train_gen_rescaling(in_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI_2DS(\n",
    "    data_fn1 = path_train_m4K,\n",
    "    data_fn2 = path_train_p4K,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path_input_norm,\n",
    "    input_transform = ('mean','maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    Qscaling = 'RH',\n",
    "    Tscaling = 'BMSE',\n",
    "    LHFscaling = 'LHF_nsDELQ',\n",
    "    hyam=hyam, hybm=hybm, # Arrays to define mid-levels of hybrid vertical coordinate\n",
    "    inp_sub_Qscaling=train_gen_RH.input_transform.sub, # What to subtract from RH inputs\n",
    "    inp_div_Qscaling=train_gen_RH.input_transform.div, # What to divide RH inputs by\n",
    "    inp_sub_Tscaling=train_gen_BMSE.input_transform.sub,\n",
    "    inp_div_Tscaling=train_gen_BMSE.input_transform.div,\n",
    "    inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "    inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_BF = DataGeneratorCI_2DS(\n",
    "    data_fn1 = path_valid_m4K,\n",
    "    data_fn2 = path_valid_p4K,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path_input_norm,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI_2DS(\n",
    "    data_fn1 = path_valid_m4K,\n",
    "    data_fn2 = path_valid_p4K,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path_input_norm,\n",
    "    input_transform = ('mean','maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    Qscaling = 'RH',\n",
    "    Tscaling = 'BMSE',\n",
    "    LHFscaling = 'LHF_nsDELQ',\n",
    "    hyam=hyam, hybm=hybm, # Arrays to define mid-levels of hybrid vertical coordinate\n",
    "    inp_sub_Qscaling=train_gen_RH.input_transform.sub, # What to subtract from RH inputs\n",
    "    inp_div_Qscaling=train_gen_RH.input_transform.div, # What to divide RH inputs by\n",
    "    inp_sub_Tscaling=train_gen_BMSE.input_transform.sub,\n",
    "    inp_div_Tscaling=train_gen_BMSE.input_transform.div,\n",
    "    inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "    inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_BF = DataGeneratorCI_2DS(\n",
    "    data_fn1 = path_test_m4K,\n",
    "    data_fn2 = path_test_p4K,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path_input_norm,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_CI = DataGeneratorCI_2DS(\n",
    "    data_fn1 = path_test_m4K,\n",
    "    data_fn2 = path_test_p4K,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path_input_norm,\n",
    "    input_transform = ('mean','maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    Qscaling = 'RH',\n",
    "    Tscaling = 'BMSE',\n",
    "    LHFscaling = 'LHF_nsDELQ',\n",
    "    hyam=hyam, hybm=hybm, # Arrays to define mid-levels of hybrid vertical coordinate\n",
    "    inp_sub_Qscaling=train_gen_RH.input_transform.sub, # What to subtract from RH inputs\n",
    "    inp_div_Qscaling=train_gen_RH.input_transform.div, # What to divide RH inputs by\n",
    "    inp_sub_Tscaling=train_gen_BMSE.input_transform.sub,\n",
    "    inp_div_Tscaling=train_gen_BMSE.input_transform.div,\n",
    "    inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "    inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(inp,N_layer):\n",
    "    if N_layer>0:\n",
    "        densout = Dense(128, activation='linear')(inp)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    else: dense_out = Dense(120, activation='linear')(inp)\n",
    "    for i in range (N_layer-1):\n",
    "        densout = Dense(128, activation='linear')(densout)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    if N_layer>0: dense_out = Dense(120, activation='linear')(densout)\n",
    "    return tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_models = {}\n",
    "CI_models = {}\n",
    "inp_BF = {}\n",
    "inp_CI = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layer_max = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for ilayer in range(N_layer_max):\n",
    "    \n",
    "    print(ilayer)\n",
    "    \n",
    "    inp_BF[ilayer] = Input(shape=(64,))\n",
    "    BF_models[ilayer] = NN_model(inp_BF[ilayer],ilayer)\n",
    "    \n",
    "    inp_CI[ilayer] = Input(shape=(64,))\n",
    "    CI_models[ilayer] = NN_model(inp_CI[ilayer],ilayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CI_models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for ilayer in range(N_layer_max):\n",
    "    \n",
    "    print(ilayer)\n",
    "    \n",
    "    BF_models[ilayer].compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "    CI_models[ilayer].compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            valid_generator,valid_name = validation_set\n",
    "            results = self.model.evaluate_generator(generator=valid_generator)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,[results]):\n",
    "                valuename = valid_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on 2 layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilayer = 2\n",
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_BF = path_data + '2022_04_13_BF_'+str(ilayer)+'.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test = AdditionalValidationSets([(test_gen_BF,'Test MSE')])\n",
    "mcp_save_BF = ModelCheckpoint(path_BF,save_best_only=True, monitor='val_loss', mode='min')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11518/11518 [==============================] - 1928s 167ms/step - loss: 116.8236 - val_loss: 109.5967\n",
      "Epoch 2/10\n",
      "11518/11518 [==============================] - 930s 81ms/step - loss: 106.1786 - val_loss: 104.2871\n",
      "Epoch 3/10\n",
      "11518/11518 [==============================] - 915s 79ms/step - loss: 102.8043 - val_loss: 101.8122\n",
      "Epoch 4/10\n",
      "11518/11518 [==============================] - 916s 79ms/step - loss: 100.9116 - val_loss: 100.2472\n",
      "Epoch 5/10\n",
      "11518/11518 [==============================] - 916s 79ms/step - loss: 99.4301 - val_loss: 100.5478\n",
      "Epoch 6/10\n",
      "11518/11518 [==============================] - 917s 80ms/step - loss: 98.3662 - val_loss: 98.1168\n",
      "Epoch 7/10\n",
      "11518/11518 [==============================] - 885s 77ms/step - loss: 97.6653 - val_loss: 98.8207\n",
      "Epoch 8/10\n",
      "11518/11518 [==============================] - 900s 78ms/step - loss: 96.9073 - val_loss: 96.6271\n",
      "Epoch 9/10\n",
      "11518/11518 [==============================] - 940s 82ms/step - loss: 96.3519 - val_loss: 99.0366\n",
      "Epoch 10/10\n",
      "11518/11518 [==============================] - 924s 80ms/step - loss: 95.7275 - val_loss: 95.7007\n"
     ]
    }
   ],
   "source": [
    "history_BF = BF_models[ilayer].fit_generator(train_gen_BF, epochs=Nep, \n",
    "                           validation_data = valid_gen_BF,\n",
    "                           callbacks=[mcp_save_BF,check_test,earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_CI = path_data + '2022_04_13_CI_'+str(ilayer)+'.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test = AdditionalValidationSets([(test_gen_CI,'Test MSE')])\n",
    "mcp_save = ModelCheckpoint(path_CI,save_best_only=True, monitor='val_loss', mode='min')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11518/11518 [==============================] - 2584s 224ms/step - loss: 120.2141 - val_loss: 109.7997\n",
      "Epoch 2/10\n",
      "11518/11518 [==============================] - 2618s 227ms/step - loss: 109.6423 - val_loss: 109.7831\n",
      "Epoch 3/10\n",
      "11518/11518 [==============================] - 2668s 232ms/step - loss: 106.6781 - val_loss: 103.8799\n",
      "Epoch 4/10\n",
      "11518/11518 [==============================] - 2517s 219ms/step - loss: 104.6863 - val_loss: 102.3276\n",
      "Epoch 5/10\n",
      " 3201/11518 [=======>......................] - ETA: 12:00 - loss: 104.8467"
     ]
    }
   ],
   "source": [
    "history_CI = CI_models[ilayer].fit_generator(train_gen_CI, epochs=Nep, \n",
    "                           validation_data = valid_gen_CI,\n",
    "                           callbacks=[mcp_save,check_test,earlyStopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train one BF and one CI model per number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_range = np.arange(3,N_layer_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "check_test_BF = AdditionalValidationSets([(test_gen_BF,'Test MSE')])\n",
    "check_test_CI = AdditionalValidationSets([(test_gen_CI,'Test MSE')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_BF = {}\n",
    "history_CI = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers =  3\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 719s 62ms/step - loss: 112.9407 - val_loss: 113.3926\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 701s 61ms/step - loss: 104.0487 - val_loss: 101.9438\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 690s 60ms/step - loss: 100.2444 - val_loss: 100.0977\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 695s 60ms/step - loss: 97.8477 - val_loss: 96.6934\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 680s 59ms/step - loss: 96.5323 - val_loss: 95.6660\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 688s 60ms/step - loss: 95.3004 - val_loss: 94.1513\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 693s 60ms/step - loss: 94.4471 - val_loss: 93.7933\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 694s 60ms/step - loss: 93.6100 - val_loss: 92.5446\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 686s 60ms/step - loss: 92.9777 - val_loss: 92.3854\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 699s 61ms/step - loss: 92.4367 - val_loss: 92.3745\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 684s 59ms/step - loss: 92.1592 - val_loss: 91.6500\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 694s 60ms/step - loss: 91.9267 - val_loss: 91.5038\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 689s 60ms/step - loss: 91.4627 - val_loss: 91.1874\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 693s 60ms/step - loss: 90.8901 - val_loss: 90.6620\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 695s 60ms/step - loss: 90.7414 - val_loss: 89.9434\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 690s 60ms/step - loss: 90.7808 - val_loss: 90.0710\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 686s 60ms/step - loss: 90.2692 - val_loss: 90.9456\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 689s 60ms/step - loss: 90.1136 - val_loss: 90.1217\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 706s 61ms/step - loss: 89.8670 - val_loss: 89.6308\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 699s 61ms/step - loss: 89.6928 - val_loss: 89.9144\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 1780s 154ms/step - loss: 125.7258 - val_loss: 111.0189\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 1797s 156ms/step - loss: 109.3354 - val_loss: 104.0631\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 1820s 158ms/step - loss: 105.6542 - val_loss: 104.8964\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 1828s 159ms/step - loss: 103.4448 - val_loss: 101.9566\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 1858s 161ms/step - loss: 102.7247 - val_loss: 108.0926\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 1831s 159ms/step - loss: 101.9965 - val_loss: 105.1612\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 1842s 160ms/step - loss: 101.0071 - val_loss: 99.3568\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 1832s 159ms/step - loss: 100.7086 - val_loss: 98.8236\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 1833s 159ms/step - loss: 99.9920 - val_loss: 99.8036\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 1812s 157ms/step - loss: 99.9889 - val_loss: 99.2008\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 1805s 157ms/step - loss: 99.3648 - val_loss: 97.9725\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 1824s 158ms/step - loss: 99.5097 - val_loss: 97.9435\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 1839s 160ms/step - loss: 98.9927 - val_loss: 98.5046\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 1820s 158ms/step - loss: 98.9812 - val_loss: 98.2240\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 1815s 158ms/step - loss: 98.7712 - val_loss: 100.6576\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 1863s 162ms/step - loss: 98.7784 - val_loss: 97.7677\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 1814s 158ms/step - loss: 98.3201 - val_loss: 98.8820\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 1825s 158ms/step - loss: 98.6245 - val_loss: 101.2493\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 1827s 159ms/step - loss: 98.6402 - val_loss: 98.2188\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 1815s 158ms/step - loss: 98.1112 - val_loss: 97.1916\n",
      "Number of layers =  4\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 717s 62ms/step - loss: 112.8484 - val_loss: 105.5443\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 719s 62ms/step - loss: 102.6453 - val_loss: 111.7834\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 718s 62ms/step - loss: 98.7269 - val_loss: 96.4199\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 717s 62ms/step - loss: 96.0918 - val_loss: 94.1768\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 714s 62ms/step - loss: 95.1092 - val_loss: 92.8418\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 711s 62ms/step - loss: 93.4100 - val_loss: 92.2072\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 714s 62ms/step - loss: 92.7066 - val_loss: 91.3248\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 706s 61ms/step - loss: 92.0428 - val_loss: 94.4979\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 713s 62ms/step - loss: 91.3370 - val_loss: 94.5350\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 713s 62ms/step - loss: 90.8160 - val_loss: 89.9788\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 717s 62ms/step - loss: 90.3918 - val_loss: 90.8655\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 714s 62ms/step - loss: 90.0854 - val_loss: 89.5636\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 695s 60ms/step - loss: 89.8505 - val_loss: 89.1813\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 709s 62ms/step - loss: 89.6332 - val_loss: 89.6093\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 714s 62ms/step - loss: 89.5277 - val_loss: 88.3872\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 765s 66ms/step - loss: 89.1680 - val_loss: 88.5256\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 1218s 106ms/step - loss: 88.7471 - val_loss: 88.1442\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 820s 71ms/step - loss: 88.6898 - val_loss: 88.1152\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 747s 65ms/step - loss: 88.6381 - val_loss: 88.3366\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 713s 62ms/step - loss: 88.3374 - val_loss: 88.7609\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 1834s 159ms/step - loss: 131.8161 - val_loss: 144.6623\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 1869s 162ms/step - loss: 111.4559 - val_loss: 104.6846\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 1859s 161ms/step - loss: 111.0466 - val_loss: 109.4475\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 1849s 161ms/step - loss: 104.3091 - val_loss: 101.7447\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 1850s 161ms/step - loss: 102.3360 - val_loss: 102.1171\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 1855s 161ms/step - loss: 101.6587 - val_loss: 102.1644\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 1835s 159ms/step - loss: 100.3344 - val_loss: 99.3271\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 1859s 161ms/step - loss: 99.8713 - val_loss: 99.7299\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 1846s 160ms/step - loss: 100.2232 - val_loss: 98.4247\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 1890s 164ms/step - loss: 100.8788 - val_loss: 100.6592\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 1823s 158ms/step - loss: 99.2818 - val_loss: 97.7727\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 1835s 159ms/step - loss: 99.7870 - val_loss: 98.1536\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 1866s 162ms/step - loss: 98.5540 - val_loss: 96.6629\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 1836s 159ms/step - loss: 98.6647 - val_loss: 97.1840\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 1856s 161ms/step - loss: 98.0369 - val_loss: 98.0487\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 1843s 160ms/step - loss: 97.8211 - val_loss: 97.5567\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 1867s 162ms/step - loss: 97.9976 - val_loss: 96.5430\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 1829s 159ms/step - loss: 97.8709 - val_loss: 99.1253\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 1829s 159ms/step - loss: 98.1956 - val_loss: 98.1309\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 1847s 160ms/step - loss: 98.8743 - val_loss: 98.8066\n",
      "Number of layers =  5\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 729s 63ms/step - loss: 110.5835 - val_loss: 103.7017\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 727s 63ms/step - loss: 100.9251 - val_loss: 96.6492\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 728s 63ms/step - loss: 96.7151 - val_loss: 93.8708\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 734s 64ms/step - loss: 95.0041 - val_loss: 94.3549\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 727s 63ms/step - loss: 93.7290 - val_loss: 91.1724\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 727s 63ms/step - loss: 92.4650 - val_loss: 91.7422\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 733s 64ms/step - loss: 91.4379 - val_loss: 93.2705\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 732s 64ms/step - loss: 90.9874 - val_loss: 91.1092\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 734s 64ms/step - loss: 90.2958 - val_loss: 94.7584\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 736s 64ms/step - loss: 89.8302 - val_loss: 90.9236\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 733s 64ms/step - loss: 89.6211 - val_loss: 92.4158\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 712s 62ms/step - loss: 89.0757 - val_loss: 88.2146\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 738s 64ms/step - loss: 88.9141 - val_loss: 88.0209\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 727s 63ms/step - loss: 88.6449 - val_loss: 88.7581\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 736s 64ms/step - loss: 88.4629 - val_loss: 94.2294\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 739s 64ms/step - loss: 88.1108 - val_loss: 87.3616\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 732s 64ms/step - loss: 88.0217 - val_loss: 87.9710\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 736s 64ms/step - loss: 87.7614 - val_loss: 86.6363\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 730s 63ms/step - loss: 87.7296 - val_loss: 90.5027\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 729s 63ms/step - loss: 87.6182 - val_loss: 87.3642\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 1873s 163ms/step - loss: 136.0433 - val_loss: 115.8189\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 1823s 158ms/step - loss: 113.5772 - val_loss: 109.1466\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 1833s 159ms/step - loss: 106.4176 - val_loss: 101.8347\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 1833s 159ms/step - loss: 103.8950 - val_loss: 107.9636\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 1853s 161ms/step - loss: 105.5806 - val_loss: 109.3136\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 1830s 159ms/step - loss: 101.9994 - val_loss: 99.5302\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 1831s 159ms/step - loss: 100.2848 - val_loss: 99.5618\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 1800s 156ms/step - loss: 100.0428 - val_loss: 98.4690\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 1863s 162ms/step - loss: 100.6901 - val_loss: 98.5114\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 1823s 158ms/step - loss: 100.5686 - val_loss: 122.1954\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 1865s 162ms/step - loss: 100.8598 - val_loss: 97.3086\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 1790s 155ms/step - loss: 98.8827 - val_loss: 97.2655\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 1870s 162ms/step - loss: 98.1564 - val_loss: 96.7526\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 1862s 162ms/step - loss: 101.6771 - val_loss: 101.9016\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 1885s 164ms/step - loss: 99.8467 - val_loss: 96.6608\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 1860s 161ms/step - loss: 97.8921 - val_loss: 96.6979\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 1847s 160ms/step - loss: 97.8574 - val_loss: 97.2390\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 1868s 162ms/step - loss: 97.8086 - val_loss: 96.6316\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 1852s 161ms/step - loss: 98.7116 - val_loss: 104.2835\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 1936s 168ms/step - loss: 97.2413 - val_loss: 96.0783\n",
      "Number of layers =  6\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 763s 66ms/step - loss: 112.1672 - val_loss: 112.7866\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 754s 65ms/step - loss: 100.4542 - val_loss: 96.5382\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 765s 66ms/step - loss: 96.8484 - val_loss: 94.4261\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 757s 66ms/step - loss: 94.2833 - val_loss: 91.7282\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 760s 66ms/step - loss: 92.7102 - val_loss: 93.6569\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 753s 65ms/step - loss: 92.0525 - val_loss: 91.2702\n",
      "Epoch 7/20\n",
      "11518/11518 [==============================] - 754s 65ms/step - loss: 91.2827 - val_loss: 89.3962\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 752s 65ms/step - loss: 90.3177 - val_loss: 90.5874\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 749s 65ms/step - loss: 90.0016 - val_loss: 88.9780\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 759s 66ms/step - loss: 89.5500 - val_loss: 90.9371\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 754s 65ms/step - loss: 88.9193 - val_loss: 87.3453\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 767s 67ms/step - loss: 88.7162 - val_loss: 88.4129\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 779s 68ms/step - loss: 88.4038 - val_loss: 87.5077\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 762s 66ms/step - loss: 88.0866 - val_loss: 87.8076\n",
      "Epoch 15/20\n",
      "11518/11518 [==============================] - 766s 67ms/step - loss: 88.0270 - val_loss: 87.6306\n",
      "Epoch 16/20\n",
      "11518/11518 [==============================] - 772s 67ms/step - loss: 87.8503 - val_loss: 87.3159\n",
      "Epoch 17/20\n",
      "11518/11518 [==============================] - 737s 64ms/step - loss: 87.6338 - val_loss: 88.2317\n",
      "Epoch 18/20\n",
      "11518/11518 [==============================] - 728s 63ms/step - loss: 88.1128 - val_loss: 86.9541\n",
      "Epoch 19/20\n",
      "11518/11518 [==============================] - 745s 65ms/step - loss: 87.3941 - val_loss: 86.6898\n",
      "Epoch 20/20\n",
      "11518/11518 [==============================] - 742s 64ms/step - loss: 87.0551 - val_loss: 87.7865\n",
      "Epoch 1/20\n",
      "11518/11518 [==============================] - 1864s 162ms/step - loss: 135.6977 - val_loss: 121.5459\n",
      "Epoch 2/20\n",
      "11518/11518 [==============================] - 1890s 164ms/step - loss: 120.3390 - val_loss: 111.6525\n",
      "Epoch 3/20\n",
      "11518/11518 [==============================] - 1899s 165ms/step - loss: 108.6470 - val_loss: 103.6573\n",
      "Epoch 4/20\n",
      "11518/11518 [==============================] - 2075s 180ms/step - loss: 115.6963 - val_loss: 125.5282\n",
      "Epoch 5/20\n",
      "11518/11518 [==============================] - 1827s 159ms/step - loss: 108.9571 - val_loss: 108.4378\n",
      "Epoch 6/20\n",
      "11518/11518 [==============================] - 1796s 156ms/step - loss: 106.3436 - val_loss: 100.5633\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11518/11518 [==============================] - 1816s 158ms/step - loss: 102.2888 - val_loss: 100.0648\n",
      "Epoch 8/20\n",
      "11518/11518 [==============================] - 2009s 174ms/step - loss: 102.5505 - val_loss: 98.9804\n",
      "Epoch 9/20\n",
      "11518/11518 [==============================] - 1905s 165ms/step - loss: 100.4476 - val_loss: 99.2355\n",
      "Epoch 10/20\n",
      "11518/11518 [==============================] - 2074s 180ms/step - loss: 99.8990 - val_loss: 106.7556\n",
      "Epoch 11/20\n",
      "11518/11518 [==============================] - 1697s 147ms/step - loss: 99.6918 - val_loss: 97.7775\n",
      "Epoch 12/20\n",
      "11518/11518 [==============================] - 1854s 161ms/step - loss: 99.1390 - val_loss: 97.6088\n",
      "Epoch 13/20\n",
      "11518/11518 [==============================] - 1810s 157ms/step - loss: 99.3583 - val_loss: 97.1940\n",
      "Epoch 14/20\n",
      "11518/11518 [==============================] - 1829s 159ms/step - loss: 99.1550 - val_loss: 96.8473\n",
      "Epoch 15/20\n",
      "11517/11518 [============================>.] - ETA: 0s - loss: 98.7425"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ilayer in updated_range:\n",
    "    \n",
    "    print('Number of layers = ',ilayer)\n",
    "    \n",
    "    path_BF = path_data + '2022_04_14_BF_'+str(ilayer)+'.hdf5'\n",
    "    mcp_save = ModelCheckpoint(path_BF,save_best_only=True, monitor='val_loss', mode='min')\n",
    "    history_BF[ilayer] = BF_models[ilayer].fit_generator(train_gen_BF, epochs=Nep, \n",
    "                                                         validation_data = valid_gen_BF,\n",
    "                                                         callbacks=[mcp_save,earlyStopping])\n",
    "    \n",
    "    path_CI = path_data + '2022_04_14_CI_'+str(ilayer)+'.hdf5'\n",
    "    mcp_save = ModelCheckpoint(path_CI,save_best_only=True, monitor='val_loss', mode='min')\n",
    "    history_CI[ilayer] = CI_models[ilayer].fit_generator(train_gen_CI, epochs=Nep, \n",
    "                                                         validation_data = valid_gen_CI,\n",
    "                                                         callbacks=[mcp_save,earlyStopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the best architecture for BF/CI neural nets, logarithmically increase data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Final evaluation on training/validation/test set and make plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate for each # of layers, for each training/validation/test set on joint/-4K/+4K datasets. This should amount to 90 evaluations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different training data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
