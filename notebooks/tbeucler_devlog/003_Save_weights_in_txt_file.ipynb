{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 2/5/2019 - The goal of this notebook is to save a model's weights, saved in a .h5 file, to a .txt file so that it can be used by CAM.  \n",
    "The notebook closely follows the function save_weights.py coded by Stephan Rasp:  \n",
    "https://github.com/raspstephan/CBRAIN-CAM/blob/master/save_weights.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/t/Tom.Beucler/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from cbrain.imports import *\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "metrics_dict = dict([(f.__name__, f) for f in all_metrics])\n",
    "get_custom_objects().update(metrics_dict)\n",
    "import h5py\n",
    "import os, sys\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfm\n",
    "fmt = '%.6e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Tom.Beucler/SPCAM_PHYS\n",
      "32_col_lgsc_12m_train_features.nc\r\n",
      "32_col_lgsc_12m_train_norm.nc\r\n",
      "32_col_lgsc_12m_train_oldnorm.nc\r\n",
      "32_col_lgsc_12m_train_shuffle_features.nc\r\n",
      "32_col_lgsc_12m_train_shuffle_targets.nc\r\n",
      "32_col_lgsc_12m_train_targets.nc\r\n",
      "32_col_lgsc_12m_valid_features.nc\r\n",
      "32_col_lgsc_12m_valid_shuffle_features.nc\r\n",
      "32_col_lgsc_12m_valid_shuffle_targets.nc\r\n",
      "32_col_lgsc_12m_valid_targets.nc\r\n",
      "32_col_lgsc_1m_train_features.nc\r\n",
      "32_col_lgsc_1m_train_norm.nc\r\n",
      "32_col_lgsc_1m_train_oldnorm.nc\r\n",
      "32_col_lgsc_1m_train_shuffle_features.nc\r\n",
      "32_col_lgsc_1m_train_shuffle_targets.nc\r\n",
      "32_col_lgsc_1m_train_targets.nc\r\n",
      "32_col_lgsc_1m_valid_features.nc\r\n",
      "32_col_lgsc_1m_valid_shuffle_features.nc\r\n",
      "32_col_lgsc_1m_valid_shuffle_targets.nc\r\n",
      "32_col_lgsc_1m_valid_targets.nc\r\n",
      "HDF5_DATA\r\n",
      "local\r\n",
      "TXT_DATA\r\n"
     ]
    }
   ],
   "source": [
    "TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/sp32fbp_andkua/'\n",
    "%cd $TRAINDIR\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 2/5/2019 - Subtlties with load_model discussed here:  \n",
    "https://github.com/keras-team/keras/issues/4871  \n",
    "Ideally, I would implement get_config() in the custom layer, then use custom_objects to pass a dictionary to load_model.  \n",
    "In practice, I'll redefine my layers and my model below and just load the weights.  \n",
    "  \n",
    "Careful: The mass and enthalpy conservation layers depend on the input shape, so make sure to use the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsub.shape= (304,)\n",
      "fdiv.shape= (304,)\n",
      "normq.shape= (158,)\n",
      "hyai.shape= (31,)\n",
      "hybi.shape= (31,)\n"
     ]
    }
   ],
   "source": [
    "PREFIX = '32_col_lgsc_12m_'\n",
    "# 1) Open the file containing the normalization of the targets\n",
    "ds = xr.open_dataset(TRAINDIR + PREFIX + 'train_norm.nc')\n",
    "# 2) Open the pickle files containing the pressure converters\n",
    "with open(os.path.join('/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/cbrain', 'hyai_hybi.pkl'), 'rb') as f:\n",
    "            hyai, hybi = pickle.load(f)\n",
    "# 3) Define fsub, fdiv, normq\n",
    "fsub = ds.feature_means.values\n",
    "fdiv = ds.feature_stds_by_var.values\n",
    "normq = ds.target_conv.values\n",
    "print('fsub.shape=',fsub.shape)\n",
    "print('fdiv.shape=',fdiv.shape)\n",
    "print('normq.shape=',normq.shape)\n",
    "print('hyai.shape=',hyai.shape)\n",
    "print('hybi.shape=',hybi.shape)\n",
    "\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgb - 2/5/2019 - Adapated the mass conservation layer to new input format\n",
    "class MasConsLay(Layer):\n",
    "    \n",
    "    def __init__(self, fsub, fdiv, normq, hyai, hybi, output_dim, **kwargs):\n",
    "        self.fsub = fsub # Subtraction for normalization of inputs \n",
    "        self.fdiv = fdiv # Division for normalization of inputs\n",
    "        self.normq = normq # Normalization of output's water concentration\n",
    "        self.hyai = hyai # CAM constants to calculate d_pressure\n",
    "        self.hybi = hybi # CAM constants to calculate d_pressure\n",
    "        self.output_dim = output_dim # Dimension of output\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)  # Be sure to call this somewhere!\n",
    "        \n",
    "    # tgb - 2/6/2019 - following https://github.com/keras-team/keras/issues/4871\n",
    "    def get_config(self):\n",
    "        config = {'fsub': list(self.fsub), 'fdiv': list(self.fdiv),\n",
    "                  'normq': list(self.normq), 'hyai': list(self.hyai),\n",
    "                  'hybi': list(self.hybi), 'output_dim': list(self.output_dim)}\n",
    "        base_config = super(MasConsLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "    def call(self, arrs):\n",
    "        # arrs (for arrays) is a list with \n",
    "        # [inputs=inp and the output of the previous layer=densout]\n",
    "        # inputs will be [n_sample, 304 = 30*10+4] with\n",
    "        # [QBP, QCBP, QIBP, TBP, VBP, Qdt_adiabatic, QCdt_adiabatic, QIdt_adiabatic,\n",
    "        # Tdt_adiabatic, Vdt_adiabatic, PS, SOLIN, SHFLX, LHFLX]\n",
    "        # outputs of the previous dense layer will be [n_samples, 124 = 30*4+6-2] with\n",
    "        # [DELQ\\{PHQ AT LOWEST LVL}, DELCLDLIQ, DELCLDICE, \n",
    "        # TPHYSTND\\{TPHYSTND AT LOWEST LVL}, FSNT, FSNS, FLNT, FLNS, PRECT, PRECTEND]\n",
    "        \n",
    "        # Split between the inputs inp & the output of the densely connected\n",
    "        # neural network, densout\n",
    "        inp, densout = arrs\n",
    "        \n",
    "        # 0) Constants\n",
    "        G = 9.80616; # Reference gravity constant [m.s-2]\n",
    "        L_V = 2.501e6; # Latent heat of vaporization of water [W.kg-1]\n",
    "        P0 = 1e5; # Reference surface pressure [Pa]\n",
    "        \n",
    "        # 1) Get non-dimensional pressure differences (p_tilde above)\n",
    "        # In the input vector, PS is the 151st element after \n",
    "        # the first elements = [QBP, ..., VBP with shape 30*5=150]\n",
    "        PS = tfm.add( tfm.multiply( inp[:,300], self.fdiv[300]), self.fsub[300])\n",
    "        # Reference for calculation of d_pressure is cbrain/models.py (e.g. QLayer)\n",
    "        P = tfm.add( tfm.multiply( P0, self.hyai), \\\n",
    "                    tfm.multiply( PS[:,None], self.hybi))\n",
    "        dP = tfm.subtract( P[:, 1:], P[:, :-1])\n",
    "        # norm_output = dp_norm * L_V/G so dp_norm = norm_output * G/L_V\n",
    "        dP_NORM = tfm.divide( \\\n",
    "                             tfm.multiply(self.normq[:30], \\\n",
    "                                   G), L_V)\n",
    "        # dp_tilde = dp/dp_norm\n",
    "        # Wondering about broadcasting here...\n",
    "        # tf.div or simply \\ would support broadcasting \n",
    "        dP_TILD = tfm.divide( dP, dP_NORM)\n",
    "        \n",
    "        # 2) Calculate cloud water vertical integral from level 1 to level 30\n",
    "        # The indices are tricky here because we are missing del(q_v)@(level 30)\n",
    "        # so e.g. q_liq@(level 1) is the 30th element of the output of the \n",
    "        # previous dense layer\n",
    "        CLDVEC = tfm.multiply( dP_TILD, \\\n",
    "                                  tfm.add( densout[:, 29:59], densout[:, 59:89]))\n",
    "        CLDINT = tfm.reduce_sum( CLDVEC, axis=1)\n",
    "        \n",
    "        # 3) Calculate water vapor vertical integral from level 1 to level 29\n",
    "        VAPVEC = tfm.multiply( dP_TILD[:, :29], \\\n",
    "                                  densout[:, :29])\n",
    "        VAPINT = tfm.reduce_sum( VAPVEC, axis=1)\n",
    "        \n",
    "        # 4) Calculate forcing on the right-hand side (Net Evaporation-Precipitation)\n",
    "        # E-P is already normalized to units W.m-2 in the output vector\n",
    "        # so all we need to do is input-unnormalize LHF that is taken from the input vector\n",
    "        LHF = tfm.add( tfm.multiply( inp[:,303], self.fdiv[303]), self.fsub[303])\n",
    "        # Note that total precipitation = PRECT + 1e-3*PRECTEND in the CAM model\n",
    "        # PRECTEND already multiplied by 1e-3 in output vector so no need to redo it\n",
    "        PREC = tfm.add( densout[:, 152], densout[:, 153])\n",
    "        \n",
    "        # 5) Infer water vapor tendency at level 30 as a residual\n",
    "        # Composing tfm.add 3 times because not sure how to use tfm.add_n\n",
    "        DELQV30 = tfm.divide( \\\n",
    "                             tfm.add( tfm.add( tfm.add (\\\n",
    "                                                        LHF, tfm.negative(PREC)), \\\n",
    "                                              tfm.negative(CLDINT)), \\\n",
    "                                     tfm.negative(VAPINT)), \\\n",
    "                             dP_TILD[:, 29])\n",
    "        \n",
    "        # 6) Concatenate the water tendencies with the newly inferred tendency\n",
    "        # to get the final vector out of shape (#samples,125) with\n",
    "        # [DELQ, DELCLDLIQ, DELCLDICE, \n",
    "        # TPHYSTND\\{TPHYSTND AT SURFACE}, FSNT, FSNS, FLNT, FLNS, PRECT PRECTEND]\n",
    "        # Uses https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "        DELQV30 = tf.expand_dims(DELQV30,1) # Adds dimension=1 to axis=1\n",
    "        out = tf.concat([densout[:, :29], DELQV30, densout[:, 29:]], 1)\n",
    "        return out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.output_dim) # The output has size 125=30*4+6-1\n",
    "    # and is ready to be fed to the energy conservation layer\n",
    "    # before we reach the total number of outputs = 126\n",
    "    \n",
    "# tgb - 2/5/2019 - Change to adapt to new input format\n",
    "class EntConsLay(Layer):\n",
    "    \n",
    "    def __init__(self, fsub, fdiv, normq, hyai, hybi, output_dim, **kwargs):\n",
    "        self.fsub = fsub # Subtraction for normalization of inputs \n",
    "        self.fdiv = fdiv # Division for normalization of inputs\n",
    "        self.normq = normq # Normalization of output's water concentration\n",
    "        self.hyai = hyai # CAM constants to calculate d_pressure\n",
    "        self.hybi = hybi # CAM constants to calculate d_pressure\n",
    "        self.output_dim = output_dim # Dimension of output\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)  # Be sure to call this somewhere!\n",
    "        \n",
    "    # tgb - 2/6/2019 - following https://github.com/keras-team/keras/issues/4871\n",
    "    def get_config(self):\n",
    "        config = {'fsub': list(self.fsub), 'fdiv': list(self.fdiv),\n",
    "                  'normq': list(self.normq), 'hyai': list(self.hyai),\n",
    "                  'hybi': list(self.hybi), 'output_dim': list(self.output_dim)}\n",
    "        base_config = super(MasConsLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "        \n",
    "    def call(self, arrs):\n",
    "        # arrs (for arrays) is a list with \n",
    "        # [inputs=inp and the output of the previous layer=massout]\n",
    "        # inputs will be [n_sample, 304 = 30*10+4] with\n",
    "        # [QBP, QCBP, QIBP, TBP, VBP, Qdt_adiabatic, QCdt_adiabatic, QIdt_adiabatic,\n",
    "        # Tdt_adiabatic, Vdt_adiabatic, PS, SOLIN, SHFLX, LHFLX]\n",
    "        # outputs of the previous dense layer will be [n_samples, 157 = 30*5+8-1] with\n",
    "        # [DELQ, DELCLDLIQ, DELCLDICE, \n",
    "        # TPHYSTND\\{TPHYSTND AT LOWEST LVL}, DTVKE,\n",
    "        # FSNT, FSNS, FLNT, FLNS, PRECT, PRECTEND, PRECST, PRECSTEN]\n",
    "        \n",
    "        # Split between the inputs inp & the output of the densely connected\n",
    "        # neural network, massout\n",
    "        inp, massout = arrs\n",
    "        \n",
    "        # 0) Constants\n",
    "        G = 9.80616; # Reference gravity constant [m.s-2]\n",
    "        L_F = 3.337e5; # Latent heat of fusion of water [W.kg-1]\n",
    "        L_V = 2.501e6; # Latent heat of vaporization of water [W.kg-1]\n",
    "        P0 = 1e5; # Reference surface pressure [Pa]\n",
    "        \n",
    "        # 1) Get non-dimensional pressure differences (p_tilde above)\n",
    "        # In the input vector, PS is the 151st element after \n",
    "        # the first elements = [QBP, ..., VBP with shape 30*5=150]\n",
    "        PS = tfm.add( tfm.multiply( inp[:,300], self.fdiv[300]), self.fsub[300])\n",
    "        # Reference for calculation of d_pressure is cbrain/models.py (e.g. QLayer)\n",
    "        P = tfm.add( tfm.multiply( P0, self.hyai), \\\n",
    "                    tfm.multiply( PS[:,None], self.hybi))\n",
    "        dP = tfm.subtract( P[:, 1:], P[:, :-1])\n",
    "        # norm_output = dp_norm * L_V/G so dp_norm = norm_output * G/L_V\n",
    "        dP_NORM = tfm.divide( \\\n",
    "                             tfm.multiply(self.normq[:30], \\\n",
    "                                          G),\\\n",
    "                             L_V)\n",
    "        # dp_tilde = dp/dp_norm\n",
    "        dP_TILD = tfm.divide( dP, dP_NORM)\n",
    "        \n",
    "        # 2) Calculate net energy input from phase change and precipitation\n",
    "        # PHAS = Lf/Lv*((PRECST+PRECSTEN)-(PRECT+PRECTEND))\n",
    "        PHAS = tfm.divide( tfm.multiply( tfm.subtract(\\\n",
    "                                                      tfm.add( massout[:,155], massout[:,156]),\\\n",
    "                                                      tfm.add( massout[:,153], massout[:,154])),\\\n",
    "                                        L_F),\\\n",
    "                          L_V)\n",
    "        \n",
    "        # 3) Calculate net energy input from radiation, sensible heat flux and turbulent KE\n",
    "        # 3.1) RAD = FSNT-FSNS-FLNT+FLNS\n",
    "        RAD = tfm.add(\\\n",
    "                      tfm.subtract( massout[:,149], massout[:,150]),\\\n",
    "                      tfm.subtract( massout[:,152], massout[:,151]))\n",
    "        # 3.2) Unnormalize sensible heat flux\n",
    "        SHF = tfm.add( tfm.multiply( inp[:,302], self.fdiv[302]), self.fsub[302])\n",
    "        # 3.3) Net turbulent kinetic energy dissipative heating is the column-integrated \n",
    "        # turbulent kinetic energy energy dissipative heating\n",
    "        KEDVEC = tfm.multiply( dP_TILD, massout[:, 119:149])\n",
    "        KEDINT = tfm.reduce_sum( KEDVEC, axis=1)\n",
    "        \n",
    "        # 4) Calculate tendency of normalized column water vapor due to phase change\n",
    "        # 4.1) Unnormalize latent heat flux\n",
    "        LHF = tfm.add( tfm.multiply( inp[:,303], self.fdiv[303]), self.fsub[303])\n",
    "        # 4.2) Column water vapor is the column integral of specific humidity\n",
    "        PHQVEC = tfm.multiply( dP_TILD, massout[:, :30])\n",
    "        PHQINT = tfm.reduce_sum( PHQVEC, axis=1)\n",
    "        # 4.3) Multiply by L_S/L_V to normalize (explanation above)\n",
    "        SPDQINT = tfm.divide( tfm.multiply( tfm.subtract(\\\n",
    "                                                         PHQINT, LHF),\\\n",
    "                                           L_S),\\\n",
    "                             L_V)\n",
    "        \n",
    "        # 5) Same operation for liquid water tendency but multiplied by L_F/L_V\n",
    "        SPDQCINT = tfm.divide( tfm.multiply(\\\n",
    "                                            tfm.reduce_sum(\\\n",
    "                                                           tfm.multiply( dP_TILD, massout[:, 30:60]),\\\n",
    "                                                           axis=1),\\\n",
    "                                            L_F),\\\n",
    "                              L_V)\n",
    "        \n",
    "        # 6) Same operation for temperature but only integrate from level 1 to level 29\n",
    "        DTINT = tfm.reduce_sum( tfm.multiply( dP_TILD[:, :29], massout[:, 90:119]), axis=1)\n",
    "\n",
    "        # 7) Now calculate dT30 as a residual\n",
    "        dT30 = tfm.divide(tfm.add(tfm.add(tfm.add(tfm.add(tfm.add(tfm.add(\\\n",
    "                                                                          PHAS,RAD),\\\n",
    "                                                                  SHF),\\\n",
    "                                                          KEDINT),\\\n",
    "                                                  tfm.negative( SPDQINT)),\\\n",
    "                                          tfm.negative( SPDQCINT)),\\\n",
    "                                  tfm.negative( DTINT)),\\\n",
    "                          dP_TILD[:, 29])\n",
    "        dT30 = tf.expand_dims(dT30,1)\n",
    "\n",
    "        out = tf.concat([massout[:, :119], dT30, massout[:, 119:]], 1)\n",
    "        return out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.output_dim)\n",
    "    # and is ready to be used in the cost function\n",
    "    \n",
    "# tgb - 2/6/2019 - Adding get    \n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "metrics_dict = dict([(f.__name__, f) for f in all_metrics])\n",
    "get_custom_objects().update(metrics_dict)\n",
    "get_custom_objects().update({\n",
    "    'MasConsLay': MasConsLay,\n",
    "    'EntConsLay': EntConsLay,\n",
    "    })\n",
    "from configargparse import ArgParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Properly save the conserving models with get_config() to avoid having to redefine them when loading them  \n",
    "tgb - 2/5/2019 - Now ready to save the weights and the norm to .txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./TXT_DATA/mod_cons_5dens/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 6 required positional arguments: 'fsub', 'fdiv', 'normq', 'hyai', 'hybi', and 'output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5fcd51f397e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./HDF5_DATA/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    143\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 144\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2511\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2512\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                                            list(custom_objects.items())))\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \"\"\"\n\u001b[0;32m-> 1271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 6 required positional arguments: 'fsub', 'fdiv', 'normq', 'hyai', 'hybi', and 'output_dim'"
     ]
    }
   ],
   "source": [
    "exp_name = 'mod_cons_5dens'\n",
    "save_dir = f'./TXT_DATA/'+exp_name+'/'\n",
    "print(save_dir)\n",
    "if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "model = load_model(f'./HDF5_DATA/'+exp_name+'.h5')\n",
    "model.save_weights(save_dir + 'weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = save_dir + 'weights.h5'\n",
    "weights = []; biases = []\n",
    "with h5py.File(weight_file, 'r') as f:\n",
    "    layer_names = [n.decode('utf8') for n in f.attrs['layer_names']\n",
    "                   if 'dense' in n.decode('utf8')]\n",
    "    for il, l in enumerate(layer_names):\n",
    "        g = f[l]\n",
    "        w = g[l + '/kernel:0'][:]\n",
    "        b = g[l + '/bias:0'][:]\n",
    "        weights.append(w); biases.append(b)\n",
    "        np.savetxt(save_dir+f'/layer{il+1}_kernel.txt', w.T, fmt=fmt,\n",
    "                   delimiter=',')\n",
    "        np.savetxt(save_dir + f'/layer{il+1}_bias.txt', b.reshape(1, -1),\n",
    "                   fmt=fmt, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_path = TRAINDIR + '32_col_lgsc_1m_train_norm.nc'\n",
    "with nc.Dataset(norm_path) as ds:\n",
    "    np.savetxt(\n",
    "        save_dir + '/inp_means.txt', ds['feature_means'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/inp_stds.txt', ds['feature_stds'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/inp_mins.txt', ds['feature_mins'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/inp_maxs.txt', ds['feature_maxs'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/outp_mins.txt', ds['target_mins'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/outp_maxs.txt', ds['target_maxs'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    # tgb - 2/6/2019 - Adding output conversion file to not have to hardcode output unnormalization\n",
    "    # in the CAM code\n",
    "    np.savetxt(\n",
    "        save_dir + '/outp_conv.txt', ds['target_conv'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/inp_stds_by_var.txt', ds['feature_stds_by_var'][:].reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')\n",
    "    np.savetxt(\n",
    "        save_dir + '/inp_max_rs.txt',\n",
    "        np.maximum(ds['feature_stds_by_var'][:],\n",
    "                   ds['feature_maxs'][:] - ds['feature_mins'][:]).reshape(1, -1),\n",
    "        fmt=fmt, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
