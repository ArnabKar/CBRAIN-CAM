{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2020\n",
    "- Adapting Ankitesh's notebook that builds and train a \"brute-force\" network to David Walling's hyperparameter search  \n",
    "- Adding the option to choose between aquaplanet and real-geography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "from cbrain.climate_invariant import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "\n",
    "# Comet path below\n",
    "# coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "#                     decode_times=False)\n",
    "\n",
    "# GP path below\n",
    "path_0K = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/fluxbypass_aqua/'\n",
    "coor = xr.open_dataset(path_0K+\"AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-09-02-00000.nc\")\n",
    "\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet path below\n",
    "# TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "# path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "\n",
    "# GP path below\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path = '/export/nfs0home/tbeucler/CBRAIN-CAM/cbrain/'\n",
    "path_nnconfig = '/export/nfs0home/tbeucler/CBRAIN-CAM/nn_config/'\n",
    "\n",
    "# Load hyam and hybm to calculate pressure field in SPCAM\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "\n",
    "# Scale dictionary to convert the loss to W/m2\n",
    "scale_dict = load_pickle(path_nnconfig+'scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data generator class for the climate-invariant network. Calculates the physical rescalings needed to make the NN climate-invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorClimInv(DataGenerator):\n",
    "    \n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "             norm_fn=None, input_transform=None, output_transform=None,\n",
    "             batch_size=1024, shuffle=True, xarray=False, var_cut_off=None,\n",
    "             rh_trans=True,t2tns_trans=True,\n",
    "             lhflx_trans=True,\n",
    "             scaling=True,interpolate=True,\n",
    "             hyam=None,hybm=None,                 \n",
    "             inp_subRH=None,inp_divRH=None,\n",
    "             inp_subTNS=None,inp_divTNS=None,\n",
    "             lev=None, interm_size=40,\n",
    "             lower_lim=6,\n",
    "             is_continous=True,Tnot=5,\n",
    "                mode='train',portion=1):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.rh_trans = rh_trans\n",
    "        self.t2tns_trans = t2tns_trans\n",
    "        self.lhflx_trans = lhflx_trans\n",
    "        self.inp_shape = 64\n",
    "        self.mode=mode\n",
    "        super().__init__(data_fn, input_vars,output_vars,norm_fn,input_transform,output_transform,\n",
    "                        batch_size,shuffle,xarray,var_cut_off) ## call the base data generator\n",
    "        self.inp_sub = self.input_transform.sub\n",
    "        self.inp_div = self.input_transform.div\n",
    "        if rh_trans:\n",
    "            self.qv2rhLayer = QV2RHNumpy(self.inp_sub,self.inp_div,inp_subRH,inp_divRH,hyam,hybm)\n",
    "        \n",
    "        if lhflx_trans:\n",
    "            self.lhflxLayer = LhflxTransNumpy(self.inp_sub,self.inp_div,hyam,hybm)\n",
    "            \n",
    "        if t2tns_trans:\n",
    "            self.t2tnsLayer = T2TmTNSNumpy(self.inp_sub,self.inp_div,inp_subTNS,inp_divTNS,hyam,hybm)\n",
    "            \n",
    "        if scaling:\n",
    "            self.scalingLayer = ScalingNumpy(hyam,hybm)\n",
    "            self.inp_shape += 1\n",
    "                    \n",
    "        if interpolate:\n",
    "            self.interpLayer = InterpolationNumpy(lev,is_continous,Tnot,lower_lim,interm_size)\n",
    "            self.inp_shape += interm_size*2 + 4 + 30 ## 4 same as 60-64 and 30 for lev_tilde.size\n",
    "            \n",
    "        # tgb - 7/9/2020 - Test only training on a subset of the data determined by portion\n",
    "        self.portion = portion\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # If portion<1, only look at a subset of the data by putting an upper bound on index\n",
    "        if self.portion<1: index = index % round(1/self.portion)\n",
    "        elif self.portion>1: print('Setting portion=1 because portion>1')\n",
    "        elif self.portion<0: print('Setting portion=1 because portion<0')\n",
    "        \n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result) \n",
    "            \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "            \n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        if self.mode=='val':\n",
    "            return xr.DataArray(X_result), xr.DataArray(Y)\n",
    "        return X_result,Y\n",
    "    \n",
    "    ##transforms the input data into the required format, take the unnormalized dataset\n",
    "    def transform(self,X):\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result)  \n",
    "        \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "\n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        return X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP paths below\n",
    "path_aquaplanet = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path_realgeography = ''\n",
    "\n",
    "# Comet paths below\n",
    "# path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "# path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography/'\n",
    "\n",
    "path = path_aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Data Generator, without ozone as input but with radiative outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','QRL', 'QRS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_O3 = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE_O3 = '2021_01_24_NORM_O3_small.nc'\n",
    "VALIDFILE_O3 = '2021_01_24_O3_VALID.nc'\n",
    "TESTFILE_O3 = '2021_01_24_O3_TEST.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_noO3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_noO3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_noO3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling_2.pkl')\n",
    "scale_dict_RH = scale_dict.copy()\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','QRL','QRS']\n",
    "\n",
    "# New GP path below\n",
    "TRAINFILE_RH = '2021_01_24_O3_small_shuffle.nc'\n",
    "NORMFILE_RH = '2021_02_01_NORM_O3_RH_small.nc'\n",
    "    \n",
    "# Comet/Ankitesh path below\n",
    "# TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "# NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "# VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_TNS = 'CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'CI_TNS_M4K_NORM_norm.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using $O_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','O3_AQUA','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','QRL', 'QRS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_O3 = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE_O3 = '2021_01_24_NORM_O3_small.nc'\n",
    "VALIDFILE_O3 = '2021_01_24_O3_VALID.nc'\n",
    "TESTFILE_O3 = '2021_01_24_O3_TEST.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_O3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_O3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_O3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Combined (latest flexible version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "# elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "if path==path_aquaplanet: out_vars=['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "TRAINFILE = '2021_03_18_O3_TRAIN_M4K.nc'\n",
    "VALIDFILE = '2021_03_18_O3_VALID_M4K.nc'\n",
    "TESTFILE = '2021_03_18_O3_TEST_P4K.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old data generator by Ankitesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "# NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "# VALIDFILE = 'CI_SP_M4K_valid.nc'\n",
    "\n",
    "\n",
    "# train_gen_CI = DataGeneratorClimInv(\n",
    "#     data_fn = path+TRAINFILE,\n",
    "#     input_vars = in_vars,\n",
    "#     output_vars = out_vars,\n",
    "#     norm_fn = path+NORMFILE,\n",
    "#     input_transform = ('mean', 'maxrs'),\n",
    "#     output_transform = scale_dict,\n",
    "#     batch_size=8192,\n",
    "#     shuffle=True,\n",
    "#     lev=lev,\n",
    "#     hyam=hyam,hybm=hybm,\n",
    "#     inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "#     inp_subTNS=train_gen_RH.input_transform.sub,inp_divTNS=train_gen_RH.input_transform.div,\n",
    "#     rh_trans=True,t2tns_trans=False,\n",
    "#     lhflx_trans=False,\n",
    "#     scaling=False,\n",
    "#     interpolate=False\n",
    "# )\n",
    "\n",
    "# valid_gen_CI = DataGeneratorClimInv(\n",
    "#     data_fn = path+VALIDFILE,\n",
    "#     input_vars = in_vars,\n",
    "#     output_vars = out_vars,\n",
    "#     norm_fn = path+NORMFILE,\n",
    "#     input_transform = ('mean', 'maxrs'),\n",
    "#     output_transform = scale_dict,\n",
    "#     batch_size=8192,\n",
    "#     shuffle=True,\n",
    "#     lev=lev,\n",
    "#     hyam=hyam,hybm=hybm,\n",
    "#     inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "#     inp_subTNS=train_gen_RH.input_transform.sub,inp_divTNS=train_gen_RH.input_transform.div,\n",
    "#     rh_trans=True,t2tns_trans=False,\n",
    "#     lhflx_trans=False,\n",
    "#     scaling=False,\n",
    "#     interpolate=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved flexible data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add callback class to track loss on multiple sets during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [https://stackoverflow.com/questions/47731935/using-multiple-validation-sets-with-keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [3, 4]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,results):\n",
    "                valuename = validation_set_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate-invariant (T,Q,PS,S0,SHF,LHF)->($\\dot{T}$,$\\dot{q}$,RADFLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ozone (T,Q,$O_{3}$,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(94,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_O3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ozone (T,Q,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_noO3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nep = 15\n",
    "# model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "#               callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_18_MLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_02_01_MLR_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfromNS linear version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bcons linear version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chech on validation dataset (m4K simulation) to make sure the NN was properly fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ankitesh_models = '/oasis/scratch/comet/ankitesh/temp_project/models/'\n",
    "path_aqua = 'BF.hdf5'\n",
    "path_geo = 'BF_Geography.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aqua = load_model(Ankitesh_models+path_aqua)\n",
    "model_geo = load_model(Ankitesh_models+path_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/8/2020 - Test loading `pb` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'CI_SP_M4K_CONFIG.yml' # Configuration file\n",
    "if path==path_aquaplanet: data_file = ['CI_SP_M4K_valid.nc','CI_SP_P4K_valid.nc']  # Validation/test data sets\n",
    "elif path==path_realgeography: data_file = ['geography/CI_SP_M4K_valid.nc','geography/CI_SP_P4K_valid.nc']\n",
    "NNarray = [path_aqua,path_geo]\n",
    "NNname = ['Fit_aqua','Fit_geo'] # Name of NNs for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {}; md = {};\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs)\n",
    "    md[NNs] = {}\n",
    "    for j,data in enumerate(data_file):\n",
    "        print('data name is ',data)\n",
    "        \n",
    "        NN[NNs] = load_model(Ankitesh_models+NNs)\n",
    "        md[NNs][data[6:-3]] = ModelDiagnostics(NN[NNs],\n",
    "                                                '/home/ankitesh/CBrain_project/PrepData/'+config_file,\n",
    "                                                '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_file[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md[NNs][data[6:-3]].valid_gen.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics at random drawn time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNarray = ['2021_01_25_O3','2021_01_25_noO3']\n",
    "#dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {};\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs+'.hdf5')\n",
    "    path_model = path+NNs+'.hdf5'\n",
    "    NN[NNs] = load_model(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = valid_gen_O3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$O_{3}$ vs no $O_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 100\n",
    "t_random = np.random.choice(np.linspace(0,1691,1692),size=((Nt,)),replace=False).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct\n",
    "MSE = {}\n",
    "VAR = {}\n",
    "INP = {}\n",
    "\n",
    "for iar,itime in enumerate(t_random):\n",
    "    print('iar=',iar,'/',Nt-1,' & itime',itime,end=\"\\r\")\n",
    "    for i,NNs in enumerate(NNarray):\n",
    "        if iar==0: MSE[NNs] = {}; VAR[NNs] = {}\n",
    "        \n",
    "        inp = gen[itime][0]\n",
    "        inp_noO3 = np.delete(inp,np.arange(60,90),axis=1)\n",
    "        truth = gen[itime][1]\n",
    "        \n",
    "        if i==0: p = NN[NNs].predict_on_batch(inp)\n",
    "        elif i==1: p = NN[NNs].predict_on_batch(inp_noO3)\n",
    "        \n",
    "        inp_geo = np.reshape(inp,(64,128,inp.shape[1],1))\n",
    "        p_geo = np.reshape(p,(64,128,p.shape[1]))\n",
    "        t_geo = np.reshape(truth,(64,128,truth.shape[1]))\n",
    "        \n",
    "        if iar==0: \n",
    "            MSE[NNs] = np.mean((t_geo-p_geo)**2,axis=2)\n",
    "            VAR[NNs] = np.var(p_geo,axis=2)\n",
    "            INP[NNs] = inp_geo\n",
    "        else: \n",
    "            MSE[NNs] = np.concatenate((MSE[NNs],np.mean((t_geo-p_geo)**2,axis=2)),axis=1)\n",
    "            VAR[NNs] = np.concatenate((VAR[NNs],np.var(p_geo,axis=2)),axis=1)\n",
    "            INP[NNs] = np.concatenate((INP[NNs],inp_geo),axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP[NNs].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,89,:],axis=(1,2)),label='z=29')\n",
    "plt.plot(np.mean(INP[NNs][:,:,79,:],axis=(1,2)),label='z=19')\n",
    "plt.plot(np.mean(INP[NNs][:,:,69,:],axis=(1,2)),label='z=9')\n",
    "plt.legend()\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Norm O3 value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,89,:],axis=(0,2)),label='29')\n",
    "plt.plot(np.mean(INP[NNs][:,:,79,:],axis=(0,2)),label='19')\n",
    "plt.plot(np.mean(INP[NNs][:,:,69,:],axis=(0,2)),label='9')\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Norm O3 value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,60:90,:],axis=(0,1,3))*\\\n",
    "         train_gen_O3.input_transform.div[60:90]+\\\n",
    "         train_gen_O3.input_transform.sub[60:90])\n",
    "plt.ylabel('True O3 value')\n",
    "plt.xlabel('Vertical level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,60:90,:],axis=(0,1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE['2021_01_25_O3'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE['2021_01_25_noO3'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using md\n",
    "MSE = {}\n",
    "VAR = {}\n",
    "diagno = {}\n",
    "diagno['truth'] = {}\n",
    "diagno['pred'] = {}\n",
    "\n",
    "for iar,itime in enumerate(t_random):\n",
    "    print('iar=',iar,'/',Nt-1,' & itime',itime,end=\"\\r\")\n",
    "    for i,NNs in enumerate(NNarray):\n",
    "        if iar==0: MSE[NNs] = {}; VAR[NNs] = {}\n",
    "\n",
    "        inp, p, truth = md[NNs][data[6:-3]].get_inp_pred_truth(itime)  # [lat, lon, var, lev]\n",
    "        \n",
    "        t_geo = md[NNs][data[6:-3]].reshape_ngeo(truth)[:,:,:]\n",
    "        p_geo = md[NNs][data[6:-3]].reshape_ngeo(p.numpy())[:,:,:]\n",
    "        \n",
    "        if iar==0: \n",
    "            MSE[NNs][data[6:-3]] = np.mean((t_geo-p_geo)**2,axis=2)\n",
    "            VAR[NNs][data[6:-3]] = np.var(p_geo,axis=2)\n",
    "        else: \n",
    "            MSE[NNs][data[6:-3]] = np.concatenate((MSE[NNs][data[6:-3]],\n",
    "                                                   np.mean((t_geo-p_geo)**2,axis=2)),axis=1)\n",
    "            VAR[NNs][data[6:-3]] = np.concatenate((VAR[NNs][data[6:-3]],\n",
    "                                                   np.var(p_geo,axis=2)),axis=1)\n",
    "\n",
    "#         MSE[NNs] = (MSE['samples']*MSE[NNs]+np.mean((p-truth)**2))/(MSE['samples']+1)\n",
    "#         MSE['samples'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,NNs in enumerate(NNarray):\n",
    "    plt.scatter(np.mean(coor.TS,axis=(0,2)),np.log10(np.mean(MSE[NNs][data[6:-3]],axis=1)),label=NNs)\n",
    "plt.legend()\n",
    "plt.title(data[6:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,NNs in enumerate(NNarray):\n",
    "    plt.scatter(np.mean(coor.TS,axis=(0,2)),np.mean(MSE[NNs][data[6:-3]],axis=1)/\n",
    "                np.mean(VAR[NNs][data[6:-3]],axis=1),label=NNs)\n",
    "plt.legend()\n",
    "plt.title(data[6:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor.TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aqua.evaluate_generator(valid_gen,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_geo.evaluate_generator(valid_gen,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_HDF5+save_name+'.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on test dataset (p4k simulation) to test ability of NN to generalize to unseen climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_HDF5+save_name+'.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain BF models for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_geog = NN['BF_Geography.hdf5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_geog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_Geog_2020_07_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "BF_geog.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/9/2020 - Use portion<1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From BF aqua to BF geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/23/2020 - Only training last three indices, be careful with saved names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_array = [0.01,0.1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {}\n",
    "for i,por in enumerate(por_array):\n",
    "    print('por=',por)\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    #with tf.Session(graph=graph): # Legacy from tf1\n",
    "        \n",
    "    # 1) Define new generators\n",
    "    train_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    portion=por\n",
    "    )\n",
    "\n",
    "    valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    )\n",
    "\n",
    "    # 2) Load model\n",
    "    NN[por] = load_model('/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/BF_Aqua_2020_07_22.hdf5') # Load aquaplanet model\n",
    "\n",
    "    # 3) Define callbacks and save_name of new model\n",
    "    path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "    save_name = 'TL_BF_2020_07_23_porindex_'+str(i+3)\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=5, verbose=0, mode='min')\n",
    "    mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='loss', mode='min')\n",
    "\n",
    "    # 4) Train model for Nep epochs and CANNOT save state of best validation loss because\n",
    "    # it would NOT be consistent with transfer learning scenario\n",
    "    NN[por].fit_generator(train_gen, epochs=Nep, callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/23/2020 - Legacy = Trained por_array = `[0.00001,0.0001,0.001]` already and now only training the last three indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_array = [0.00001,0.0001,0.001,0.01,0.1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {}\n",
    "for i,por in enumerate(por_array):\n",
    "    print('por=',por)\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    #with tf.Session(graph=graph): # Legacy from tf1\n",
    "        \n",
    "    # 1) Define new generators\n",
    "    train_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    portion=por\n",
    "    )\n",
    "\n",
    "    valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    )\n",
    "\n",
    "    # 2) Load model\n",
    "    NN[por] = load_model('/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/BF_Aqua_2020_07_22.hdf5') # Load aquaplanet model\n",
    "\n",
    "    # 3) Define callbacks and save_name of new model\n",
    "    path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "    save_name = 'TL_BF_2020_07_23_porindex_'+str(i)\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=5, verbose=0, mode='min')\n",
    "    mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='loss', mode='min')\n",
    "\n",
    "    # 4) Train model for Nep epochs and CANNOT save state of best validation loss because\n",
    "    # it would NOT be consistent with transfer learning scenario\n",
    "    NN[por].fit_generator(train_gen, epochs=Nep, callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
