{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2020\n",
    "- Adapting Ankitesh's notebook that builds and train a \"brute-force\" network to David Walling's hyperparameter search  \n",
    "- Adding the option to choose between aquaplanet and real-geography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "from cbrain.climate_invariant import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "\n",
    "# Comet path below\n",
    "# coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "#                     decode_times=False)\n",
    "\n",
    "# GP path below\n",
    "path_0K = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/fluxbypass_aqua/'\n",
    "coor = xr.open_dataset(path_0K+\"AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-09-02-00000.nc\")\n",
    "\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet path below\n",
    "# TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "# path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "\n",
    "# GP path below\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path = '/export/nfs0home/tbeucler/CBRAIN-CAM/cbrain/'\n",
    "path_nnconfig = '/export/nfs0home/tbeucler/CBRAIN-CAM/nn_config/'\n",
    "\n",
    "# Load hyam and hybm to calculate pressure field in SPCAM\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "\n",
    "# Scale dictionary to convert the loss to W/m2\n",
    "scale_dict = load_pickle(path_nnconfig+'scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data generator class for the climate-invariant network. Calculates the physical rescalings needed to make the NN climate-invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorClimInv(DataGenerator):\n",
    "    \n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "             norm_fn=None, input_transform=None, output_transform=None,\n",
    "             batch_size=1024, shuffle=True, xarray=False, var_cut_off=None,\n",
    "             rh_trans=True,t2tns_trans=True,\n",
    "             lhflx_trans=True,\n",
    "             scaling=True,interpolate=True,\n",
    "             hyam=None,hybm=None,                 \n",
    "             inp_subRH=None,inp_divRH=None,\n",
    "             inp_subTNS=None,inp_divTNS=None,\n",
    "             lev=None, interm_size=40,\n",
    "             lower_lim=6,\n",
    "             is_continous=True,Tnot=5,\n",
    "                mode='train',portion=1):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.rh_trans = rh_trans\n",
    "        self.t2tns_trans = t2tns_trans\n",
    "        self.lhflx_trans = lhflx_trans\n",
    "        self.inp_shape = 64\n",
    "        self.mode=mode\n",
    "        super().__init__(data_fn, input_vars,output_vars,norm_fn,input_transform,output_transform,\n",
    "                        batch_size,shuffle,xarray,var_cut_off) ## call the base data generator\n",
    "        self.inp_sub = self.input_transform.sub\n",
    "        self.inp_div = self.input_transform.div\n",
    "        if rh_trans:\n",
    "            self.qv2rhLayer = QV2RHNumpy(self.inp_sub,self.inp_div,inp_subRH,inp_divRH,hyam,hybm)\n",
    "        \n",
    "        if lhflx_trans:\n",
    "            self.lhflxLayer = LhflxTransNumpy(self.inp_sub,self.inp_div,hyam,hybm)\n",
    "            \n",
    "        if t2tns_trans:\n",
    "            self.t2tnsLayer = T2TmTNSNumpy(self.inp_sub,self.inp_div,inp_subTNS,inp_divTNS,hyam,hybm)\n",
    "            \n",
    "        if scaling:\n",
    "            self.scalingLayer = ScalingNumpy(hyam,hybm)\n",
    "            self.inp_shape += 1\n",
    "                    \n",
    "        if interpolate:\n",
    "            self.interpLayer = InterpolationNumpy(lev,is_continous,Tnot,lower_lim,interm_size)\n",
    "            self.inp_shape += interm_size*2 + 4 + 30 ## 4 same as 60-64 and 30 for lev_tilde.size\n",
    "            \n",
    "        # tgb - 7/9/2020 - Test only training on a subset of the data determined by portion\n",
    "        self.portion = portion\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # If portion<1, only look at a subset of the data by putting an upper bound on index\n",
    "        if self.portion<1: index = index % round(1/self.portion)\n",
    "        elif self.portion>1: print('Setting portion=1 because portion>1')\n",
    "        elif self.portion<0: print('Setting portion=1 because portion<0')\n",
    "        \n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result) \n",
    "            \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "            \n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        if self.mode=='val':\n",
    "            return xr.DataArray(X_result), xr.DataArray(Y)\n",
    "        return X_result,Y\n",
    "    \n",
    "    ##transforms the input data into the required format, take the unnormalized dataset\n",
    "    def transform(self,X):\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result)  \n",
    "        \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "\n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        return X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP paths below\n",
    "path_aquaplanet = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path_realgeography = ''\n",
    "\n",
    "# Comet paths below\n",
    "# path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "# path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography/'\n",
    "\n",
    "path = path_aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Data Generator, without ozone as input but with radiative outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','QRL', 'QRS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_O3 = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE_O3 = '2021_01_24_NORM_O3_small.nc'\n",
    "VALIDFILE_O3 = '2021_01_24_O3_VALID.nc'\n",
    "TESTFILE_O3 = '2021_01_24_O3_TEST.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_noO3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_noO3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_noO3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling_2.pkl')\n",
    "scale_dict_RH = scale_dict.copy()\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','QRL','QRS']\n",
    "\n",
    "# New GP path below\n",
    "TRAINFILE_RH = '2021_01_24_O3_small_shuffle.nc'\n",
    "NORMFILE_RH = '2021_02_01_NORM_O3_RH_small.nc'\n",
    "    \n",
    "# Comet/Ankitesh path below\n",
    "# TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "# NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "# VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using QSATdeficit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the norm file for this generator as we are solely using it as an input to determine the right normalization for the combined generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New GP path below\n",
    "TRAINFILE_QSATdeficit = '2021_02_01_O3_QSATdeficit_small_shuffle.nc'\n",
    "NORMFILE_QSATdeficit = '2021_02_01_NORM_O3_QSATdeficit_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_QSATdeficit = ['QSATdeficit','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_QSATdeficit = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_QSATdeficit = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_QSATdeficit,\n",
    "    input_vars = in_vars_QSATdeficit,\n",
    "    output_vars = out_vars_QSATdeficit,\n",
    "    norm_fn = path+NORMFILE_QSATdeficit,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_TNS = '2021_02_01_O3_TfromNS_small_shuffle.nc'\n",
    "NORMFILE_TNS = '2021_02_01_NORM_O3_TfromNS_small.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using BCONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','BCONS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_BCONS = '2021_02_01_O3_BCONS_small_shuffle.nc'\n",
    "NORMFILE_BCONS = '2021_02_01_NORM_O3_BCONS_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BCONS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_BCONS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_BCONS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','T_NSto220','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_T_NSto220 = '2021_03_31_O3_T_NSto220_small.nc'\n",
    "NORMFILE_T_NSto220 = '2021_03_31_NORM_O3_T_NSto220_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_T_NSto220 = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_T_NSto220,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_T_NSto220,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsDELQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsDELQ = '2021_02_01_O3_LHF_nsDELQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsDELQ = '2021_02_01_NORM_O3_LHF_nsDELQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsDELQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsDELQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsDELQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsQ = '2021_02_01_O3_LHF_nsQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsQ = '2021_02_01_NORM_O3_LHF_nsQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using $O_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','O3_AQUA','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','QRL', 'QRS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_O3 = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE_O3 = '2021_01_24_NORM_O3_small.nc'\n",
    "VALIDFILE_O3 = '2021_01_24_O3_VALID.nc'\n",
    "TESTFILE_O3 = '2021_01_24_O3_TEST.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_O3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_O3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_O3 = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILE_O3,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_O3,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Combined (latest flexible version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "# elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "if path==path_aquaplanet: out_vars=['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "TRAINFILE = '2021_03_18_O3_TRAIN_M4K.nc'\n",
    "#TRAINFILE = '2021_01_24_O3_small_shuffle.nc'\n",
    "VALIDFILE = '2021_03_18_O3_VALID_M4K.nc'\n",
    "TESTFILE = '2021_03_18_O3_TEST_P4K.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old data generator by Ankitesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "# NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "# VALIDFILE = 'CI_SP_M4K_valid.nc'\n",
    "\n",
    "\n",
    "# train_gen_CI = DataGeneratorClimInv(\n",
    "#     data_fn = path+TRAINFILE,\n",
    "#     input_vars = in_vars,\n",
    "#     output_vars = out_vars,\n",
    "#     norm_fn = path+NORMFILE,\n",
    "#     input_transform = ('mean', 'maxrs'),\n",
    "#     output_transform = scale_dict,\n",
    "#     batch_size=8192,\n",
    "#     shuffle=True,\n",
    "#     lev=lev,\n",
    "#     hyam=hyam,hybm=hybm,\n",
    "#     inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "#     inp_subTNS=train_gen_RH.input_transform.sub,inp_divTNS=train_gen_RH.input_transform.div,\n",
    "#     rh_trans=True,t2tns_trans=False,\n",
    "#     lhflx_trans=False,\n",
    "#     scaling=False,\n",
    "#     interpolate=False\n",
    "# )\n",
    "\n",
    "# valid_gen_CI = DataGeneratorClimInv(\n",
    "#     data_fn = path+VALIDFILE,\n",
    "#     input_vars = in_vars,\n",
    "#     output_vars = out_vars,\n",
    "#     norm_fn = path+NORMFILE,\n",
    "#     input_transform = ('mean', 'maxrs'),\n",
    "#     output_transform = scale_dict,\n",
    "#     batch_size=8192,\n",
    "#     shuffle=True,\n",
    "#     lev=lev,\n",
    "#     hyam=hyam,hybm=hybm,\n",
    "#     inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "#     inp_subTNS=train_gen_RH.input_transform.sub,inp_divTNS=train_gen_RH.input_transform.div,\n",
    "#     rh_trans=True,t2tns_trans=False,\n",
    "#     lhflx_trans=False,\n",
    "#     scaling=False,\n",
    "#     interpolate=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved flexible data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling='TfromNS',\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_TNS.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_TNS.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling='TfromNS',\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_TNS.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_TNS.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling='TfromNS',\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_TNS.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_TNS.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add callback class to track loss on multiple sets during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [https://stackoverflow.com/questions/47731935/using-multiple-validation-sets-with-keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen_CI[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            valid_generator,valid_name = validation_set\n",
    "            #tf.print('Results')\n",
    "            results = self.model.evaluate_generator(generator=valid_generator)\n",
    "            #tf.print(results)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,[results]):\n",
    "                #tf.print(metric,result)\n",
    "                valuename = valid_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate-invariant (T,Q,PS,S0,SHF,LHF)->($\\dot{T}$,$\\dot{q}$,RADFLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ozone (T,Q,$O_{3}$,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(94,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_O3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ozone (T,Q,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_noO3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nep = 15\n",
    "# model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "#               callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_18_MLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_08_NN6L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2542s 441ms/step - loss: 342.0934 - val_loss: 329.9304\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2701s 469ms/step - loss: 320.9061 - val_loss: 316.9567\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 2452s 426ms/step - loss: 311.2353 - val_loss: 309.6577\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 2254s 391ms/step - loss: 305.5034 - val_loss: 304.9600\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1731s 301ms/step - loss: 301.7379 - val_loss: 301.6836\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1117s 194ms/step - loss: 299.0327 - val_loss: 299.1967\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 816s 142ms/step - loss: 296.9427 - val_loss: 297.2431\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 295.2764 - val_loss: 295.7120\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 694s 121ms/step - loss: 293.9242 - val_loss: 294.4352\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 292.8106 - val_loss: 293.3954\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 712s 124ms/step - loss: 291.8827 - val_loss: 292.5364\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 291.1030 - val_loss: 291.8280\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 290.4395 - val_loss: 291.2006\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 699s 121ms/step - loss: 289.8687 - val_loss: 290.6818\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 702s 122ms/step - loss: 289.3730 - val_loss: 290.2150\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 703s 122ms/step - loss: 288.9378 - val_loss: 289.8255\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 684s 119ms/step - loss: 288.5535 - val_loss: 289.4663\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 694s 120ms/step - loss: 288.2098 - val_loss: 289.1482\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 287.9012 - val_loss: 288.8622\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 287.6209 - val_loss: 288.5948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee43236630>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   3/5759 [..............................] - ETA: 56:44 - loss: 362.5890  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QSATdeficit linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_QSATdeficit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfromNS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_TfromNS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCONS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8098s 1s/step - loss: 324.9880 - val_loss: 313.2678\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 12555s 2s/step - loss: 308.3165 - val_loss: 307.0678\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 13642s 2s/step - loss: 304.0913 - val_loss: 304.2872\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10465s 2s/step - loss: 301.8938 - val_loss: 302.7330\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7909s 1s/step - loss: 300.5873 - val_loss: 301.7510\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 8526s 1s/step - loss: 299.7002 - val_loss: 301.0397\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7230s 1s/step - loss: 299.0066 - val_loss: 300.3937\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 8294s 1s/step - loss: 298.4328 - val_loss: 299.8434\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 8250s 1s/step - loss: 297.9426 - val_loss: 299.4925\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8909s 2s/step - loss: 297.5149 - val_loss: 299.0509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcce5600f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [324.98798001675704,\n",
       "  308.31648811596017,\n",
       "  304.0912586973243,\n",
       "  301.8937511361293,\n",
       "  300.5873382457078,\n",
       "  299.7002091802725,\n",
       "  299.0065637804771,\n",
       "  298.4328209674489,\n",
       "  297.9425933249689,\n",
       "  297.5149222372472],\n",
       " 'val_loss': [313.2678370875624,\n",
       "  307.0677993880396,\n",
       "  304.2871500202989,\n",
       "  302.73299462630064,\n",
       "  301.75103527639715,\n",
       "  301.03966337007205,\n",
       "  300.39373488234764,\n",
       "  299.8433922314147,\n",
       "  299.4924658236212,\n",
       "  299.0508612894959],\n",
       " 'testP4K_loss': [696.3224985280676,\n",
       "  686.2635380876054,\n",
       "  682.7061903721445,\n",
       "  680.7840038270242,\n",
       "  679.5804400619129,\n",
       "  678.422916433854,\n",
       "  677.2276842855165,\n",
       "  676.2437589546028,\n",
       "  675.5998048438668,\n",
       "  674.6606088692057]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+(T-TNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_31_MLR_RH_NSto220'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 10546s 2s/step - loss: 333.2154 - val_loss: 318.1506\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10194s 2s/step - loss: 310.9067 - val_loss: 308.3093\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 9862s 2s/step - loss: 304.7300 - val_loss: 304.2289\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 11121s 2s/step - loss: 301.8291 - val_loss: 302.1135\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10748s 2s/step - loss: 300.2339 - val_loss: 300.8775\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10117s 2s/step - loss: 299.1923 - val_loss: 299.9938\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10174s 2s/step - loss: 298.3948 - val_loss: 299.3052\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 9887s 2s/step - loss: 297.7428 - val_loss: 298.7333\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10299s 2s/step - loss: 297.1934 - val_loss: 298.2455\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 10431s 2s/step - loss: 296.7218 - val_loss: 297.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9680546668>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.21542532209264,\n",
       "  310.90667960419796,\n",
       "  304.72998016670067,\n",
       "  301.8290961309116,\n",
       "  300.2339088743316,\n",
       "  299.19231390816446,\n",
       "  298.39477655642935,\n",
       "  297.74276377828113,\n",
       "  297.19338681379486,\n",
       "  296.7217506637878],\n",
       " 'val_loss': [318.15060213358146,\n",
       "  308.30926291885646,\n",
       "  304.22893252623965,\n",
       "  302.1134895894682,\n",
       "  300.8775425180225,\n",
       "  299.9937965616662,\n",
       "  299.30517368746393,\n",
       "  298.73325744039545,\n",
       "  298.2454878341621,\n",
       "  297.8241368286815],\n",
       " 'testP4K_loss': [723.0185877851295,\n",
       "  698.1272752300885,\n",
       "  685.258197891926,\n",
       "  679.2784368682783,\n",
       "  676.55788616959,\n",
       "  674.8558076801512,\n",
       "  673.4969380158018,\n",
       "  672.4355979235329,\n",
       "  671.423601997825,\n",
       "  670.7109736936912]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 7449s 1s/step - loss: 332.8352 - val_loss: 318.1170\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 5519s 958ms/step - loss: 311.1496 - val_loss: 308.7574\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5822s 1s/step - loss: 305.3264 - val_loss: 304.9336\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6582s 1s/step - loss: 302.6262 - val_loss: 302.9958\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7103s 1s/step - loss: 301.1792 - val_loss: 301.9337\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 14946s 3s/step - loss: 300.2849 - val_loss: 301.1922\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 4869s 846ms/step - loss: 299.6383 - val_loss: 300.6844\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5781s 1s/step - loss: 299.1321 - val_loss: 300.2569\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6748s 1s/step - loss: 298.7180 - val_loss: 299.8978\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 4745s 824ms/step - loss: 298.3688 - val_loss: 299.6018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232cfd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [332.83523499690364,\n",
       "  311.1495506735721,\n",
       "  305.32637336838263,\n",
       "  302.6261628686316,\n",
       "  301.1792370861946,\n",
       "  300.28492045017515,\n",
       "  299.6382695135298,\n",
       "  299.1321185072753,\n",
       "  298.7179960372403,\n",
       "  298.3687616935474],\n",
       " 'val_loss': [318.11703873779254,\n",
       "  308.75738331521546,\n",
       "  304.9335873032069,\n",
       "  302.9957635315034,\n",
       "  301.9337132302216,\n",
       "  301.1921791930977,\n",
       "  300.68438835990605,\n",
       "  300.2568701773645,\n",
       "  299.89779036501074,\n",
       "  299.60179900476817],\n",
       " 'testP4K_loss': [732.1974313007879,\n",
       "  706.4723622909272,\n",
       "  691.7300854846619,\n",
       "  684.7900059335971,\n",
       "  681.8280336922402,\n",
       "  679.9574934180021,\n",
       "  678.7282812103871,\n",
       "  677.7188479585591,\n",
       "  676.8431889352383,\n",
       "  676.1222872974321]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsDELQ NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_09_NN7L_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2906s 505ms/step - loss: 202.5537 - val_loss: 189.6939\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2024s 352ms/step - loss: 185.1673 - val_loss: 182.6220\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 1676s 291ms/step - loss: 180.0439 - val_loss: 180.6006\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 1666s 289ms/step - loss: 177.2701 - val_loss: 176.7251\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1665s 289ms/step - loss: 175.5439 - val_loss: 175.3660\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1634s 284ms/step - loss: 174.3043 - val_loss: 174.8005\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 1668s 290ms/step - loss: 173.3022 - val_loss: 173.3208\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 1678s 291ms/step - loss: 172.4542 - val_loss: 172.6152\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 2503s 435ms/step - loss: 171.7443 - val_loss: 172.0109\n",
      "Epoch 10/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 171.1936"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_TfromNS_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 6478s 1s/step - loss: 334.0426 - val_loss: 318.8078\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 7037s 1s/step - loss: 310.9031 - val_loss: 307.7163\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5910s 1s/step - loss: 304.0670 - val_loss: 303.4669\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7744s 1s/step - loss: 301.0845 - val_loss: 301.4636\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7767s 1s/step - loss: 299.5355 - val_loss: 300.3262\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7525s 1s/step - loss: 298.5930 - val_loss: 299.6363\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7967s 1s/step - loss: 297.9281 - val_loss: 299.1007\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 6677s 1s/step - loss: 297.4247 - val_loss: 298.6999\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6912s 1s/step - loss: 297.0260 - val_loss: 298.3802\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8484s 1s/step - loss: 296.6993 - val_loss: 298.1298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232ceb8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [334.04261498667336,\n",
       "  310.90313761506275,\n",
       "  304.06700017431956,\n",
       "  301.08452700392866,\n",
       "  299.5355056150648,\n",
       "  298.59301007193477,\n",
       "  297.92813114855807,\n",
       "  297.42471469800324,\n",
       "  297.02595976008,\n",
       "  296.69932817398495],\n",
       " 'val_loss': [318.8077631269736,\n",
       "  307.7162972562862,\n",
       "  303.4669281734807,\n",
       "  301.4636359146121,\n",
       "  300.3261635153653,\n",
       "  299.6362964928039,\n",
       "  299.10066872735194,\n",
       "  298.6998974468755,\n",
       "  298.3802061189176,\n",
       "  298.1298349087428],\n",
       " 'testP4K_loss': [717.7692014099695,\n",
       "  692.5093093676612,\n",
       "  681.1892812258153,\n",
       "  677.2928423535407,\n",
       "  675.2491797104337,\n",
       "  673.9023133940009,\n",
       "  672.9248305401367,\n",
       "  672.2034651915096,\n",
       "  671.5856063464887,\n",
       "  671.1150249686433]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+BCONS+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8133s 1s/step - loss: 330.7609 - val_loss: 316.7712\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6660s 1s/step - loss: 310.4906 - val_loss: 307.8904\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 7116s 1s/step - loss: 304.7100 - val_loss: 303.9924\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7062s 1s/step - loss: 301.7960 - val_loss: 301.8254\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 8137s 1s/step - loss: 300.0964 - val_loss: 300.5073\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7596s 1s/step - loss: 298.9904 - val_loss: 299.6046\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7545s 1s/step - loss: 298.1876 - val_loss: 298.9532\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 7470s 1s/step - loss: 297.5743 - val_loss: 298.4452\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7456s 1s/step - loss: 297.0949 - val_loss: 298.0184\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 7469s 1s/step - loss: 296.7098 - val_loss: 297.6911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad92882ba8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [330.76085305019353,\n",
       "  310.4906432967692,\n",
       "  304.7099609745938,\n",
       "  301.7960417360497,\n",
       "  300.09642175858914,\n",
       "  298.9904117309337,\n",
       "  298.18756228044725,\n",
       "  297.57428517435176,\n",
       "  297.0949075244785,\n",
       "  296.709771785746],\n",
       " 'val_loss': [316.77116454788614,\n",
       "  307.8904337055013,\n",
       "  303.99240362525046,\n",
       "  301.8254467632818,\n",
       "  300.5073347083597,\n",
       "  299.6046408204614,\n",
       "  298.95319436894175,\n",
       "  298.4452129081611,\n",
       "  298.01840061586466,\n",
       "  297.69113439902117],\n",
       " 'testP4K_loss': [702.3246120228104,\n",
       "  686.3533080012243,\n",
       "  680.0862056377094,\n",
       "  676.7398991455286,\n",
       "  674.6481522557676,\n",
       "  673.1816021796275,\n",
       "  672.0055764137222,\n",
       "  671.111931773641,\n",
       "  670.3688568089174,\n",
       "  669.7110506886809]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_01_MLR_RH_NSto220_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 11407s 2s/step - loss: 333.0526 - val_loss: 317.6788\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10810s 2s/step - loss: 310.2102 - val_loss: 307.3595\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 10216s 2s/step - loss: 303.6873 - val_loss: 302.9961\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10683s 2s/step - loss: 300.5542 - val_loss: 300.6824\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10550s 2s/step - loss: 298.8112 - val_loss: 299.3256\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10292s 2s/step - loss: 297.7034 - val_loss: 298.4254\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10560s 2s/step - loss: 296.9044 - val_loss: 297.7670\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 11866s 2s/step - loss: 296.3028 - val_loss: 297.2482\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10787s 2s/step - loss: 295.8395 - val_loss: 296.8615\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 11359s 2s/step - loss: 295.4770 - val_loss: 296.5718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96803fe2e8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05256303355685,\n",
       "  310.2102426647167,\n",
       "  303.6872879666863,\n",
       "  300.55415829901773,\n",
       "  298.81120031363105,\n",
       "  297.7034077711415,\n",
       "  296.90441165903536,\n",
       "  296.30281556532356,\n",
       "  295.83954845970965,\n",
       "  295.4770164907051],\n",
       " 'val_loss': [317.6788116925534,\n",
       "  307.3594649500833,\n",
       "  302.9960944944573,\n",
       "  300.68239808442047,\n",
       "  299.32556045328016,\n",
       "  298.42535546180176,\n",
       "  297.7669507749966,\n",
       "  297.24817319492274,\n",
       "  296.86145162218895,\n",
       "  296.5717960327293],\n",
       " 'testP4K_loss': [721.7310292205681,\n",
       "  696.0703459624854,\n",
       "  682.7236189375567,\n",
       "  676.256974551415,\n",
       "  672.9861661186186,\n",
       "  670.9493864272823,\n",
       "  669.4111307171855,\n",
       "  668.1374724690269,\n",
       "  667.1579007533959,\n",
       "  666.4447404165944]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_03_MLR_RH_NSto220_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 9714s 2s/step - loss: 333.0516 - val_loss: 317.9425\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6439s 1s/step - loss: 310.6932 - val_loss: 308.0465\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5183s 900ms/step - loss: 304.5171 - val_loss: 304.0613\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6201s 1s/step - loss: 301.7091 - val_loss: 302.0904\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 6448s 1s/step - loss: 300.2337 - val_loss: 301.0068\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 6364s 1s/step - loss: 299.3400 - val_loss: 300.3276\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 5246s 911ms/step - loss: 298.7033 - val_loss: 299.8050\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5084s 883ms/step - loss: 298.2143 - val_loss: 299.3951\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7377s 1s/step - loss: 297.8223 - val_loss: 299.0667\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 6087s 1s/step - loss: 297.4986 - val_loss: 298.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96800d5a20>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05156581512364,\n",
       "  310.6931948111193,\n",
       "  304.51710277887906,\n",
       "  301.70909371684877,\n",
       "  300.2336531922436,\n",
       "  299.34000020094226,\n",
       "  298.7033277413232,\n",
       "  298.2142985748487,\n",
       "  297.8223111576081,\n",
       "  297.49855535381016],\n",
       " 'val_loss': [317.9424894715131,\n",
       "  308.0464616504419,\n",
       "  304.0613429893301,\n",
       "  302.090381744452,\n",
       "  301.0067826105784,\n",
       "  300.32760883262154,\n",
       "  299.8049709262554,\n",
       "  299.39514361780095,\n",
       "  299.06670788960923,\n",
       "  298.80823058409953],\n",
       " 'testP4K_loss': [722.2397246819291,\n",
       "  697.4373292892026,\n",
       "  684.9307260671301,\n",
       "  679.3653844453859,\n",
       "  676.8490013480899,\n",
       "  675.4289473119096,\n",
       "  674.3540836741433,\n",
       "  673.5583135512015,\n",
       "  672.867135846849,\n",
       "  672.3943842652513]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chech on validation dataset (m4K simulation) to make sure the NN was properly fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ankitesh_models = '/oasis/scratch/comet/ankitesh/temp_project/models/'\n",
    "path_aqua = 'BF.hdf5'\n",
    "path_geo = 'BF_Geography.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aqua = load_model(Ankitesh_models+path_aqua)\n",
    "model_geo = load_model(Ankitesh_models+path_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/8/2020 - Test loading `pb` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'CI_SP_M4K_CONFIG.yml' # Configuration file\n",
    "if path==path_aquaplanet: data_file = ['CI_SP_M4K_valid.nc','CI_SP_P4K_valid.nc']  # Validation/test data sets\n",
    "elif path==path_realgeography: data_file = ['geography/CI_SP_M4K_valid.nc','geography/CI_SP_P4K_valid.nc']\n",
    "NNarray = [path_aqua,path_geo]\n",
    "NNname = ['Fit_aqua','Fit_geo'] # Name of NNs for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {}; md = {};\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs)\n",
    "    md[NNs] = {}\n",
    "    for j,data in enumerate(data_file):\n",
    "        print('data name is ',data)\n",
    "        \n",
    "        NN[NNs] = load_model(Ankitesh_models+NNs)\n",
    "        md[NNs][data[6:-3]] = ModelDiagnostics(NN[NNs],\n",
    "                                                '/home/ankitesh/CBrain_project/PrepData/'+config_file,\n",
    "                                                '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_file[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md[NNs][data[6:-3]].valid_gen.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics at random drawn time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNarray = ['2021_01_25_O3','2021_01_25_noO3']\n",
    "#dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {};\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs+'.hdf5')\n",
    "    path_model = path+NNs+'.hdf5'\n",
    "    NN[NNs] = load_model(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = valid_gen_O3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$O_{3}$ vs no $O_{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 100\n",
    "t_random = np.random.choice(np.linspace(0,1691,1692),size=((Nt,)),replace=False).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct\n",
    "MSE = {}\n",
    "VAR = {}\n",
    "INP = {}\n",
    "\n",
    "for iar,itime in enumerate(t_random):\n",
    "    print('iar=',iar,'/',Nt-1,' & itime',itime,end=\"\\r\")\n",
    "    for i,NNs in enumerate(NNarray):\n",
    "        if iar==0: MSE[NNs] = {}; VAR[NNs] = {}\n",
    "        \n",
    "        inp = gen[itime][0]\n",
    "        inp_noO3 = np.delete(inp,np.arange(60,90),axis=1)\n",
    "        truth = gen[itime][1]\n",
    "        \n",
    "        if i==0: p = NN[NNs].predict_on_batch(inp)\n",
    "        elif i==1: p = NN[NNs].predict_on_batch(inp_noO3)\n",
    "        \n",
    "        inp_geo = np.reshape(inp,(64,128,inp.shape[1],1))\n",
    "        p_geo = np.reshape(p,(64,128,p.shape[1]))\n",
    "        t_geo = np.reshape(truth,(64,128,truth.shape[1]))\n",
    "        \n",
    "        if iar==0: \n",
    "            MSE[NNs] = np.mean((t_geo-p_geo)**2,axis=2)\n",
    "            VAR[NNs] = np.var(p_geo,axis=2)\n",
    "            INP[NNs] = inp_geo\n",
    "        else: \n",
    "            MSE[NNs] = np.concatenate((MSE[NNs],np.mean((t_geo-p_geo)**2,axis=2)),axis=1)\n",
    "            VAR[NNs] = np.concatenate((VAR[NNs],np.var(p_geo,axis=2)),axis=1)\n",
    "            INP[NNs] = np.concatenate((INP[NNs],inp_geo),axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP[NNs].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,89,:],axis=(1,2)),label='z=29')\n",
    "plt.plot(np.mean(INP[NNs][:,:,79,:],axis=(1,2)),label='z=19')\n",
    "plt.plot(np.mean(INP[NNs][:,:,69,:],axis=(1,2)),label='z=9')\n",
    "plt.legend()\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Norm O3 value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,89,:],axis=(0,2)),label='29')\n",
    "plt.plot(np.mean(INP[NNs][:,:,79,:],axis=(0,2)),label='19')\n",
    "plt.plot(np.mean(INP[NNs][:,:,69,:],axis=(0,2)),label='9')\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Norm O3 value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,60:90,:],axis=(0,1,3))*\\\n",
    "         train_gen_O3.input_transform.div[60:90]+\\\n",
    "         train_gen_O3.input_transform.sub[60:90])\n",
    "plt.ylabel('True O3 value')\n",
    "plt.xlabel('Vertical level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(INP[NNs][:,:,60:90,:],axis=(0,1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE['2021_01_25_O3'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE['2021_01_25_noO3'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using md\n",
    "MSE = {}\n",
    "VAR = {}\n",
    "diagno = {}\n",
    "diagno['truth'] = {}\n",
    "diagno['pred'] = {}\n",
    "\n",
    "for iar,itime in enumerate(t_random):\n",
    "    print('iar=',iar,'/',Nt-1,' & itime',itime,end=\"\\r\")\n",
    "    for i,NNs in enumerate(NNarray):\n",
    "        if iar==0: MSE[NNs] = {}; VAR[NNs] = {}\n",
    "\n",
    "        inp, p, truth = md[NNs][data[6:-3]].get_inp_pred_truth(itime)  # [lat, lon, var, lev]\n",
    "        \n",
    "        t_geo = md[NNs][data[6:-3]].reshape_ngeo(truth)[:,:,:]\n",
    "        p_geo = md[NNs][data[6:-3]].reshape_ngeo(p.numpy())[:,:,:]\n",
    "        \n",
    "        if iar==0: \n",
    "            MSE[NNs][data[6:-3]] = np.mean((t_geo-p_geo)**2,axis=2)\n",
    "            VAR[NNs][data[6:-3]] = np.var(p_geo,axis=2)\n",
    "        else: \n",
    "            MSE[NNs][data[6:-3]] = np.concatenate((MSE[NNs][data[6:-3]],\n",
    "                                                   np.mean((t_geo-p_geo)**2,axis=2)),axis=1)\n",
    "            VAR[NNs][data[6:-3]] = np.concatenate((VAR[NNs][data[6:-3]],\n",
    "                                                   np.var(p_geo,axis=2)),axis=1)\n",
    "\n",
    "#         MSE[NNs] = (MSE['samples']*MSE[NNs]+np.mean((p-truth)**2))/(MSE['samples']+1)\n",
    "#         MSE['samples'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,NNs in enumerate(NNarray):\n",
    "    plt.scatter(np.mean(coor.TS,axis=(0,2)),np.log10(np.mean(MSE[NNs][data[6:-3]],axis=1)),label=NNs)\n",
    "plt.legend()\n",
    "plt.title(data[6:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,NNs in enumerate(NNarray):\n",
    "    plt.scatter(np.mean(coor.TS,axis=(0,2)),np.mean(MSE[NNs][data[6:-3]],axis=1)/\n",
    "                np.mean(VAR[NNs][data[6:-3]],axis=1),label=NNs)\n",
    "plt.legend()\n",
    "plt.title(data[6:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor.TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aqua.evaluate_generator(valid_gen,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_geo.evaluate_generator(valid_gen,steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_HDF5+save_name+'.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on test dataset (p4k simulation) to test ability of NN to generalize to unseen climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_HDF5+save_name+'.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain BF models for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_geog = NN['BF_Geography.hdf5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_geog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_Geog_2020_07_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "BF_geog.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/9/2020 - Use portion<1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From BF aqua to BF geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/23/2020 - Only training last three indices, be careful with saved names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_array = [0.01,0.1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {}\n",
    "for i,por in enumerate(por_array):\n",
    "    print('por=',por)\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    #with tf.Session(graph=graph): # Legacy from tf1\n",
    "        \n",
    "    # 1) Define new generators\n",
    "    train_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    portion=por\n",
    "    )\n",
    "\n",
    "    valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    )\n",
    "\n",
    "    # 2) Load model\n",
    "    NN[por] = load_model('/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/BF_Aqua_2020_07_22.hdf5') # Load aquaplanet model\n",
    "\n",
    "    # 3) Define callbacks and save_name of new model\n",
    "    path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "    save_name = 'TL_BF_2020_07_23_porindex_'+str(i+3)\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=5, verbose=0, mode='min')\n",
    "    mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='loss', mode='min')\n",
    "\n",
    "    # 4) Train model for Nep epochs and CANNOT save state of best validation loss because\n",
    "    # it would NOT be consistent with transfer learning scenario\n",
    "    NN[por].fit_generator(train_gen, epochs=Nep, callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 7/23/2020 - Legacy = Trained por_array = `[0.00001,0.0001,0.001]` already and now only training the last three indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_array = [0.00001,0.0001,0.001,0.01,0.1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {}\n",
    "for i,por in enumerate(por_array):\n",
    "    print('por=',por)\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    #with tf.Session(graph=graph): # Legacy from tf1\n",
    "        \n",
    "    # 1) Define new generators\n",
    "    train_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    portion=por\n",
    "    )\n",
    "\n",
    "    valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False,\n",
    "    )\n",
    "\n",
    "    # 2) Load model\n",
    "    NN[por] = load_model('/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/BF_Aqua_2020_07_22.hdf5') # Load aquaplanet model\n",
    "\n",
    "    # 3) Define callbacks and save_name of new model\n",
    "    path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "    save_name = 'TL_BF_2020_07_23_porindex_'+str(i)\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=5, verbose=0, mode='min')\n",
    "    mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='loss', mode='min')\n",
    "\n",
    "    # 4) Train model for Nep epochs and CANNOT save state of best validation loss because\n",
    "    # it would NOT be consistent with transfer learning scenario\n",
    "    NN[por].fit_generator(train_gen, epochs=Nep, callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
