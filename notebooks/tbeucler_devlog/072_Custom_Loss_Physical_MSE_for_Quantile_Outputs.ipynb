{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 6/12/2021 - The goal is to see whether it would be possible to train a NN/MLR outputting results in quantile space while still penalizing them following the mean squared error in physical space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2021 - Recycling this notebook but fitting in percentile space (no scale_dict, use output in percentile units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2020\n",
    "- Adapting Ankitesh's notebook that builds and train a \"brute-force\" network to David Walling's hyperparameter search  \n",
    "- Adding the option to choose between aquaplanet and real-geography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "from cbrain.climate_invariant import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "\n",
    "# Comet path below\n",
    "# coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "#                     decode_times=False)\n",
    "\n",
    "# GP path below\n",
    "path_0K = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/fluxbypass_aqua/'\n",
    "coor = xr.open_dataset(path_0K+\"AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-09-02-00000.nc\")\n",
    "\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet path below\n",
    "# TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "# path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "\n",
    "# GP path below\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path = '/export/nfs0home/tbeucler/CBRAIN-CAM/cbrain/'\n",
    "path_nnconfig = '/export/nfs0home/tbeucler/CBRAIN-CAM/nn_config/'\n",
    "\n",
    "# Load hyam and hybm to calculate pressure field in SPCAM\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "\n",
    "# Scale dictionary to convert the loss to W/m2\n",
    "scale_dict = load_pickle(path_nnconfig+'scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data generator class for the climate-invariant network. Calculates the physical rescalings needed to make the NN climate-invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP paths below\n",
    "#path_aquaplanet = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "#path_realgeography = ''\n",
    "\n",
    "# GP /fast paths below\n",
    "path_aquaplanet = '/fast/tbeucler/climate_invariant/aquaplanet/'\n",
    "\n",
    "# Comet paths below\n",
    "# path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "# path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography/'\n",
    "\n",
    "path = path_aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling_2.pkl')\n",
    "scale_dict_RH = scale_dict.copy()\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','QRL','QRS']\n",
    "\n",
    "# New GP path below\n",
    "TRAINFILE_RH = '2021_01_24_O3_small_shuffle.nc'\n",
    "NORMFILE_RH = '2021_02_01_NORM_O3_RH_small.nc'\n",
    "    \n",
    "# Comet/Ankitesh path below\n",
    "# TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "# NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "# VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using QSATdeficit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the norm file for this generator as we are solely using it as an input to determine the right normalization for the combined generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New GP path below\n",
    "TRAINFILE_QSATdeficit = '2021_02_01_O3_QSATdeficit_small_shuffle.nc'\n",
    "NORMFILE_QSATdeficit = '2021_02_01_NORM_O3_QSATdeficit_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_QSATdeficit = ['QSATdeficit','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_QSATdeficit = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_QSATdeficit = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_QSATdeficit,\n",
    "    input_vars = in_vars_QSATdeficit,\n",
    "    output_vars = out_vars_QSATdeficit,\n",
    "    norm_fn = path+NORMFILE_QSATdeficit,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_TNS = '2021_02_01_O3_TfromNS_small_shuffle.nc'\n",
    "NORMFILE_TNS = '2021_02_01_NORM_O3_TfromNS_small.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using BCONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','BCONS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_BCONS = '2021_02_01_O3_BCONS_small_shuffle.nc'\n",
    "NORMFILE_BCONS = '2021_02_01_NORM_O3_BCONS_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BCONS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_BCONS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_BCONS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','T_NSto220','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_T_NSto220 = '2021_03_31_O3_T_NSto220_small.nc'\n",
    "NORMFILE_T_NSto220 = '2021_03_31_NORM_O3_T_NSto220_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_T_NSto220 = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_T_NSto220,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_T_NSto220,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsDELQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsDELQ = '2021_02_01_O3_LHF_nsDELQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsDELQ = '2021_02_01_NORM_O3_LHF_nsDELQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsDELQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsDELQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsDELQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsQ = '2021_02_01_O3_LHF_nsQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsQ = '2021_02_01_NORM_O3_LHF_nsQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Combined (latest flexible version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "#if path==path_aquaplanet: out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In physical space\n",
    "TRAINFILE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_03_18_O3_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_M4K_shuffle.nc'\n",
    "\n",
    "# In percentile space\n",
    "#TRAINFILE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "#TRAINFILE = '2021_01_24_O3_small_shuffle.nc'\n",
    "#VALIDFILE = '2021_04_09_PERC_VALID_M4K.nc'\n",
    "#TESTFILE = '2021_04_09_PERC_TEST_P4K.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old data generator by Ankitesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved flexible data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add callback class to track loss on multiple sets during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [https://stackoverflow.com/questions/47731935/using-multiple-validation-sets-with-keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_diffgeog_gen_CI[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            valid_generator,valid_name = validation_set\n",
    "            #tf.print('Results')\n",
    "            results = self.model.evaluate_generator(generator=valid_generator)\n",
    "            #tf.print(results)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,[results]):\n",
    "                #tf.print(metric,result)\n",
    "                valuename = valid_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick test to develop custom loss fx (no loss tracking across multiple datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid (TensorF [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ipath,path in enumerate([TRAINFILE,VALIDFILE,TESTFILE_DIFFCLIMATE,TESTFILE_DIFFGEOG]):\n",
    "    hf = open(pathPKL+'/'+path+'_PERC.pkl','rb')\n",
    "    pdf[path] = pickle.load(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_physical(pdf):\n",
    "    \n",
    "    def loss(y_true,y_pred):\n",
    "        y_true_physical = tf.identity(y_true)\n",
    "        y_pred_physical = tf.identity(y_pred)\n",
    "        for ilev in range(120):\n",
    "            y_true_physical[:,ilev] = \\\n",
    "            tfp.math.interp_regular_1d_grid(y_true[:,ilev],\n",
    "                                            x_ref_min=0,x_ref_max=1,y_ref=pdf[:,ilev])\n",
    "            y_pred_physical[:,ilev] = \\\n",
    "            tfp.math.interp_regular_1d_grid(y_pred[:,ilev],\n",
    "                                            x_ref_min=0,x_ref_max=1,y_ref=pdf[:,ilev])\n",
    "        return tf.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('model.h5', \n",
    "#                    custom_objects={'loss': asymmetric_loss(alpha)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a191c00a68ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.compile(tf.keras.optimizers.Adam(), \n\u001b[0;32m----> 2\u001b[0;31m               loss=mse_physical(pdf=np.float32(pdf['2021_03_18_O3_TRAIN_M4K_shuffle.nc']['PERC_array'][:,94:])))\n\u001b[0m",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;31m# Creates the model loss and weighted metrics sub-graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_weights_loss_and_weighted_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m       \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_weights_loss_and_weighted_metrics\u001b[0;34m(self, sample_weights)\u001b[0m\n\u001b[1;32m   1651\u001b[0m       \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m       \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reduction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m             \u001b[0mper_sample_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m             weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[1;32m   1715\u001b[0m                 \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    220\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-d5e95e50c8f2>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0my_true_physical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0milev\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             tfp.math.interp_regular_1d_grid(y_true[:,ilev],\n\u001b[0;32m----> 9\u001b[0;31m                                             x_ref_min=0,x_ref_max=1,y_ref=pdf[:,ilev])\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0my_pred_physical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0milev\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             tfp.math.interp_regular_1d_grid(y_pred[:,ilev],\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), \n",
    "              loss=mse_physical(pdf=np.float32(pdf['2021_03_18_O3_TRAIN_M4K_shuffle.nc']['PERC_array'][:,94:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "#save_name = '2021_06_12_LOGI_PERC_RH_BCONS_LHF_nsDELQ'\n",
    "save_name = '2021_06_12_Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1577/5759 [=======>......................] - ETA: 27:04 - loss: 0.0337"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-4b9c874e606f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n\u001b[0;32m----> 3\u001b[0;31m                     callbacks=[earlyStopping, mcp_save_pos])\n\u001b[0m",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DFS-L/DATA/pritchard/tbeucler/Miniconda3_2021_04_12/miniconda3/envs/CbrainCustomLayer/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models tracking losses across climates and geography (Based on cold Aquaplanet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR or Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_MLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 341.8513"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_MLR_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 3237s 562ms/step - loss: 333.7198 - val_loss: 318.1690\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 3207s 557ms/step - loss: 310.0145 - val_loss: 306.5839\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 3198s 555ms/step - loss: 302.6933 - val_loss: 301.9040\n",
      "Epoch 4/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 299.3518"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In percentile space\n",
    "TRAINFILE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_09_PERC_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_09_PERC_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_24_RG_PERC_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_LOGI_PERC_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_LOGI_PERC_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "#if path==path_aquaplanet: out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In physical space\n",
    "TRAINFILE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_03_18_O3_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_NN_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_NN_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In percentile space\n",
    "TRAINFILE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_09_PERC_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_09_PERC_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_24_RG_PERC_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_NN_PERC_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_26_NN_PERC_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models tracking losses across climates/geography (Warm to Cold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate-invariant (T,Q,PS,S0,SHF,LHF)->($\\dot{T}$,$\\dot{q}$,RADFLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ozone (T,Q,$O_{3}$,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(94,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_O3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ozone (T,Q,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_noO3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nep = 15\n",
    "# model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "#               callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_MLR_PERC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 1184s 206ms/step - loss: 0.0510 - val_loss: 0.0429\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 1035s 180ms/step - loss: 0.0419 - val_loss: 0.0427\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 753s 131ms/step - loss: 0.0417 - val_loss: 0.0425\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 726s 126ms/step - loss: 0.0416 - val_loss: 0.0424\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 765s 133ms/step - loss: 0.0415 - val_loss: 0.0424\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 0.0415 - val_loss: 0.0424\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 756s 131ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 766s 133ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 763s 132ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 729s 127ms/step - loss: 0.0414 - val_loss: 0.0424\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 774s 134ms/step - loss: 0.0414 - val_loss: 0.0422\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 744s 129ms/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 750s 130ms/step - loss: 0.0414 - val_loss: 0.0421\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 731s 127ms/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 744s 129ms/step - loss: 0.0414 - val_loss: 0.0425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402db4240>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.050982531596102595,\n",
       "  0.04185105333621251,\n",
       "  0.04165498730630319,\n",
       "  0.04157832884143202,\n",
       "  0.04153471013858079,\n",
       "  0.041505846159183686,\n",
       "  0.04148416520251771,\n",
       "  0.04146769321144383,\n",
       "  0.04145484425892099,\n",
       "  0.04144392009125254,\n",
       "  0.04143447252735948,\n",
       "  0.04142640940075244,\n",
       "  0.04141932996618994,\n",
       "  0.04141429779437562,\n",
       "  0.04140815107314552],\n",
       " 'val_loss': [0.042909197750994055,\n",
       "  0.04265815539936504,\n",
       "  0.04253125871611655,\n",
       "  0.0423782235956507,\n",
       "  0.04237728513551328,\n",
       "  0.04235305504845943,\n",
       "  0.04230615223943935,\n",
       "  0.04227608388790575,\n",
       "  0.0423463198359328,\n",
       "  0.042378264729721796,\n",
       "  0.042240786968061286,\n",
       "  0.04227235703251836,\n",
       "  0.042130285002797335,\n",
       "  0.04226516442969454,\n",
       "  0.04249188080922277],\n",
       " 'testP4K_loss': [0.04237089483123066,\n",
       "  0.04204679281155743,\n",
       "  0.041950938805466936,\n",
       "  0.041866756795589787,\n",
       "  0.04182252665865533,\n",
       "  0.04181972140371545,\n",
       "  0.041768535935374836,\n",
       "  0.041756746478308786,\n",
       "  0.041807406456754515,\n",
       "  0.04179418787080238,\n",
       "  0.041728623833007836,\n",
       "  0.041724651345679334,\n",
       "  0.04165810040754197,\n",
       "  0.04172214069534936,\n",
       "  0.0418310413666763]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.050982531596102595,\n",
       "  0.04185105333621251,\n",
       "  0.04165498730630319,\n",
       "  0.04157832884143202,\n",
       "  0.04153471013858079,\n",
       "  0.041505846159183686,\n",
       "  0.04148416520251771,\n",
       "  0.04146769321144383,\n",
       "  0.04145484425892099,\n",
       "  0.04144392009125254,\n",
       "  0.04143447252735948,\n",
       "  0.04142640940075244,\n",
       "  0.04141932996618994,\n",
       "  0.04141429779437562,\n",
       "  0.04140815107314552],\n",
       " 'val_loss': [0.042909197750994055,\n",
       "  0.04265815539936504,\n",
       "  0.04253125871611655,\n",
       "  0.0423782235956507,\n",
       "  0.04237728513551328,\n",
       "  0.04235305504845943,\n",
       "  0.04230615223943935,\n",
       "  0.04227608388790575,\n",
       "  0.0423463198359328,\n",
       "  0.042378264729721796,\n",
       "  0.042240786968061286,\n",
       "  0.04227235703251836,\n",
       "  0.042130285002797335,\n",
       "  0.04226516442969454,\n",
       "  0.04249188080922277],\n",
       " 'testP4K_loss': [0.04237089483123066,\n",
       "  0.04204679281155743,\n",
       "  0.041950938805466936,\n",
       "  0.041866756795589787,\n",
       "  0.04182252665865533,\n",
       "  0.04181972140371545,\n",
       "  0.041768535935374836,\n",
       "  0.041756746478308786,\n",
       "  0.041807406456754515,\n",
       "  0.04179418787080238,\n",
       "  0.041728623833007836,\n",
       "  0.041724651345679334,\n",
       "  0.04165810040754197,\n",
       "  0.04172214069534936,\n",
       "  0.0418310413666763]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF Logistic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_1 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_Log_PERC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 794s 138ms/step - loss: 0.0492 - val_loss: 0.0427\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 761s 132ms/step - loss: 0.0411 - val_loss: 0.0412\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 842s 146ms/step - loss: 0.0405 - val_loss: 0.0410\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 0.0403 - val_loss: 0.0408\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 727s 126ms/step - loss: 0.0402 - val_loss: 0.0408\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 727s 126ms/step - loss: 0.0401 - val_loss: 0.0407\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 738s 128ms/step - loss: 0.0401 - val_loss: 0.0406\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 740s 129ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 769s 133ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 768s 133ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 738s 128ms/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 687s 119ms/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 639s 111ms/step - loss: 0.0399 - val_loss: 0.0405\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 749s 130ms/step - loss: 0.0399 - val_loss: 0.0405\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 0.0399 - val_loss: 0.0404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402d40668>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.04923291314722536,\n",
       "  0.04110610745569074,\n",
       "  0.04050010521521811,\n",
       "  0.040299923023502326,\n",
       "  0.040188955621450057,\n",
       "  0.040115493637423776,\n",
       "  0.040061687980442955,\n",
       "  0.04002036269023862,\n",
       "  0.039987151799029744,\n",
       "  0.03995977671317889,\n",
       "  0.03993697982862977,\n",
       "  0.03991729508678539,\n",
       "  0.0399002306710558,\n",
       "  0.03988536803773406,\n",
       "  0.03987165327315054],\n",
       " 'val_loss': [0.04265024852781968,\n",
       "  0.041217454327464445,\n",
       "  0.04097958452281418,\n",
       "  0.04079991503641219,\n",
       "  0.04078920084213819,\n",
       "  0.04070766954445322,\n",
       "  0.04064866158667609,\n",
       "  0.0406280841281429,\n",
       "  0.04056495156529776,\n",
       "  0.04057289374916206,\n",
       "  0.040560972740690364,\n",
       "  0.0405654035945343,\n",
       "  0.0404891143274493,\n",
       "  0.040506530972142435,\n",
       "  0.0404415049514598],\n",
       " 'testP4K_loss': [0.07411513028238494,\n",
       "  0.07390664614972449,\n",
       "  0.07520650478889554,\n",
       "  0.07573994763074626,\n",
       "  0.0764975735435722,\n",
       "  0.0768441669025755,\n",
       "  0.07727208656825946,\n",
       "  0.07749857328113179,\n",
       "  0.07786896164909984,\n",
       "  0.07816397840018766,\n",
       "  0.07837125465184781,\n",
       "  0.07873877331958073,\n",
       "  0.07878147320397179,\n",
       "  0.07903415168914706,\n",
       "  0.07906030325885516]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.04923291314722536,\n",
       "  0.04110610745569074,\n",
       "  0.04050010521521811,\n",
       "  0.040299923023502326,\n",
       "  0.040188955621450057,\n",
       "  0.040115493637423776,\n",
       "  0.040061687980442955,\n",
       "  0.04002036269023862,\n",
       "  0.039987151799029744,\n",
       "  0.03995977671317889,\n",
       "  0.03993697982862977,\n",
       "  0.03991729508678539,\n",
       "  0.0399002306710558,\n",
       "  0.03988536803773406,\n",
       "  0.03987165327315054],\n",
       " 'val_loss': [0.04265024852781968,\n",
       "  0.041217454327464445,\n",
       "  0.04097958452281418,\n",
       "  0.04079991503641219,\n",
       "  0.04078920084213819,\n",
       "  0.04070766954445322,\n",
       "  0.04064866158667609,\n",
       "  0.0406280841281429,\n",
       "  0.04056495156529776,\n",
       "  0.04057289374916206,\n",
       "  0.040560972740690364,\n",
       "  0.0405654035945343,\n",
       "  0.0404891143274493,\n",
       "  0.040506530972142435,\n",
       "  0.0404415049514598],\n",
       " 'testP4K_loss': [0.07411513028238494,\n",
       "  0.07390664614972449,\n",
       "  0.07520650478889554,\n",
       "  0.07573994763074626,\n",
       "  0.0764975735435722,\n",
       "  0.0768441669025755,\n",
       "  0.07727208656825946,\n",
       "  0.07749857328113179,\n",
       "  0.07786896164909984,\n",
       "  0.07816397840018766,\n",
       "  0.07837125465184781,\n",
       "  0.07873877331958073,\n",
       "  0.07878147320397179,\n",
       "  0.07903415168914706,\n",
       "  0.07906030325885516]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_08_NN6L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2542s 441ms/step - loss: 342.0934 - val_loss: 329.9304\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2701s 469ms/step - loss: 320.9061 - val_loss: 316.9567\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 2452s 426ms/step - loss: 311.2353 - val_loss: 309.6577\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 2254s 391ms/step - loss: 305.5034 - val_loss: 304.9600\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1731s 301ms/step - loss: 301.7379 - val_loss: 301.6836\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1117s 194ms/step - loss: 299.0327 - val_loss: 299.1967\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 816s 142ms/step - loss: 296.9427 - val_loss: 297.2431\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 295.2764 - val_loss: 295.7120\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 694s 121ms/step - loss: 293.9242 - val_loss: 294.4352\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 292.8106 - val_loss: 293.3954\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 712s 124ms/step - loss: 291.8827 - val_loss: 292.5364\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 291.1030 - val_loss: 291.8280\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 290.4395 - val_loss: 291.2006\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 699s 121ms/step - loss: 289.8687 - val_loss: 290.6818\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 702s 122ms/step - loss: 289.3730 - val_loss: 290.2150\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 703s 122ms/step - loss: 288.9378 - val_loss: 289.8255\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 684s 119ms/step - loss: 288.5535 - val_loss: 289.4663\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 694s 120ms/step - loss: 288.2098 - val_loss: 289.1482\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 287.9012 - val_loss: 288.8622\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 287.6209 - val_loss: 288.5948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee43236630>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH Logistic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_2 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_Log_PERC_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 1103s 191ms/step - loss: 0.0467 - val_loss: 0.0405\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 1136s 197ms/step - loss: 0.0392 - val_loss: 0.0394\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 1123s 195ms/step - loss: 0.0389 - val_loss: 0.0394\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 1115s 194ms/step - loss: 0.0388 - val_loss: 0.0393\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 1128s 196ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 1082s 188ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 1136s 197ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 1069s 186ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 1093s 190ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 1121s 195ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 1114s 193ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 1146s 199ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 1111s 193ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 1079s 187ms/step - loss: 0.0386 - val_loss: 0.0392\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 1139s 198ms/step - loss: 0.0386 - val_loss: 0.0392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402f0acc0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.0467287344276843,\n",
       "  0.0392072251961502,\n",
       "  0.038853517252732575,\n",
       "  0.03877987418298528,\n",
       "  0.03874506841666439,\n",
       "  0.03872233596301302,\n",
       "  0.03870567677141188,\n",
       "  0.03869225124110984,\n",
       "  0.03868109595084236,\n",
       "  0.03867188213775712,\n",
       "  0.0386642090824872,\n",
       "  0.03865747418455789,\n",
       "  0.03865132474455802,\n",
       "  0.03864620624032156,\n",
       "  0.03864140413024914],\n",
       " 'val_loss': [0.040451113186371485,\n",
       "  0.03944033429256318,\n",
       "  0.03935017663365135,\n",
       "  0.03931213896163092,\n",
       "  0.03929163821896072,\n",
       "  0.03926565892478177,\n",
       "  0.039275243295873806,\n",
       "  0.039249983558271286,\n",
       "  0.0392355400208758,\n",
       "  0.03925549958671793,\n",
       "  0.03922620319762756,\n",
       "  0.03919682341832721,\n",
       "  0.03921325347639839,\n",
       "  0.039212426125584225,\n",
       "  0.03919467062651657],\n",
       " 'testP4K_loss': [0.06156855558834964,\n",
       "  0.05892425673790308,\n",
       "  0.05871588077863804,\n",
       "  0.058408490616755036,\n",
       "  0.05826237939559019,\n",
       "  0.05821150895383091,\n",
       "  0.058247450274676975,\n",
       "  0.05818161563383118,\n",
       "  0.05822653928238749,\n",
       "  0.05826851157272707,\n",
       "  0.05817507403042966,\n",
       "  0.058177342630420154,\n",
       "  0.05819879022675839,\n",
       "  0.05813914520759851,\n",
       "  0.05809993178835799]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   3/5759 [..............................] - ETA: 56:44 - loss: 362.5890  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QSATdeficit linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_QSATdeficit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfromNS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_TfromNS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCONS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8098s 1s/step - loss: 324.9880 - val_loss: 313.2678\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 12555s 2s/step - loss: 308.3165 - val_loss: 307.0678\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 13642s 2s/step - loss: 304.0913 - val_loss: 304.2872\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10465s 2s/step - loss: 301.8938 - val_loss: 302.7330\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7909s 1s/step - loss: 300.5873 - val_loss: 301.7510\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 8526s 1s/step - loss: 299.7002 - val_loss: 301.0397\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7230s 1s/step - loss: 299.0066 - val_loss: 300.3937\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 8294s 1s/step - loss: 298.4328 - val_loss: 299.8434\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 8250s 1s/step - loss: 297.9426 - val_loss: 299.4925\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8909s 2s/step - loss: 297.5149 - val_loss: 299.0509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcce5600f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [324.98798001675704,\n",
       "  308.31648811596017,\n",
       "  304.0912586973243,\n",
       "  301.8937511361293,\n",
       "  300.5873382457078,\n",
       "  299.7002091802725,\n",
       "  299.0065637804771,\n",
       "  298.4328209674489,\n",
       "  297.9425933249689,\n",
       "  297.5149222372472],\n",
       " 'val_loss': [313.2678370875624,\n",
       "  307.0677993880396,\n",
       "  304.2871500202989,\n",
       "  302.73299462630064,\n",
       "  301.75103527639715,\n",
       "  301.03966337007205,\n",
       "  300.39373488234764,\n",
       "  299.8433922314147,\n",
       "  299.4924658236212,\n",
       "  299.0508612894959],\n",
       " 'testP4K_loss': [696.3224985280676,\n",
       "  686.2635380876054,\n",
       "  682.7061903721445,\n",
       "  680.7840038270242,\n",
       "  679.5804400619129,\n",
       "  678.422916433854,\n",
       "  677.2276842855165,\n",
       "  676.2437589546028,\n",
       "  675.5998048438668,\n",
       "  674.6606088692057]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+(T-TNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_31_MLR_RH_NSto220'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 10546s 2s/step - loss: 333.2154 - val_loss: 318.1506\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10194s 2s/step - loss: 310.9067 - val_loss: 308.3093\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 9862s 2s/step - loss: 304.7300 - val_loss: 304.2289\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 11121s 2s/step - loss: 301.8291 - val_loss: 302.1135\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10748s 2s/step - loss: 300.2339 - val_loss: 300.8775\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10117s 2s/step - loss: 299.1923 - val_loss: 299.9938\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10174s 2s/step - loss: 298.3948 - val_loss: 299.3052\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 9887s 2s/step - loss: 297.7428 - val_loss: 298.7333\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10299s 2s/step - loss: 297.1934 - val_loss: 298.2455\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 10431s 2s/step - loss: 296.7218 - val_loss: 297.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9680546668>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.21542532209264,\n",
       "  310.90667960419796,\n",
       "  304.72998016670067,\n",
       "  301.8290961309116,\n",
       "  300.2339088743316,\n",
       "  299.19231390816446,\n",
       "  298.39477655642935,\n",
       "  297.74276377828113,\n",
       "  297.19338681379486,\n",
       "  296.7217506637878],\n",
       " 'val_loss': [318.15060213358146,\n",
       "  308.30926291885646,\n",
       "  304.22893252623965,\n",
       "  302.1134895894682,\n",
       "  300.8775425180225,\n",
       "  299.9937965616662,\n",
       "  299.30517368746393,\n",
       "  298.73325744039545,\n",
       "  298.2454878341621,\n",
       "  297.8241368286815],\n",
       " 'testP4K_loss': [723.0185877851295,\n",
       "  698.1272752300885,\n",
       "  685.258197891926,\n",
       "  679.2784368682783,\n",
       "  676.55788616959,\n",
       "  674.8558076801512,\n",
       "  673.4969380158018,\n",
       "  672.4355979235329,\n",
       "  671.423601997825,\n",
       "  670.7109736936912]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 7449s 1s/step - loss: 332.8352 - val_loss: 318.1170\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 5519s 958ms/step - loss: 311.1496 - val_loss: 308.7574\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5822s 1s/step - loss: 305.3264 - val_loss: 304.9336\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6582s 1s/step - loss: 302.6262 - val_loss: 302.9958\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7103s 1s/step - loss: 301.1792 - val_loss: 301.9337\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 14946s 3s/step - loss: 300.2849 - val_loss: 301.1922\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 4869s 846ms/step - loss: 299.6383 - val_loss: 300.6844\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5781s 1s/step - loss: 299.1321 - val_loss: 300.2569\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6748s 1s/step - loss: 298.7180 - val_loss: 299.8978\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 4745s 824ms/step - loss: 298.3688 - val_loss: 299.6018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232cfd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [332.83523499690364,\n",
       "  311.1495506735721,\n",
       "  305.32637336838263,\n",
       "  302.6261628686316,\n",
       "  301.1792370861946,\n",
       "  300.28492045017515,\n",
       "  299.6382695135298,\n",
       "  299.1321185072753,\n",
       "  298.7179960372403,\n",
       "  298.3687616935474],\n",
       " 'val_loss': [318.11703873779254,\n",
       "  308.75738331521546,\n",
       "  304.9335873032069,\n",
       "  302.9957635315034,\n",
       "  301.9337132302216,\n",
       "  301.1921791930977,\n",
       "  300.68438835990605,\n",
       "  300.2568701773645,\n",
       "  299.89779036501074,\n",
       "  299.60179900476817],\n",
       " 'testP4K_loss': [732.1974313007879,\n",
       "  706.4723622909272,\n",
       "  691.7300854846619,\n",
       "  684.7900059335971,\n",
       "  681.8280336922402,\n",
       "  679.9574934180021,\n",
       "  678.7282812103871,\n",
       "  677.7188479585591,\n",
       "  676.8431889352383,\n",
       "  676.1222872974321]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsDELQ NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_09_NN7L_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2906s 505ms/step - loss: 202.5537 - val_loss: 189.6939\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2024s 352ms/step - loss: 185.1673 - val_loss: 182.6220\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 1676s 291ms/step - loss: 180.0439 - val_loss: 180.6006\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 1666s 289ms/step - loss: 177.2701 - val_loss: 176.7251\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1665s 289ms/step - loss: 175.5439 - val_loss: 175.3660\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1634s 284ms/step - loss: 174.3043 - val_loss: 174.8005\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 1668s 290ms/step - loss: 173.3022 - val_loss: 173.3208\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 1678s 291ms/step - loss: 172.4542 - val_loss: 172.6152\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 2503s 435ms/step - loss: 171.7443 - val_loss: 172.0109\n",
      "Epoch 10/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 171.1936"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_TfromNS_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 6478s 1s/step - loss: 334.0426 - val_loss: 318.8078\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 7037s 1s/step - loss: 310.9031 - val_loss: 307.7163\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5910s 1s/step - loss: 304.0670 - val_loss: 303.4669\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7744s 1s/step - loss: 301.0845 - val_loss: 301.4636\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7767s 1s/step - loss: 299.5355 - val_loss: 300.3262\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7525s 1s/step - loss: 298.5930 - val_loss: 299.6363\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7967s 1s/step - loss: 297.9281 - val_loss: 299.1007\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 6677s 1s/step - loss: 297.4247 - val_loss: 298.6999\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6912s 1s/step - loss: 297.0260 - val_loss: 298.3802\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8484s 1s/step - loss: 296.6993 - val_loss: 298.1298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232ceb8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [334.04261498667336,\n",
       "  310.90313761506275,\n",
       "  304.06700017431956,\n",
       "  301.08452700392866,\n",
       "  299.5355056150648,\n",
       "  298.59301007193477,\n",
       "  297.92813114855807,\n",
       "  297.42471469800324,\n",
       "  297.02595976008,\n",
       "  296.69932817398495],\n",
       " 'val_loss': [318.8077631269736,\n",
       "  307.7162972562862,\n",
       "  303.4669281734807,\n",
       "  301.4636359146121,\n",
       "  300.3261635153653,\n",
       "  299.6362964928039,\n",
       "  299.10066872735194,\n",
       "  298.6998974468755,\n",
       "  298.3802061189176,\n",
       "  298.1298349087428],\n",
       " 'testP4K_loss': [717.7692014099695,\n",
       "  692.5093093676612,\n",
       "  681.1892812258153,\n",
       "  677.2928423535407,\n",
       "  675.2491797104337,\n",
       "  673.9023133940009,\n",
       "  672.9248305401367,\n",
       "  672.2034651915096,\n",
       "  671.5856063464887,\n",
       "  671.1150249686433]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+BCONS+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8133s 1s/step - loss: 330.7609 - val_loss: 316.7712\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6660s 1s/step - loss: 310.4906 - val_loss: 307.8904\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 7116s 1s/step - loss: 304.7100 - val_loss: 303.9924\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7062s 1s/step - loss: 301.7960 - val_loss: 301.8254\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 8137s 1s/step - loss: 300.0964 - val_loss: 300.5073\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7596s 1s/step - loss: 298.9904 - val_loss: 299.6046\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7545s 1s/step - loss: 298.1876 - val_loss: 298.9532\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 7470s 1s/step - loss: 297.5743 - val_loss: 298.4452\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7456s 1s/step - loss: 297.0949 - val_loss: 298.0184\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 7469s 1s/step - loss: 296.7098 - val_loss: 297.6911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad92882ba8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [330.76085305019353,\n",
       "  310.4906432967692,\n",
       "  304.7099609745938,\n",
       "  301.7960417360497,\n",
       "  300.09642175858914,\n",
       "  298.9904117309337,\n",
       "  298.18756228044725,\n",
       "  297.57428517435176,\n",
       "  297.0949075244785,\n",
       "  296.709771785746],\n",
       " 'val_loss': [316.77116454788614,\n",
       "  307.8904337055013,\n",
       "  303.99240362525046,\n",
       "  301.8254467632818,\n",
       "  300.5073347083597,\n",
       "  299.6046408204614,\n",
       "  298.95319436894175,\n",
       "  298.4452129081611,\n",
       "  298.01840061586466,\n",
       "  297.69113439902117],\n",
       " 'testP4K_loss': [702.3246120228104,\n",
       "  686.3533080012243,\n",
       "  680.0862056377094,\n",
       "  676.7398991455286,\n",
       "  674.6481522557676,\n",
       "  673.1816021796275,\n",
       "  672.0055764137222,\n",
       "  671.111931773641,\n",
       "  670.3688568089174,\n",
       "  669.7110506886809]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_01_MLR_RH_NSto220_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 11407s 2s/step - loss: 333.0526 - val_loss: 317.6788\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10810s 2s/step - loss: 310.2102 - val_loss: 307.3595\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 10216s 2s/step - loss: 303.6873 - val_loss: 302.9961\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10683s 2s/step - loss: 300.5542 - val_loss: 300.6824\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10550s 2s/step - loss: 298.8112 - val_loss: 299.3256\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10292s 2s/step - loss: 297.7034 - val_loss: 298.4254\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10560s 2s/step - loss: 296.9044 - val_loss: 297.7670\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 11866s 2s/step - loss: 296.3028 - val_loss: 297.2482\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10787s 2s/step - loss: 295.8395 - val_loss: 296.8615\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 11359s 2s/step - loss: 295.4770 - val_loss: 296.5718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96803fe2e8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05256303355685,\n",
       "  310.2102426647167,\n",
       "  303.6872879666863,\n",
       "  300.55415829901773,\n",
       "  298.81120031363105,\n",
       "  297.7034077711415,\n",
       "  296.90441165903536,\n",
       "  296.30281556532356,\n",
       "  295.83954845970965,\n",
       "  295.4770164907051],\n",
       " 'val_loss': [317.6788116925534,\n",
       "  307.3594649500833,\n",
       "  302.9960944944573,\n",
       "  300.68239808442047,\n",
       "  299.32556045328016,\n",
       "  298.42535546180176,\n",
       "  297.7669507749966,\n",
       "  297.24817319492274,\n",
       "  296.86145162218895,\n",
       "  296.5717960327293],\n",
       " 'testP4K_loss': [721.7310292205681,\n",
       "  696.0703459624854,\n",
       "  682.7236189375567,\n",
       "  676.256974551415,\n",
       "  672.9861661186186,\n",
       "  670.9493864272823,\n",
       "  669.4111307171855,\n",
       "  668.1374724690269,\n",
       "  667.1579007533959,\n",
       "  666.4447404165944]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_03_MLR_RH_NSto220_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 9714s 2s/step - loss: 333.0516 - val_loss: 317.9425\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6439s 1s/step - loss: 310.6932 - val_loss: 308.0465\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5183s 900ms/step - loss: 304.5171 - val_loss: 304.0613\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6201s 1s/step - loss: 301.7091 - val_loss: 302.0904\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 6448s 1s/step - loss: 300.2337 - val_loss: 301.0068\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 6364s 1s/step - loss: 299.3400 - val_loss: 300.3276\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 5246s 911ms/step - loss: 298.7033 - val_loss: 299.8050\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5084s 883ms/step - loss: 298.2143 - val_loss: 299.3951\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7377s 1s/step - loss: 297.8223 - val_loss: 299.0667\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 6087s 1s/step - loss: 297.4986 - val_loss: 298.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96800d5a20>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05156581512364,\n",
       "  310.6931948111193,\n",
       "  304.51710277887906,\n",
       "  301.70909371684877,\n",
       "  300.2336531922436,\n",
       "  299.34000020094226,\n",
       "  298.7033277413232,\n",
       "  298.2142985748487,\n",
       "  297.8223111576081,\n",
       "  297.49855535381016],\n",
       " 'val_loss': [317.9424894715131,\n",
       "  308.0464616504419,\n",
       "  304.0613429893301,\n",
       "  302.090381744452,\n",
       "  301.0067826105784,\n",
       "  300.32760883262154,\n",
       "  299.8049709262554,\n",
       "  299.39514361780095,\n",
       "  299.06670788960923,\n",
       "  298.80823058409953],\n",
       " 'testP4K_loss': [722.2397246819291,\n",
       "  697.4373292892026,\n",
       "  684.9307260671301,\n",
       "  679.3653844453859,\n",
       "  676.8490013480899,\n",
       "  675.4289473119096,\n",
       "  674.3540836741433,\n",
       "  673.5583135512015,\n",
       "  672.867135846849,\n",
       "  672.3943842652513]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "631.521px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
