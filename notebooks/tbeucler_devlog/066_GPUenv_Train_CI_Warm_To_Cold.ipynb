{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/27/2021 - Recycling notebook [065] for additional training from warm to cold climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2021 - Recycling this notebook but fitting in percentile space (no scale_dict, use output in percentile units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2020\n",
    "- Adapting Ankitesh's notebook that builds and train a \"brute-force\" network to David Walling's hyperparameter search  \n",
    "- Adding the option to choose between aquaplanet and real-geography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "from cbrain.climate_invariant import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "\n",
    "# Comet path below\n",
    "# coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "#                     decode_times=False)\n",
    "\n",
    "# GP path below\n",
    "path_0K = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/fluxbypass_aqua/'\n",
    "coor = xr.open_dataset(path_0K+\"AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-09-02-00000.nc\")\n",
    "\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet path below\n",
    "# TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "# path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "\n",
    "# GP path below\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path = '/export/nfs0home/tbeucler/CBRAIN-CAM/cbrain/'\n",
    "path_nnconfig = '/export/nfs0home/tbeucler/CBRAIN-CAM/nn_config/'\n",
    "\n",
    "# Load hyam and hybm to calculate pressure field in SPCAM\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "\n",
    "# Scale dictionary to convert the loss to W/m2\n",
    "scale_dict = load_pickle(path_nnconfig+'scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data generator class for the climate-invariant network. Calculates the physical rescalings needed to make the NN climate-invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP paths below\n",
    "#path_aquaplanet = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "#path_realgeography = ''\n",
    "\n",
    "# GP /fast paths below\n",
    "path_aquaplanet = '/fast/tbeucler/climate_invariant/aquaplanet/'\n",
    "\n",
    "# Comet paths below\n",
    "# path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "# path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography/'\n",
    "\n",
    "path = path_aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling_2.pkl')\n",
    "scale_dict_RH = scale_dict.copy()\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','QRL','QRS']\n",
    "\n",
    "# New GP path below\n",
    "TRAINFILE_RH = '2021_01_24_O3_small_shuffle.nc'\n",
    "NORMFILE_RH = '2021_02_01_NORM_O3_RH_small.nc'\n",
    "    \n",
    "# Comet/Ankitesh path below\n",
    "# TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "# NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "# VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using QSATdeficit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the norm file for this generator as we are solely using it as an input to determine the right normalization for the combined generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New GP path below\n",
    "TRAINFILE_QSATdeficit = '2021_02_01_O3_QSATdeficit_small_shuffle.nc'\n",
    "NORMFILE_QSATdeficit = '2021_02_01_NORM_O3_QSATdeficit_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_QSATdeficit = ['QSATdeficit','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_QSATdeficit = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_QSATdeficit = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_QSATdeficit,\n",
    "    input_vars = in_vars_QSATdeficit,\n",
    "    output_vars = out_vars_QSATdeficit,\n",
    "    norm_fn = path+NORMFILE_QSATdeficit,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_TNS = '2021_02_01_O3_TfromNS_small_shuffle.nc'\n",
    "NORMFILE_TNS = '2021_02_01_NORM_O3_TfromNS_small.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using BCONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','BCONS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_BCONS = '2021_02_01_O3_BCONS_small_shuffle.nc'\n",
    "NORMFILE_BCONS = '2021_02_01_NORM_O3_BCONS_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BCONS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_BCONS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_BCONS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using BMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','BMSE','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_BMSE = '2021_06_16_BMSE_small_shuffle.nc'\n",
    "NORMFILE_BMSE = '2021_06_16_NORM_BMSE_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BMSE = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_BMSE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_BMSE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','T_NSto220','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_T_NSto220 = '2021_03_31_O3_T_NSto220_small.nc'\n",
    "NORMFILE_T_NSto220 = '2021_03_31_NORM_O3_T_NSto220_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_T_NSto220 = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_T_NSto220,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_T_NSto220,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsDELQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsDELQ = '2021_02_01_O3_LHF_nsDELQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsDELQ = '2021_02_01_NORM_O3_LHF_nsDELQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsDELQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsDELQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsDELQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsQ = '2021_02_01_O3_LHF_nsQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsQ = '2021_02_01_NORM_O3_LHF_nsQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Combined (latest flexible version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "#if path==path_aquaplanet: out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In physical space\n",
    "# TRAINFILE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "# VALIDFILE = '2021_03_18_O3_VALID_M4K.nc'\n",
    "# TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "# TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_M4K_shuffle.nc'\n",
    "TRAINFILE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "VALIDFILE = '2021_03_18_O3_VALID_P4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_P4K_shuffle.nc'\n",
    "\n",
    "# In percentile space\n",
    "#TRAINFILE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "#TRAINFILE = '2021_01_24_O3_small_shuffle.nc'\n",
    "#VALIDFILE = '2021_04_09_PERC_VALID_M4K.nc'\n",
    "#TESTFILE = '2021_04_09_PERC_TEST_P4K.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old data generator by Ankitesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved flexible data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add callback class to track loss on multiple sets during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [https://stackoverflow.com/questions/47731935/using-multiple-validation-sets-with-keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_diffgeog_gen_CI[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            valid_generator,valid_name = validation_set\n",
    "            #tf.print('Results')\n",
    "            results = self.model.evaluate_generator(generator=valid_generator)\n",
    "            #tf.print(results)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,[results]):\n",
    "                #tf.print(metric,result)\n",
    "                valuename = valid_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models tracking losses across climates/geography (Warm to Cold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR or Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_MLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 1160s 201ms/step - loss: 742.0739 - val_loss: 714.2344\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 1241s 216ms/step - loss: 699.6751 - val_loss: 687.2894\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 1250s 217ms/step - loss: 680.5441 - val_loss: 674.0122\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 1143s 199ms/step - loss: 670.4840 - val_loss: 666.8877\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1171s 203ms/step - loss: 664.6647 - val_loss: 662.6266\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1201s 209ms/step - loss: 660.8634 - val_loss: 659.6823\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 1170s 203ms/step - loss: 658.0548 - val_loss: 657.3923\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 1255s 218ms/step - loss: 655.8021 - val_loss: 655.4963\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 1274s 221ms/step - loss: 653.9026 - val_loss: 653.8405\n",
      "Epoch 10/20\n",
      "5757/5759 [============================>.] - ETA: 0s - loss: 652.2641"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_MLR_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BMSE'\n",
    "train_gen_T = train_gen_BMSE\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_17_W2C_MLR_RH_BMSE_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 5003s 869ms/step - loss: 736.8382 - val_loss: 707.1181\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 4891s 849ms/step - loss: 693.6267 - val_loss: 684.6549\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 4945s 859ms/step - loss: 679.9093 - val_loss: 675.9388\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 5074s 881ms/step - loss: 673.1540 - val_loss: 670.7144\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 4919s 854ms/step - loss: 668.7142 - val_loss: 667.0403\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 4958s 861ms/step - loss: 665.4504 - val_loss: 664.2064\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 5342s 928ms/step - loss: 662.8525 - val_loss: 661.9069\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 5012s 870ms/step - loss: 660.6993 - val_loss: 659.9792\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 4958s 861ms/step - loss: 658.8892 - val_loss: 658.3722\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 5084s 883ms/step - loss: 657.3453 - val_loss: 656.9799\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 4866s 845ms/step - loss: 656.0202 - val_loss: 655.7716\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 5027s 873ms/step - loss: 654.8780 - val_loss: 654.7538\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 4972s 863ms/step - loss: 653.8894 - val_loss: 653.8409\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 4862s 844ms/step - loss: 653.0305 - val_loss: 653.0766\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 5317s 923ms/step - loss: 652.2808 - val_loss: 652.3976\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 5014s 871ms/step - loss: 651.6207 - val_loss: 651.7927\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 5193s 902ms/step - loss: 651.0351 - val_loss: 651.2384\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 5046s 876ms/step - loss: 650.5102 - val_loss: 650.7591\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 4973s 864ms/step - loss: 650.0370 - val_loss: 650.3163\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 5025s 873ms/step - loss: 649.6079 - val_loss: 649.9122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff3d07ae9b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [736.8382002308377,\n",
       "  693.6267317599525,\n",
       "  679.9092979636492,\n",
       "  673.1540137011261,\n",
       "  668.7142416226573,\n",
       "  665.450419623853,\n",
       "  662.8525016931718,\n",
       "  660.699270723677,\n",
       "  658.8891682638892,\n",
       "  657.3453387263584,\n",
       "  656.0201622468975,\n",
       "  654.8779864515731,\n",
       "  653.8893610349359,\n",
       "  653.0305173131694,\n",
       "  652.2807639443764,\n",
       "  651.6207137507766,\n",
       "  651.0351364949156,\n",
       "  650.5102129971159,\n",
       "  650.0369898906534,\n",
       "  649.607871695305],\n",
       " 'val_loss': [707.1180680998903,\n",
       "  684.6549348748782,\n",
       "  675.9388487394353,\n",
       "  670.7144277224637,\n",
       "  667.0402541589519,\n",
       "  664.2063778233128,\n",
       "  661.9068520281166,\n",
       "  659.9791512157318,\n",
       "  658.372245973396,\n",
       "  656.9798537145605,\n",
       "  655.7715662963185,\n",
       "  654.7538440671387,\n",
       "  653.8408591069954,\n",
       "  653.0765755879723,\n",
       "  652.3976332069797,\n",
       "  651.7926883481023,\n",
       "  651.238442176846,\n",
       "  650.7591341908213,\n",
       "  650.3163447288786,\n",
       "  649.9122428086255],\n",
       " 'trainM4K_loss': [327.32705814595397,\n",
       "  318.7806688465371,\n",
       "  315.41254870306597,\n",
       "  314.63722187405887,\n",
       "  314.69008588120226,\n",
       "  315.022111752605,\n",
       "  315.33471461894055,\n",
       "  315.8069295643063,\n",
       "  316.1530928693726,\n",
       "  316.62396453788534,\n",
       "  317.2524544685577,\n",
       "  317.61300329045093,\n",
       "  318.3338919108186,\n",
       "  318.7533899788476,\n",
       "  319.14277764711346,\n",
       "  319.5632865045981,\n",
       "  319.8870853636699,\n",
       "  320.1234742376742,\n",
       "  320.3472849519163,\n",
       "  320.57193931106644],\n",
       " 'trainP4K_RG_loss': [528.5217279236102,\n",
       "  571.105564769914,\n",
       "  601.0444944472852,\n",
       "  630.3807361695558,\n",
       "  660.4604146513714,\n",
       "  689.9908682918958,\n",
       "  719.1704992378901,\n",
       "  749.343155561237,\n",
       "  779.1035114023102,\n",
       "  808.760580866081,\n",
       "  838.0468820832216,\n",
       "  866.6536655496972,\n",
       "  894.9121128223076,\n",
       "  924.1478444932493,\n",
       "  951.2126768430347,\n",
       "  978.4825315007768,\n",
       "  1003.7782720362236,\n",
       "  1029.0123941498457,\n",
       "  1052.4025005797903,\n",
       "  1077.5180564624966]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In percentile space\n",
    "TRAINFILE = '2021_04_09_PERC_TRAIN_P4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_09_PERC_TEST_P4K.nc' # VALID_P4K gave nan values for the output\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_24_RG_PERC_TRAIN_P4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_1 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_LOGI_PERC_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 3261s 566ms/step - loss: 0.0476 - val_loss: 0.0405\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 3318s 576ms/step - loss: 0.0397 - val_loss: 0.0395\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 3293s 572ms/step - loss: 0.0394 - val_loss: 0.0394\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 3190s 554ms/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 3136s 545ms/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 3098s 538ms/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 3105s 539ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 3027s 526ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 3127s 543ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 3096s 538ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 3145s 546ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 3167s 550ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 2997s 520ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 3080s 535ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 3181s 552ms/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 3130s 543ms/step - loss: 0.0392 - val_loss: 0.0393\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 3117s 541ms/step - loss: 0.0392 - val_loss: 0.0393\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 3027s 526ms/step - loss: 0.0392 - val_loss: 0.0393\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 3133s 544ms/step - loss: 0.0392 - val_loss: 0.0393\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 3153s 547ms/step - loss: 0.0392 - val_loss: 0.0393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc03186bda0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_gen_CI[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(valid_gen_CI[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.047649630160562735,\n",
       "  0.03974937068508366,\n",
       "  0.039389253814723635,\n",
       "  0.039326359704330904,\n",
       "  0.03930367637741978,\n",
       "  0.03929065300705098,\n",
       "  0.039281487640204955,\n",
       "  0.03927466818484856,\n",
       "  0.03926932298256346,\n",
       "  0.03926478482279581,\n",
       "  0.039261215481726654,\n",
       "  0.039258111339436984,\n",
       "  0.03925538351938819,\n",
       "  0.03925317229276462,\n",
       "  0.03925120957321251,\n",
       "  0.03924940947011789,\n",
       "  0.03924771510601499,\n",
       "  0.03924636798890582,\n",
       "  0.039244983947500624,\n",
       "  0.039243929466531186],\n",
       " 'val_loss': [0.04053829237246086,\n",
       "  0.03951617566555306,\n",
       "  0.03941138769181033,\n",
       "  0.03937759561023582,\n",
       "  0.03936424106677611,\n",
       "  0.039356576442642156,\n",
       "  0.03934628743834215,\n",
       "  0.039339469735893196,\n",
       "  0.039332938969873346,\n",
       "  0.03933435990312077,\n",
       "  0.039329666535534685,\n",
       "  0.03932851697012057,\n",
       "  0.0393225569872903,\n",
       "  0.039322651467142096,\n",
       "  0.03931824980919225,\n",
       "  0.03931740258910226,\n",
       "  0.039319645527537105,\n",
       "  0.03931679014628713,\n",
       "  0.03931403778347065,\n",
       "  0.039312621835940524],\n",
       " 'trainM4K_loss': [0.04862951124217966,\n",
       "  0.046661666585874756,\n",
       "  0.0462891285322745,\n",
       "  0.04623751580407272,\n",
       "  0.04624802649427379,\n",
       "  0.04625441374803484,\n",
       "  0.046182042779861834,\n",
       "  0.04620840560168126,\n",
       "  0.04617842899276101,\n",
       "  0.046192379274091325,\n",
       "  0.046150355616477366,\n",
       "  0.04619787465169687,\n",
       "  0.046175698471407775,\n",
       "  0.04616298978877556,\n",
       "  0.0462012193064647,\n",
       "  0.046173370670818954,\n",
       "  0.04614504358667656,\n",
       "  0.046187837270010385,\n",
       "  0.04622961067078552,\n",
       "  0.046150877029686926],\n",
       " 'trainP4K_RG_loss': [0.08130582102463822,\n",
       "  0.08130477891672663,\n",
       "  0.08227109301925899,\n",
       "  0.08266486329667387,\n",
       "  0.08273399233775419,\n",
       "  0.08290691236185925,\n",
       "  0.08287949977685112,\n",
       "  0.08303673760914837,\n",
       "  0.083106392486839,\n",
       "  0.08308294519504934,\n",
       "  0.08305641187448187,\n",
       "  0.08334843288590127,\n",
       "  0.08312943751037717,\n",
       "  0.08321194971110858,\n",
       "  0.08328437655396728,\n",
       "  0.08311752449715769,\n",
       "  0.08331926613534468,\n",
       "  0.08334105443630437,\n",
       "  0.08310601032001948,\n",
       "  0.08322247464726752]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_2 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_LOGI_PERC_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 12382s 2s/step - loss: 0.0475 - val_loss: 0.0407\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 12508s 2s/step - loss: 0.0400 - val_loss: 0.0397\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 12167s 2s/step - loss: 0.0396 - val_loss: 0.0396\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 11962s 2s/step - loss: 0.0395 - val_loss: 0.0395\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 11892s 2s/step - loss: 0.0394 - val_loss: 0.0395\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 11992s 2s/step - loss: 0.0394 - val_loss: 0.0394\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 11713s 2s/step - loss: 0.0394 - val_loss: 0.0394\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 12444s 2s/step - loss: 0.0394 - val_loss: 0.0394\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 11901s 2s/step - loss: 0.0394 - val_loss: 0.0394\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 12297s 2s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 13297s 2s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 12417s 2s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 12452s 2s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 12600s 2s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 12003s 2s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 15786s 3s/step - loss: 0.0393 - val_loss: 0.0394\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 17526s 3s/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 13023s 2s/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 12509s 2s/step - loss: 0.0393 - val_loss: 0.0393\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 12805s 2s/step - loss: 0.0393 - val_loss: 0.0393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc0300746a0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.047501670945397596,\n",
       "  0.039957920787006974,\n",
       "  0.03957343976967325,\n",
       "  0.03947178226201707,\n",
       "  0.03942311132486174,\n",
       "  0.039396011386533064,\n",
       "  0.03937745594260572,\n",
       "  0.03936285840583191,\n",
       "  0.03935053404262132,\n",
       "  0.039339756352151305,\n",
       "  0.03933047177730945,\n",
       "  0.039322014693005836,\n",
       "  0.039314426973987356,\n",
       "  0.03930753781449714,\n",
       "  0.039301214792399716,\n",
       "  0.039295388706174654,\n",
       "  0.03928992721583823,\n",
       "  0.03928501834472883,\n",
       "  0.0392802693092082,\n",
       "  0.03927582800828204],\n",
       " 'val_loss': [0.04072376293862711,\n",
       "  0.039715988653450315,\n",
       "  0.03956640492790688,\n",
       "  0.03950349956140062,\n",
       "  0.03946945472381035,\n",
       "  0.039446367157335266,\n",
       "  0.039427269188437475,\n",
       "  0.03941402994573472,\n",
       "  0.0394025735334389,\n",
       "  0.039393156090549275,\n",
       "  0.03938779421564252,\n",
       "  0.03937718707409606,\n",
       "  0.039371443121836394,\n",
       "  0.03936611643008551,\n",
       "  0.039356361510889025,\n",
       "  0.03935277921447481,\n",
       "  0.039346960115697424,\n",
       "  0.039342005239094965,\n",
       "  0.039338021451825265,\n",
       "  0.039333539525481194],\n",
       " 'trainM4K_loss': [0.04686979042441507,\n",
       "  0.045556953114433654,\n",
       "  0.04555805732566753,\n",
       "  0.04566314598999207,\n",
       "  0.04568228215292254,\n",
       "  0.04568105770841326,\n",
       "  0.0456894082477325,\n",
       "  0.04567602497253345,\n",
       "  0.045679456145373415,\n",
       "  0.04567350990349664,\n",
       "  0.04571838095592715,\n",
       "  0.04571786017146949,\n",
       "  0.04567501591436046,\n",
       "  0.04570196133238261,\n",
       "  0.04571532072924061,\n",
       "  0.04571075977462927,\n",
       "  0.045700510278445065,\n",
       "  0.04570808743287921,\n",
       "  0.04566995114512401,\n",
       "  0.04574402381025124],\n",
       " 'trainP4K_RG_loss': [0.07881821527075188,\n",
       "  0.07888840415233876,\n",
       "  0.08035849822437302,\n",
       "  0.08106865455664279,\n",
       "  0.08131882843626756,\n",
       "  0.08162835809422153,\n",
       "  0.08168391657660107,\n",
       "  0.0817071415950812,\n",
       "  0.08183108430734179,\n",
       "  0.08154397532450454,\n",
       "  0.08162135236679399,\n",
       "  0.0816120392983734,\n",
       "  0.08155639725863849,\n",
       "  0.0816459070173797,\n",
       "  0.08172878009908019,\n",
       "  0.08167518106695579,\n",
       "  0.08146730147078314,\n",
       "  0.08154524011673334,\n",
       "  0.0814288883463337,\n",
       "  0.08149857920638481]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "#if path==path_aquaplanet: out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In physical space\n",
    "TRAINFILE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "VALIDFILE = '2021_03_18_O3_VALID_P4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_P4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 1570s 273ms/step - loss: 454.6388 - val_loss: 422.7343\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 1615s 280ms/step - loss: 414.1249 - val_loss: 405.4220\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 1579s 274ms/step - loss: 400.5938 - val_loss: 398.9153\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 1559s 271ms/step - loss: 391.7820 - val_loss: 387.4661\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1496s 260ms/step - loss: 385.8460 - val_loss: 382.9512\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1635s 284ms/step - loss: 381.5710 - val_loss: 378.2229\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 1678s 291ms/step - loss: 378.5299 - val_loss: 377.5563\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 1489s 259ms/step - loss: 376.4825 - val_loss: 377.0201\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 1653s 287ms/step - loss: 374.5604 - val_loss: 374.0197\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 1566s 272ms/step - loss: 372.6192 - val_loss: 374.0344\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 1530s 266ms/step - loss: 370.9549 - val_loss: 373.7029\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 1619s 281ms/step - loss: 369.4435 - val_loss: 372.2434\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 1452s 252ms/step - loss: 368.3710 - val_loss: 371.2019\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 1735s 301ms/step - loss: 367.3523 - val_loss: 366.6743\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 1622s 282ms/step - loss: 366.5291 - val_loss: 365.7835\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 1564s 272ms/step - loss: 365.6831 - val_loss: 366.0618\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 1678s 291ms/step - loss: 364.9191 - val_loss: 364.0993\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 1438s 250ms/step - loss: 364.2485 - val_loss: 364.2239\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 1540s 267ms/step - loss: 363.5796 - val_loss: 363.5715\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 1420s 247ms/step - loss: 363.0935 - val_loss: 362.2426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbff8400b38>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [454.6388372761401,\n",
       "  414.12489873399585,\n",
       "  400.5938128050592,\n",
       "  391.78204361070544,\n",
       "  385.845987114937,\n",
       "  381.57097559890974,\n",
       "  378.5298877364336,\n",
       "  376.4825027182318,\n",
       "  374.5603823058202,\n",
       "  372.61919528532616,\n",
       "  370.9548911034885,\n",
       "  369.4434512244846,\n",
       "  368.3710246972894,\n",
       "  367.3523237702333,\n",
       "  366.5290705340876,\n",
       "  365.68312901799766,\n",
       "  364.9190860992712,\n",
       "  364.2484729976823,\n",
       "  363.57955766755026,\n",
       "  363.09348276055175],\n",
       " 'val_loss': [422.734306949368,\n",
       "  405.42197440055475,\n",
       "  398.915293584168,\n",
       "  387.46610209474557,\n",
       "  382.95115956560585,\n",
       "  378.22292842150904,\n",
       "  377.556324339228,\n",
       "  377.0200837406893,\n",
       "  374.0197091597774,\n",
       "  374.0343704029764,\n",
       "  373.70291266941365,\n",
       "  372.2434392066683,\n",
       "  371.20190779168826,\n",
       "  366.67428423949224,\n",
       "  365.7835237275902,\n",
       "  366.061759358076,\n",
       "  364.09927124147515,\n",
       "  364.22389820117615,\n",
       "  363.57149296107383,\n",
       "  362.24255902553523],\n",
       " 'trainM4K_loss': [533.3773562389439,\n",
       "  385.9299836990878,\n",
       "  458.9240124545633,\n",
       "  901.9590901558464,\n",
       "  931.982477674635,\n",
       "  878.9030809092799,\n",
       "  1017.5792395195461,\n",
       "  797.875817345422,\n",
       "  998.9819750433934,\n",
       "  917.4072283959923,\n",
       "  882.6641854817595,\n",
       "  458.27347226216574,\n",
       "  758.6861311231436,\n",
       "  552.4603109225608,\n",
       "  576.6127356694667,\n",
       "  901.6682069874157,\n",
       "  525.59084497986,\n",
       "  710.6390993012635,\n",
       "  642.576290426703,\n",
       "  651.3208115914357],\n",
       " 'trainP4K_RG_loss': [2255.660084659866,\n",
       "  2767.374997202337,\n",
       "  5479.586718756986,\n",
       "  6308.435292361018,\n",
       "  5078.56707350484,\n",
       "  4168.779237834183,\n",
       "  4436.614384762132,\n",
       "  3635.920343648013,\n",
       "  4725.702459669646,\n",
       "  4108.492584656373,\n",
       "  3963.2449176837445,\n",
       "  2929.183661669293,\n",
       "  3073.3474258112874,\n",
       "  2329.2740834702477,\n",
       "  2065.1415345862392,\n",
       "  2504.4793523986555,\n",
       "  1354.2670768403802,\n",
       "  1695.3418890756327,\n",
       "  1453.5330845578785,\n",
       "  1493.9574848625828]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_NN_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 3372s 586ms/step - loss: 432.2796 - val_loss: 406.5870\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 3400s 590ms/step - loss: 394.4321 - val_loss: 388.0163\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 3436s 597ms/step - loss: 382.2172 - val_loss: 380.3640\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 3350s 582ms/step - loss: 375.1767 - val_loss: 373.6486\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 3317s 576ms/step - loss: 370.5738 - val_loss: 369.4698\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 3324s 577ms/step - loss: 367.5660 - val_loss: 367.3654\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 3274s 569ms/step - loss: 365.2814 - val_loss: 367.1149\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 3290s 571ms/step - loss: 363.3328 - val_loss: 364.3838\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 3341s 580ms/step - loss: 361.7881 - val_loss: 361.4773\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 3347s 581ms/step - loss: 360.5219 - val_loss: 360.9869\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 3220s 559ms/step - loss: 359.4649 - val_loss: 359.9243\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 3246s 564ms/step - loss: 358.5651 - val_loss: 358.5647\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 3314s 575ms/step - loss: 357.7212 - val_loss: 359.9324\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 3339s 580ms/step - loss: 357.1338 - val_loss: 357.2942\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 3201s 556ms/step - loss: 356.3912 - val_loss: 356.5736\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 3279s 569ms/step - loss: 355.8155 - val_loss: 356.3700\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 3294s 572ms/step - loss: 355.2491 - val_loss: 355.4182\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 3379s 587ms/step - loss: 354.8190 - val_loss: 356.0708\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 3391s 589ms/step - loss: 354.3764 - val_loss: 355.9778\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 3371s 585ms/step - loss: 353.9981 - val_loss: 354.2165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc03114feb8>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [432.27960677228884,\n",
       "  394.4320966081707,\n",
       "  382.21718972986577,\n",
       "  375.1766512685816,\n",
       "  370.5737759828112,\n",
       "  367.5660440239937,\n",
       "  365.2814293589975,\n",
       "  363.3327822134631,\n",
       "  361.7881198200134,\n",
       "  360.5218951991499,\n",
       "  359.46493838151764,\n",
       "  358.5651296624543,\n",
       "  357.72119553425716,\n",
       "  357.13380135113755,\n",
       "  356.3912042987742,\n",
       "  355.8154519066709,\n",
       "  355.2490794173047,\n",
       "  354.81901096954056,\n",
       "  354.3763842813711,\n",
       "  353.99810218181767],\n",
       " 'val_loss': [406.5870342096312,\n",
       "  388.0162791918238,\n",
       "  380.3640273254281,\n",
       "  373.64860374661356,\n",
       "  369.4697700046835,\n",
       "  367.3654327366729,\n",
       "  367.11487370613327,\n",
       "  364.3838272831429,\n",
       "  361.4772781405674,\n",
       "  360.98685087875737,\n",
       "  359.92429414490977,\n",
       "  358.56465081976324,\n",
       "  359.9323656229817,\n",
       "  357.2942246545801,\n",
       "  356.5735619422361,\n",
       "  356.3699889392262,\n",
       "  355.41817328963424,\n",
       "  356.07075688412044,\n",
       "  355.9778488342709,\n",
       "  354.21652608900865],\n",
       " 'trainM4K_loss': [545.2628909980574,\n",
       "  555.523445257898,\n",
       "  691.1638632216456,\n",
       "  556.5260610843744,\n",
       "  581.1790194517209,\n",
       "  546.5136369114681,\n",
       "  640.1976257472594,\n",
       "  582.1513671027142,\n",
       "  604.435091162084,\n",
       "  618.6299796272513,\n",
       "  579.9165769173962,\n",
       "  605.0032186109089,\n",
       "  644.4846833605467,\n",
       "  601.2768577473676,\n",
       "  576.5027869559551,\n",
       "  570.6456201748418,\n",
       "  605.574074333338,\n",
       "  465.98088895357574,\n",
       "  586.6540660457409,\n",
       "  543.6819551378142],\n",
       " 'trainP4K_RG_loss': [591.4382401128286,\n",
       "  576.3378822809091,\n",
       "  666.958006822316,\n",
       "  705.7818658159117,\n",
       "  722.8559131068938,\n",
       "  795.5562175387272,\n",
       "  787.6626227374206,\n",
       "  846.5773229212072,\n",
       "  817.5732274988964,\n",
       "  941.5282615825342,\n",
       "  960.137168649829,\n",
       "  930.113420546372,\n",
       "  1054.5761306051056,\n",
       "  1107.5116756149969,\n",
       "  998.0562085030791,\n",
       "  1008.9763842878219,\n",
       "  1205.618452277668,\n",
       "  1181.702162593465,\n",
       "  1413.1622566976262,\n",
       "  1283.5224662988142]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_NN_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 12010s 2s/step - loss: 439.8130 - val_loss: 412.8898\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 11966s 2s/step - loss: 404.3274 - val_loss: 399.0884\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 12237s 2s/step - loss: 393.6856 - val_loss: 388.9921\n",
      "Epoch 4/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 387.1949"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BMSE'\n",
    "train_gen_T = train_gen_BMSE\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_17_W2C_NN_RH_BMSE_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 5022s 872ms/step - loss: 432.7474 - val_loss: 404.7687\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 5812s 1s/step - loss: 394.2622 - val_loss: 385.8932\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 5063s 879ms/step - loss: 382.0637 - val_loss: 377.0837\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 5133s 891ms/step - loss: 375.2787 - val_loss: 373.8155\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 5112s 888ms/step - loss: 371.0315 - val_loss: 368.7421\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 5107s 887ms/step - loss: 367.9890 - val_loss: 365.8757\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 5206s 904ms/step - loss: 365.5092 - val_loss: 364.5176\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 5170s 898ms/step - loss: 363.4911 - val_loss: 362.2824\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 5595s 972ms/step - loss: 361.9082 - val_loss: 362.8249\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 5029s 873ms/step - loss: 360.5273 - val_loss: 359.9241\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 5134s 891ms/step - loss: 359.5179 - val_loss: 361.3667\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 5143s 893ms/step - loss: 358.5659 - val_loss: 359.0468\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 5178s 899ms/step - loss: 357.7577 - val_loss: 356.7385\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 5074s 881ms/step - loss: 357.0551 - val_loss: 356.5375\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 5437s 944ms/step - loss: 356.3863 - val_loss: 358.3589\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 5163s 896ms/step - loss: 355.7718 - val_loss: 355.6464\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 5131s 891ms/step - loss: 355.1339 - val_loss: 358.3305\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 5146s 893ms/step - loss: 354.6899 - val_loss: 355.5149\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 5170s 898ms/step - loss: 354.2659 - val_loss: 355.2792\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 5545s 963ms/step - loss: 353.7959 - val_loss: 355.2115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff3d0751940>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [432.74737405222226,\n",
       "  394.2622252654897,\n",
       "  382.06366247649737,\n",
       "  375.27872453925715,\n",
       "  371.03149929136055,\n",
       "  367.98895044288696,\n",
       "  365.50919494787877,\n",
       "  363.49107419395017,\n",
       "  361.9082348401771,\n",
       "  360.52731010064747,\n",
       "  359.5178892410843,\n",
       "  358.56589816067446,\n",
       "  357.7576664459128,\n",
       "  357.055145030511,\n",
       "  356.3863022875964,\n",
       "  355.771760416271,\n",
       "  355.1339050388353,\n",
       "  354.68985575142574,\n",
       "  354.26585554840625,\n",
       "  353.7959212289748],\n",
       " 'val_loss': [404.7687068152747,\n",
       "  385.89320047059624,\n",
       "  377.08373675507124,\n",
       "  373.81554922458986,\n",
       "  368.74210009864885,\n",
       "  365.87572868407915,\n",
       "  364.5176350294524,\n",
       "  362.28236105594317,\n",
       "  362.8248927681684,\n",
       "  359.92413762276925,\n",
       "  361.36669607031615,\n",
       "  359.0467891030729,\n",
       "  356.73849182284,\n",
       "  356.5375357835793,\n",
       "  358.3588891795949,\n",
       "  355.64642168549426,\n",
       "  358.3305268724672,\n",
       "  355.5148581986748,\n",
       "  355.27922830221394,\n",
       "  355.2115216508026],\n",
       " 'trainM4K_loss': [220.47777355344454,\n",
       "  219.27173378824008,\n",
       "  212.11945852659542,\n",
       "  213.04786175268976,\n",
       "  208.3534409605312,\n",
       "  206.6414937467951,\n",
       "  204.81214460189776,\n",
       "  206.6357133656373,\n",
       "  204.3123967441798,\n",
       "  205.38537097950118,\n",
       "  198.5640245594277,\n",
       "  203.32307317501596,\n",
       "  207.92465934614316,\n",
       "  205.1719706595451,\n",
       "  203.8428088302599,\n",
       "  201.78584603523086,\n",
       "  203.16771196039792,\n",
       "  205.02484616257908,\n",
       "  203.02570927641958,\n",
       "  202.32760249709355],\n",
       " 'trainP4K_RG_loss': [1718.7563996644901,\n",
       "  1551.1326796197413,\n",
       "  1478.5284012805682,\n",
       "  1970.5932535196066,\n",
       "  1574.3005166892297,\n",
       "  2619.1408309198687,\n",
       "  1805.0250921307784,\n",
       "  1980.8052429356392,\n",
       "  2060.853796676614,\n",
       "  2397.9853172500893,\n",
       "  1263.8197512350232,\n",
       "  1467.4322743358257,\n",
       "  1936.4854741846611,\n",
       "  1874.4500408228273,\n",
       "  1271.453310899639,\n",
       "  1240.8129547337435,\n",
       "  1800.668773834272,\n",
       "  1251.2393862081153,\n",
       "  1815.107254472768,\n",
       "  1440.020476070726]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In percentile space\n",
    "TRAINFILE = '2021_04_09_PERC_TRAIN_P4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_09_PERC_TEST_P4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_24_RG_PERC_TRAIN_P4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_NN_PERC_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_W2C_NN_PERC_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BMSE'\n",
    "train_gen_T = train_gen_BMSE\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 120)               15480     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid (TensorF [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_17_W2C_NN_PERC_RH_BMSE_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 5112s 888ms/step - loss: 4.6703e-04 - val_loss: 1.0698e-09\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 5197s 902ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 5085s 883ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 5274s 916ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 5317s 923ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 5144s 893ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 5193s 902ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 5340s 927ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 5149s 894ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 5177s 899ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 5322s 924ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 5492s 954ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 5249s 912ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 5639s 979ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 5205s 904ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 5168s 897ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 5893s 1s/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 5254s 912ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 5589s 970ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 5209s 904ms/step - loss: 1.0651e-09 - val_loss: 1.0698e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff3d0759a90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.00046703280939909303,\n",
       "  1.0651308703280443e-09,\n",
       "  1.0651308702702103e-09,\n",
       "  1.0651308702894884e-09,\n",
       "  1.0651308703666004e-09,\n",
       "  1.0651308703858785e-09,\n",
       "  1.0651308703666004e-09,\n",
       "  1.0651308703858785e-09,\n",
       "  1.0651308702894884e-09,\n",
       "  1.0651308702702103e-09,\n",
       "  1.0651308702316542e-09,\n",
       "  1.0651308701352639e-09,\n",
       "  1.065130870154542e-09,\n",
       "  1.0651308701159858e-09,\n",
       "  1.065130870193098e-09,\n",
       "  1.065130870193098e-09,\n",
       "  1.065130870154542e-09,\n",
       "  1.06513087017382e-09,\n",
       "  1.0651308701352639e-09,\n",
       "  1.0651308700581516e-09],\n",
       " 'val_loss': [1.0697681995893046e-09,\n",
       "  1.0697681995893046e-09,\n",
       "  1.0697681995893046e-09,\n",
       "  1.0697681995422852e-09,\n",
       "  1.0697681995799006e-09,\n",
       "  1.0697681995046695e-09,\n",
       "  1.0697681994670541e-09,\n",
       "  1.0697681995422852e-09,\n",
       "  1.0697681995234773e-09,\n",
       "  1.0697681994670541e-09,\n",
       "  1.0697681996175162e-09,\n",
       "  1.0697681995234773e-09,\n",
       "  1.0697681995987084e-09,\n",
       "  1.0697681995799006e-09,\n",
       "  1.0697681996175162e-09,\n",
       "  1.0697681996739396e-09,\n",
       "  1.069768199636324e-09,\n",
       "  1.0697681996927472e-09,\n",
       "  1.069768199636324e-09,\n",
       "  1.069768199561093e-09],\n",
       " 'trainP4K_loss': [6.856446646670683e-10,\n",
       "  6.856446646767073e-10,\n",
       "  6.856446646574293e-10,\n",
       "  6.856446645995952e-10,\n",
       "  6.856446646381512e-10,\n",
       "  6.856446646092342e-10,\n",
       "  6.856446646477902e-10,\n",
       "  6.85644664541761e-10,\n",
       "  6.856446644839268e-10,\n",
       "  6.856446644453708e-10,\n",
       "  6.856446644453708e-10,\n",
       "  6.85644664561039e-10,\n",
       "  6.856446646188732e-10,\n",
       "  6.856446646863464e-10,\n",
       "  6.856446646670683e-10,\n",
       "  6.856446647538196e-10,\n",
       "  6.856446646863464e-10,\n",
       "  6.856446646574293e-10,\n",
       "  6.856446646188732e-10,\n",
       "  6.856446647827367e-10],\n",
       " 'trainM4K_RG_loss': [1.1459357937460654e-09,\n",
       "  1.1459357937460654e-09,\n",
       "  1.1459357937015929e-09,\n",
       "  1.1459357937206525e-09,\n",
       "  1.1459357937714782e-09,\n",
       "  1.1459357937778314e-09,\n",
       "  1.1459357938032442e-09,\n",
       "  1.1459357938413634e-09,\n",
       "  1.1459357938540697e-09,\n",
       "  1.1459357939366615e-09,\n",
       "  1.1459357939303083e-09,\n",
       "  1.1459357939303083e-09,\n",
       "  1.1459357938794826e-09,\n",
       "  1.1459357938794826e-09,\n",
       "  1.1459357938604231e-09,\n",
       "  1.1459357938604231e-09,\n",
       "  1.1459357939112486e-09,\n",
       "  1.145935793885836e-09,\n",
       "  1.1459357938921891e-09,\n",
       "  1.1459357939112486e-09]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate-invariant (T,Q,PS,S0,SHF,LHF)->($\\dot{T}$,$\\dot{q}$,RADFLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ozone (T,Q,$O_{3}$,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(94,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_O3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ozone (T,Q,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_noO3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nep = 15\n",
    "# model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "#               callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_MLR_PERC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 1184s 206ms/step - loss: 0.0510 - val_loss: 0.0429\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 1035s 180ms/step - loss: 0.0419 - val_loss: 0.0427\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 753s 131ms/step - loss: 0.0417 - val_loss: 0.0425\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 726s 126ms/step - loss: 0.0416 - val_loss: 0.0424\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 765s 133ms/step - loss: 0.0415 - val_loss: 0.0424\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 0.0415 - val_loss: 0.0424\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 756s 131ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 766s 133ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 763s 132ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 729s 127ms/step - loss: 0.0414 - val_loss: 0.0424\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 774s 134ms/step - loss: 0.0414 - val_loss: 0.0422\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 744s 129ms/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 750s 130ms/step - loss: 0.0414 - val_loss: 0.0421\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 731s 127ms/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 744s 129ms/step - loss: 0.0414 - val_loss: 0.0425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402db4240>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.050982531596102595,\n",
       "  0.04185105333621251,\n",
       "  0.04165498730630319,\n",
       "  0.04157832884143202,\n",
       "  0.04153471013858079,\n",
       "  0.041505846159183686,\n",
       "  0.04148416520251771,\n",
       "  0.04146769321144383,\n",
       "  0.04145484425892099,\n",
       "  0.04144392009125254,\n",
       "  0.04143447252735948,\n",
       "  0.04142640940075244,\n",
       "  0.04141932996618994,\n",
       "  0.04141429779437562,\n",
       "  0.04140815107314552],\n",
       " 'val_loss': [0.042909197750994055,\n",
       "  0.04265815539936504,\n",
       "  0.04253125871611655,\n",
       "  0.0423782235956507,\n",
       "  0.04237728513551328,\n",
       "  0.04235305504845943,\n",
       "  0.04230615223943935,\n",
       "  0.04227608388790575,\n",
       "  0.0423463198359328,\n",
       "  0.042378264729721796,\n",
       "  0.042240786968061286,\n",
       "  0.04227235703251836,\n",
       "  0.042130285002797335,\n",
       "  0.04226516442969454,\n",
       "  0.04249188080922277],\n",
       " 'testP4K_loss': [0.04237089483123066,\n",
       "  0.04204679281155743,\n",
       "  0.041950938805466936,\n",
       "  0.041866756795589787,\n",
       "  0.04182252665865533,\n",
       "  0.04181972140371545,\n",
       "  0.041768535935374836,\n",
       "  0.041756746478308786,\n",
       "  0.041807406456754515,\n",
       "  0.04179418787080238,\n",
       "  0.041728623833007836,\n",
       "  0.041724651345679334,\n",
       "  0.04165810040754197,\n",
       "  0.04172214069534936,\n",
       "  0.0418310413666763]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.050982531596102595,\n",
       "  0.04185105333621251,\n",
       "  0.04165498730630319,\n",
       "  0.04157832884143202,\n",
       "  0.04153471013858079,\n",
       "  0.041505846159183686,\n",
       "  0.04148416520251771,\n",
       "  0.04146769321144383,\n",
       "  0.04145484425892099,\n",
       "  0.04144392009125254,\n",
       "  0.04143447252735948,\n",
       "  0.04142640940075244,\n",
       "  0.04141932996618994,\n",
       "  0.04141429779437562,\n",
       "  0.04140815107314552],\n",
       " 'val_loss': [0.042909197750994055,\n",
       "  0.04265815539936504,\n",
       "  0.04253125871611655,\n",
       "  0.0423782235956507,\n",
       "  0.04237728513551328,\n",
       "  0.04235305504845943,\n",
       "  0.04230615223943935,\n",
       "  0.04227608388790575,\n",
       "  0.0423463198359328,\n",
       "  0.042378264729721796,\n",
       "  0.042240786968061286,\n",
       "  0.04227235703251836,\n",
       "  0.042130285002797335,\n",
       "  0.04226516442969454,\n",
       "  0.04249188080922277],\n",
       " 'testP4K_loss': [0.04237089483123066,\n",
       "  0.04204679281155743,\n",
       "  0.041950938805466936,\n",
       "  0.041866756795589787,\n",
       "  0.04182252665865533,\n",
       "  0.04181972140371545,\n",
       "  0.041768535935374836,\n",
       "  0.041756746478308786,\n",
       "  0.041807406456754515,\n",
       "  0.04179418787080238,\n",
       "  0.041728623833007836,\n",
       "  0.041724651345679334,\n",
       "  0.04165810040754197,\n",
       "  0.04172214069534936,\n",
       "  0.0418310413666763]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF Logistic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_1 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_Log_PERC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 794s 138ms/step - loss: 0.0492 - val_loss: 0.0427\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 761s 132ms/step - loss: 0.0411 - val_loss: 0.0412\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 842s 146ms/step - loss: 0.0405 - val_loss: 0.0410\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 0.0403 - val_loss: 0.0408\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 727s 126ms/step - loss: 0.0402 - val_loss: 0.0408\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 727s 126ms/step - loss: 0.0401 - val_loss: 0.0407\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 738s 128ms/step - loss: 0.0401 - val_loss: 0.0406\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 740s 129ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 769s 133ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 768s 133ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 738s 128ms/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 687s 119ms/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 639s 111ms/step - loss: 0.0399 - val_loss: 0.0405\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 749s 130ms/step - loss: 0.0399 - val_loss: 0.0405\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 0.0399 - val_loss: 0.0404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402d40668>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.04923291314722536,\n",
       "  0.04110610745569074,\n",
       "  0.04050010521521811,\n",
       "  0.040299923023502326,\n",
       "  0.040188955621450057,\n",
       "  0.040115493637423776,\n",
       "  0.040061687980442955,\n",
       "  0.04002036269023862,\n",
       "  0.039987151799029744,\n",
       "  0.03995977671317889,\n",
       "  0.03993697982862977,\n",
       "  0.03991729508678539,\n",
       "  0.0399002306710558,\n",
       "  0.03988536803773406,\n",
       "  0.03987165327315054],\n",
       " 'val_loss': [0.04265024852781968,\n",
       "  0.041217454327464445,\n",
       "  0.04097958452281418,\n",
       "  0.04079991503641219,\n",
       "  0.04078920084213819,\n",
       "  0.04070766954445322,\n",
       "  0.04064866158667609,\n",
       "  0.0406280841281429,\n",
       "  0.04056495156529776,\n",
       "  0.04057289374916206,\n",
       "  0.040560972740690364,\n",
       "  0.0405654035945343,\n",
       "  0.0404891143274493,\n",
       "  0.040506530972142435,\n",
       "  0.0404415049514598],\n",
       " 'testP4K_loss': [0.07411513028238494,\n",
       "  0.07390664614972449,\n",
       "  0.07520650478889554,\n",
       "  0.07573994763074626,\n",
       "  0.0764975735435722,\n",
       "  0.0768441669025755,\n",
       "  0.07727208656825946,\n",
       "  0.07749857328113179,\n",
       "  0.07786896164909984,\n",
       "  0.07816397840018766,\n",
       "  0.07837125465184781,\n",
       "  0.07873877331958073,\n",
       "  0.07878147320397179,\n",
       "  0.07903415168914706,\n",
       "  0.07906030325885516]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.04923291314722536,\n",
       "  0.04110610745569074,\n",
       "  0.04050010521521811,\n",
       "  0.040299923023502326,\n",
       "  0.040188955621450057,\n",
       "  0.040115493637423776,\n",
       "  0.040061687980442955,\n",
       "  0.04002036269023862,\n",
       "  0.039987151799029744,\n",
       "  0.03995977671317889,\n",
       "  0.03993697982862977,\n",
       "  0.03991729508678539,\n",
       "  0.0399002306710558,\n",
       "  0.03988536803773406,\n",
       "  0.03987165327315054],\n",
       " 'val_loss': [0.04265024852781968,\n",
       "  0.041217454327464445,\n",
       "  0.04097958452281418,\n",
       "  0.04079991503641219,\n",
       "  0.04078920084213819,\n",
       "  0.04070766954445322,\n",
       "  0.04064866158667609,\n",
       "  0.0406280841281429,\n",
       "  0.04056495156529776,\n",
       "  0.04057289374916206,\n",
       "  0.040560972740690364,\n",
       "  0.0405654035945343,\n",
       "  0.0404891143274493,\n",
       "  0.040506530972142435,\n",
       "  0.0404415049514598],\n",
       " 'testP4K_loss': [0.07411513028238494,\n",
       "  0.07390664614972449,\n",
       "  0.07520650478889554,\n",
       "  0.07573994763074626,\n",
       "  0.0764975735435722,\n",
       "  0.0768441669025755,\n",
       "  0.07727208656825946,\n",
       "  0.07749857328113179,\n",
       "  0.07786896164909984,\n",
       "  0.07816397840018766,\n",
       "  0.07837125465184781,\n",
       "  0.07873877331958073,\n",
       "  0.07878147320397179,\n",
       "  0.07903415168914706,\n",
       "  0.07906030325885516]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_08_NN6L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2542s 441ms/step - loss: 342.0934 - val_loss: 329.9304\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2701s 469ms/step - loss: 320.9061 - val_loss: 316.9567\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 2452s 426ms/step - loss: 311.2353 - val_loss: 309.6577\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 2254s 391ms/step - loss: 305.5034 - val_loss: 304.9600\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1731s 301ms/step - loss: 301.7379 - val_loss: 301.6836\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1117s 194ms/step - loss: 299.0327 - val_loss: 299.1967\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 816s 142ms/step - loss: 296.9427 - val_loss: 297.2431\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 295.2764 - val_loss: 295.7120\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 694s 121ms/step - loss: 293.9242 - val_loss: 294.4352\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 292.8106 - val_loss: 293.3954\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 712s 124ms/step - loss: 291.8827 - val_loss: 292.5364\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 291.1030 - val_loss: 291.8280\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 290.4395 - val_loss: 291.2006\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 699s 121ms/step - loss: 289.8687 - val_loss: 290.6818\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 702s 122ms/step - loss: 289.3730 - val_loss: 290.2150\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 703s 122ms/step - loss: 288.9378 - val_loss: 289.8255\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 684s 119ms/step - loss: 288.5535 - val_loss: 289.4663\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 694s 120ms/step - loss: 288.2098 - val_loss: 289.1482\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 287.9012 - val_loss: 288.8622\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 287.6209 - val_loss: 288.5948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee43236630>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH Logistic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_2 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_Log_PERC_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 1103s 191ms/step - loss: 0.0467 - val_loss: 0.0405\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 1136s 197ms/step - loss: 0.0392 - val_loss: 0.0394\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 1123s 195ms/step - loss: 0.0389 - val_loss: 0.0394\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 1115s 194ms/step - loss: 0.0388 - val_loss: 0.0393\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 1128s 196ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 1082s 188ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 1136s 197ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 1069s 186ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 1093s 190ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 1121s 195ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 1114s 193ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 1146s 199ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 1111s 193ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 1079s 187ms/step - loss: 0.0386 - val_loss: 0.0392\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 1139s 198ms/step - loss: 0.0386 - val_loss: 0.0392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402f0acc0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.0467287344276843,\n",
       "  0.0392072251961502,\n",
       "  0.038853517252732575,\n",
       "  0.03877987418298528,\n",
       "  0.03874506841666439,\n",
       "  0.03872233596301302,\n",
       "  0.03870567677141188,\n",
       "  0.03869225124110984,\n",
       "  0.03868109595084236,\n",
       "  0.03867188213775712,\n",
       "  0.0386642090824872,\n",
       "  0.03865747418455789,\n",
       "  0.03865132474455802,\n",
       "  0.03864620624032156,\n",
       "  0.03864140413024914],\n",
       " 'val_loss': [0.040451113186371485,\n",
       "  0.03944033429256318,\n",
       "  0.03935017663365135,\n",
       "  0.03931213896163092,\n",
       "  0.03929163821896072,\n",
       "  0.03926565892478177,\n",
       "  0.039275243295873806,\n",
       "  0.039249983558271286,\n",
       "  0.0392355400208758,\n",
       "  0.03925549958671793,\n",
       "  0.03922620319762756,\n",
       "  0.03919682341832721,\n",
       "  0.03921325347639839,\n",
       "  0.039212426125584225,\n",
       "  0.03919467062651657],\n",
       " 'testP4K_loss': [0.06156855558834964,\n",
       "  0.05892425673790308,\n",
       "  0.05871588077863804,\n",
       "  0.058408490616755036,\n",
       "  0.05826237939559019,\n",
       "  0.05821150895383091,\n",
       "  0.058247450274676975,\n",
       "  0.05818161563383118,\n",
       "  0.05822653928238749,\n",
       "  0.05826851157272707,\n",
       "  0.05817507403042966,\n",
       "  0.058177342630420154,\n",
       "  0.05819879022675839,\n",
       "  0.05813914520759851,\n",
       "  0.05809993178835799]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   3/5759 [..............................] - ETA: 56:44 - loss: 362.5890  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QSATdeficit linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_QSATdeficit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfromNS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_TfromNS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCONS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8098s 1s/step - loss: 324.9880 - val_loss: 313.2678\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 12555s 2s/step - loss: 308.3165 - val_loss: 307.0678\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 13642s 2s/step - loss: 304.0913 - val_loss: 304.2872\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10465s 2s/step - loss: 301.8938 - val_loss: 302.7330\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7909s 1s/step - loss: 300.5873 - val_loss: 301.7510\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 8526s 1s/step - loss: 299.7002 - val_loss: 301.0397\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7230s 1s/step - loss: 299.0066 - val_loss: 300.3937\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 8294s 1s/step - loss: 298.4328 - val_loss: 299.8434\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 8250s 1s/step - loss: 297.9426 - val_loss: 299.4925\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8909s 2s/step - loss: 297.5149 - val_loss: 299.0509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcce5600f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [324.98798001675704,\n",
       "  308.31648811596017,\n",
       "  304.0912586973243,\n",
       "  301.8937511361293,\n",
       "  300.5873382457078,\n",
       "  299.7002091802725,\n",
       "  299.0065637804771,\n",
       "  298.4328209674489,\n",
       "  297.9425933249689,\n",
       "  297.5149222372472],\n",
       " 'val_loss': [313.2678370875624,\n",
       "  307.0677993880396,\n",
       "  304.2871500202989,\n",
       "  302.73299462630064,\n",
       "  301.75103527639715,\n",
       "  301.03966337007205,\n",
       "  300.39373488234764,\n",
       "  299.8433922314147,\n",
       "  299.4924658236212,\n",
       "  299.0508612894959],\n",
       " 'testP4K_loss': [696.3224985280676,\n",
       "  686.2635380876054,\n",
       "  682.7061903721445,\n",
       "  680.7840038270242,\n",
       "  679.5804400619129,\n",
       "  678.422916433854,\n",
       "  677.2276842855165,\n",
       "  676.2437589546028,\n",
       "  675.5998048438668,\n",
       "  674.6606088692057]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+(T-TNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_31_MLR_RH_NSto220'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 10546s 2s/step - loss: 333.2154 - val_loss: 318.1506\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10194s 2s/step - loss: 310.9067 - val_loss: 308.3093\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 9862s 2s/step - loss: 304.7300 - val_loss: 304.2289\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 11121s 2s/step - loss: 301.8291 - val_loss: 302.1135\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10748s 2s/step - loss: 300.2339 - val_loss: 300.8775\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10117s 2s/step - loss: 299.1923 - val_loss: 299.9938\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10174s 2s/step - loss: 298.3948 - val_loss: 299.3052\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 9887s 2s/step - loss: 297.7428 - val_loss: 298.7333\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10299s 2s/step - loss: 297.1934 - val_loss: 298.2455\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 10431s 2s/step - loss: 296.7218 - val_loss: 297.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9680546668>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.21542532209264,\n",
       "  310.90667960419796,\n",
       "  304.72998016670067,\n",
       "  301.8290961309116,\n",
       "  300.2339088743316,\n",
       "  299.19231390816446,\n",
       "  298.39477655642935,\n",
       "  297.74276377828113,\n",
       "  297.19338681379486,\n",
       "  296.7217506637878],\n",
       " 'val_loss': [318.15060213358146,\n",
       "  308.30926291885646,\n",
       "  304.22893252623965,\n",
       "  302.1134895894682,\n",
       "  300.8775425180225,\n",
       "  299.9937965616662,\n",
       "  299.30517368746393,\n",
       "  298.73325744039545,\n",
       "  298.2454878341621,\n",
       "  297.8241368286815],\n",
       " 'testP4K_loss': [723.0185877851295,\n",
       "  698.1272752300885,\n",
       "  685.258197891926,\n",
       "  679.2784368682783,\n",
       "  676.55788616959,\n",
       "  674.8558076801512,\n",
       "  673.4969380158018,\n",
       "  672.4355979235329,\n",
       "  671.423601997825,\n",
       "  670.7109736936912]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 7449s 1s/step - loss: 332.8352 - val_loss: 318.1170\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 5519s 958ms/step - loss: 311.1496 - val_loss: 308.7574\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5822s 1s/step - loss: 305.3264 - val_loss: 304.9336\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6582s 1s/step - loss: 302.6262 - val_loss: 302.9958\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7103s 1s/step - loss: 301.1792 - val_loss: 301.9337\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 14946s 3s/step - loss: 300.2849 - val_loss: 301.1922\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 4869s 846ms/step - loss: 299.6383 - val_loss: 300.6844\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5781s 1s/step - loss: 299.1321 - val_loss: 300.2569\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6748s 1s/step - loss: 298.7180 - val_loss: 299.8978\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 4745s 824ms/step - loss: 298.3688 - val_loss: 299.6018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232cfd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [332.83523499690364,\n",
       "  311.1495506735721,\n",
       "  305.32637336838263,\n",
       "  302.6261628686316,\n",
       "  301.1792370861946,\n",
       "  300.28492045017515,\n",
       "  299.6382695135298,\n",
       "  299.1321185072753,\n",
       "  298.7179960372403,\n",
       "  298.3687616935474],\n",
       " 'val_loss': [318.11703873779254,\n",
       "  308.75738331521546,\n",
       "  304.9335873032069,\n",
       "  302.9957635315034,\n",
       "  301.9337132302216,\n",
       "  301.1921791930977,\n",
       "  300.68438835990605,\n",
       "  300.2568701773645,\n",
       "  299.89779036501074,\n",
       "  299.60179900476817],\n",
       " 'testP4K_loss': [732.1974313007879,\n",
       "  706.4723622909272,\n",
       "  691.7300854846619,\n",
       "  684.7900059335971,\n",
       "  681.8280336922402,\n",
       "  679.9574934180021,\n",
       "  678.7282812103871,\n",
       "  677.7188479585591,\n",
       "  676.8431889352383,\n",
       "  676.1222872974321]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsDELQ NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_09_NN7L_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2906s 505ms/step - loss: 202.5537 - val_loss: 189.6939\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2024s 352ms/step - loss: 185.1673 - val_loss: 182.6220\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 1676s 291ms/step - loss: 180.0439 - val_loss: 180.6006\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 1666s 289ms/step - loss: 177.2701 - val_loss: 176.7251\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1665s 289ms/step - loss: 175.5439 - val_loss: 175.3660\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1634s 284ms/step - loss: 174.3043 - val_loss: 174.8005\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 1668s 290ms/step - loss: 173.3022 - val_loss: 173.3208\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 1678s 291ms/step - loss: 172.4542 - val_loss: 172.6152\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 2503s 435ms/step - loss: 171.7443 - val_loss: 172.0109\n",
      "Epoch 10/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 171.1936"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_TfromNS_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 6478s 1s/step - loss: 334.0426 - val_loss: 318.8078\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 7037s 1s/step - loss: 310.9031 - val_loss: 307.7163\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5910s 1s/step - loss: 304.0670 - val_loss: 303.4669\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7744s 1s/step - loss: 301.0845 - val_loss: 301.4636\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7767s 1s/step - loss: 299.5355 - val_loss: 300.3262\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7525s 1s/step - loss: 298.5930 - val_loss: 299.6363\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7967s 1s/step - loss: 297.9281 - val_loss: 299.1007\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 6677s 1s/step - loss: 297.4247 - val_loss: 298.6999\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6912s 1s/step - loss: 297.0260 - val_loss: 298.3802\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8484s 1s/step - loss: 296.6993 - val_loss: 298.1298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232ceb8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [334.04261498667336,\n",
       "  310.90313761506275,\n",
       "  304.06700017431956,\n",
       "  301.08452700392866,\n",
       "  299.5355056150648,\n",
       "  298.59301007193477,\n",
       "  297.92813114855807,\n",
       "  297.42471469800324,\n",
       "  297.02595976008,\n",
       "  296.69932817398495],\n",
       " 'val_loss': [318.8077631269736,\n",
       "  307.7162972562862,\n",
       "  303.4669281734807,\n",
       "  301.4636359146121,\n",
       "  300.3261635153653,\n",
       "  299.6362964928039,\n",
       "  299.10066872735194,\n",
       "  298.6998974468755,\n",
       "  298.3802061189176,\n",
       "  298.1298349087428],\n",
       " 'testP4K_loss': [717.7692014099695,\n",
       "  692.5093093676612,\n",
       "  681.1892812258153,\n",
       "  677.2928423535407,\n",
       "  675.2491797104337,\n",
       "  673.9023133940009,\n",
       "  672.9248305401367,\n",
       "  672.2034651915096,\n",
       "  671.5856063464887,\n",
       "  671.1150249686433]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+BCONS+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8133s 1s/step - loss: 330.7609 - val_loss: 316.7712\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6660s 1s/step - loss: 310.4906 - val_loss: 307.8904\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 7116s 1s/step - loss: 304.7100 - val_loss: 303.9924\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7062s 1s/step - loss: 301.7960 - val_loss: 301.8254\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 8137s 1s/step - loss: 300.0964 - val_loss: 300.5073\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7596s 1s/step - loss: 298.9904 - val_loss: 299.6046\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7545s 1s/step - loss: 298.1876 - val_loss: 298.9532\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 7470s 1s/step - loss: 297.5743 - val_loss: 298.4452\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7456s 1s/step - loss: 297.0949 - val_loss: 298.0184\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 7469s 1s/step - loss: 296.7098 - val_loss: 297.6911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad92882ba8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [330.76085305019353,\n",
       "  310.4906432967692,\n",
       "  304.7099609745938,\n",
       "  301.7960417360497,\n",
       "  300.09642175858914,\n",
       "  298.9904117309337,\n",
       "  298.18756228044725,\n",
       "  297.57428517435176,\n",
       "  297.0949075244785,\n",
       "  296.709771785746],\n",
       " 'val_loss': [316.77116454788614,\n",
       "  307.8904337055013,\n",
       "  303.99240362525046,\n",
       "  301.8254467632818,\n",
       "  300.5073347083597,\n",
       "  299.6046408204614,\n",
       "  298.95319436894175,\n",
       "  298.4452129081611,\n",
       "  298.01840061586466,\n",
       "  297.69113439902117],\n",
       " 'testP4K_loss': [702.3246120228104,\n",
       "  686.3533080012243,\n",
       "  680.0862056377094,\n",
       "  676.7398991455286,\n",
       "  674.6481522557676,\n",
       "  673.1816021796275,\n",
       "  672.0055764137222,\n",
       "  671.111931773641,\n",
       "  670.3688568089174,\n",
       "  669.7110506886809]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_01_MLR_RH_NSto220_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 11407s 2s/step - loss: 333.0526 - val_loss: 317.6788\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10810s 2s/step - loss: 310.2102 - val_loss: 307.3595\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 10216s 2s/step - loss: 303.6873 - val_loss: 302.9961\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10683s 2s/step - loss: 300.5542 - val_loss: 300.6824\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10550s 2s/step - loss: 298.8112 - val_loss: 299.3256\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10292s 2s/step - loss: 297.7034 - val_loss: 298.4254\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10560s 2s/step - loss: 296.9044 - val_loss: 297.7670\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 11866s 2s/step - loss: 296.3028 - val_loss: 297.2482\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10787s 2s/step - loss: 295.8395 - val_loss: 296.8615\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 11359s 2s/step - loss: 295.4770 - val_loss: 296.5718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96803fe2e8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05256303355685,\n",
       "  310.2102426647167,\n",
       "  303.6872879666863,\n",
       "  300.55415829901773,\n",
       "  298.81120031363105,\n",
       "  297.7034077711415,\n",
       "  296.90441165903536,\n",
       "  296.30281556532356,\n",
       "  295.83954845970965,\n",
       "  295.4770164907051],\n",
       " 'val_loss': [317.6788116925534,\n",
       "  307.3594649500833,\n",
       "  302.9960944944573,\n",
       "  300.68239808442047,\n",
       "  299.32556045328016,\n",
       "  298.42535546180176,\n",
       "  297.7669507749966,\n",
       "  297.24817319492274,\n",
       "  296.86145162218895,\n",
       "  296.5717960327293],\n",
       " 'testP4K_loss': [721.7310292205681,\n",
       "  696.0703459624854,\n",
       "  682.7236189375567,\n",
       "  676.256974551415,\n",
       "  672.9861661186186,\n",
       "  670.9493864272823,\n",
       "  669.4111307171855,\n",
       "  668.1374724690269,\n",
       "  667.1579007533959,\n",
       "  666.4447404165944]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_03_MLR_RH_NSto220_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 9714s 2s/step - loss: 333.0516 - val_loss: 317.9425\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6439s 1s/step - loss: 310.6932 - val_loss: 308.0465\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5183s 900ms/step - loss: 304.5171 - val_loss: 304.0613\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6201s 1s/step - loss: 301.7091 - val_loss: 302.0904\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 6448s 1s/step - loss: 300.2337 - val_loss: 301.0068\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 6364s 1s/step - loss: 299.3400 - val_loss: 300.3276\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 5246s 911ms/step - loss: 298.7033 - val_loss: 299.8050\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5084s 883ms/step - loss: 298.2143 - val_loss: 299.3951\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7377s 1s/step - loss: 297.8223 - val_loss: 299.0667\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 6087s 1s/step - loss: 297.4986 - val_loss: 298.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96800d5a20>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05156581512364,\n",
       "  310.6931948111193,\n",
       "  304.51710277887906,\n",
       "  301.70909371684877,\n",
       "  300.2336531922436,\n",
       "  299.34000020094226,\n",
       "  298.7033277413232,\n",
       "  298.2142985748487,\n",
       "  297.8223111576081,\n",
       "  297.49855535381016],\n",
       " 'val_loss': [317.9424894715131,\n",
       "  308.0464616504419,\n",
       "  304.0613429893301,\n",
       "  302.090381744452,\n",
       "  301.0067826105784,\n",
       "  300.32760883262154,\n",
       "  299.8049709262554,\n",
       "  299.39514361780095,\n",
       "  299.06670788960923,\n",
       "  298.80823058409953],\n",
       " 'testP4K_loss': [722.2397246819291,\n",
       "  697.4373292892026,\n",
       "  684.9307260671301,\n",
       "  679.3653844453859,\n",
       "  676.8490013480899,\n",
       "  675.4289473119096,\n",
       "  674.3540836741433,\n",
       "  673.5583135512015,\n",
       "  672.867135846849,\n",
       "  672.3943842652513]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "579.521px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
