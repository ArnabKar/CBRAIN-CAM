{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to notebook [088] but using a single generator and compact syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "from cbrain.climate_invariant import *\n",
    "from cbrain.climate_invariant_utils import *\n",
    "import pickle\n",
    "#import h5netcdf\n",
    "import time\n",
    "\n",
    "from cbrain.imports import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.normalization import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path_array = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_str = ['cold','hot','both']\n",
    "set_str = ['train','valid','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_array['cold'] = [path_data+'2021_03_18_O3_TRAIN_M4K_shuffle.nc',\n",
    "                      path_data+'2021_03_18_O3_VALID_M4K.nc',\n",
    "                      path_data+'2021_03_18_O3_TEST_M4K.nc']\n",
    "path_array['hot'] = [path_data+'2021_03_18_O3_TRAIN_P4K_shuffle.nc',\n",
    "                     path_data+'2021_03_18_O3_VALID_P4K.nc',\n",
    "                     path_data+'2021_03_18_O3_TEST_P4K.nc']\n",
    "path_array['both'] = [path_data+'2022_04_18_TRAIN_M4K_P4K_shuffle.nc',\n",
    "                      path_data+'2022_04_18_VALID_M4K_P4K.nc',\n",
    "                      path_data+'2022_04_18_TEST_M4K_P4K.nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_norm = path_data + '2021_01_24_NORM_O3_small.nc'\n",
    "path_norm_RH = path_data + '2021_02_01_NORM_O3_RH_small.nc'\n",
    "path_norm_BMSE = path_data + '2021_06_16_NORM_BMSE_small.nc'\n",
    "path_norm_LHF_nsDELQ = path_data + '2021_02_01_NORM_O3_LHF_nsDELQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS','SOLIN','SHFLX','LHFLX'] # We take the large-scale climate state as inputs\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS'] # and we output the response of clouds/storms to these climate conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = pickle.load(open(path_data+'009_Wm2_scaling.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, define generators necessary for rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen_rescaling(input_rescaling):\n",
    "    return DataGeneratorCI(\n",
    "        data_fn = path_array['cold'][0],\n",
    "        input_vars = input_rescaling,\n",
    "        output_vars = out_vars,\n",
    "        norm_fn = path_input_norm,\n",
    "        input_transform = ('mean', 'maxrs'),\n",
    "        output_transform = scale_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = train_gen_rescaling(in_vars)\n",
    "train_gen_BMSE = train_gen_rescaling(in_vars)\n",
    "train_gen_LHF_nsDELQ = train_gen_rescaling(in_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, generate structure containing all data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator_singleDS(path,rescaling=None):\n",
    "    \n",
    "    in_vars = ['QBP','TBP','PS','SOLIN','SHFLX','LHFLX'] # We take the large-scale climate state as inputs\n",
    "    out_vars = ['PHQ','TPHYSTND','QRL','QRS'] # and we output the response of clouds/storms to these climate conditions\n",
    "    path_input_norm = path_data + '2021_01_24_NORM_O3_small.nc'\n",
    "    scale_dict = pickle.load(open(path_data+'009_Wm2_scaling.pkl','rb'))\n",
    "    \n",
    "    if rescaling=='CI':\n",
    "        gen = DataGeneratorCI(\n",
    "        data_fn = path,\n",
    "        input_vars = in_vars,\n",
    "        output_vars = out_vars,\n",
    "        norm_fn = path_input_norm,\n",
    "        batch_size=8192,\n",
    "        input_transform = ('mean', 'maxrs'),\n",
    "        output_transform = scale_dict,\n",
    "        Qscaling = 'RH',\n",
    "        Tscaling = 'BMSE',\n",
    "        LHFscaling = 'LHF_nsDELQ',\n",
    "        hyam=hyam, hybm=hybm, # Arrays to define mid-levels of hybrid vertical coordinate\n",
    "        inp_sub_Qscaling=train_gen_RH.input_transform.sub, # What to subtract from RH inputs\n",
    "        inp_div_Qscaling=train_gen_RH.input_transform.div, # What to divide RH inputs by\n",
    "        inp_sub_Tscaling=train_gen_BMSE.input_transform.sub,\n",
    "        inp_div_Tscaling=train_gen_BMSE.input_transform.div,\n",
    "        inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "        inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div\n",
    "        ) \n",
    "    else:\n",
    "        gen = DataGeneratorCI(\n",
    "        data_fn = path,\n",
    "        input_vars = in_vars,\n",
    "        output_vars = out_vars,\n",
    "        norm_fn = path_input_norm,\n",
    "        batch_size=8192,\n",
    "        input_transform = ('mean', 'maxrs'),\n",
    "        output_transform = scale_dict\n",
    "        )\n",
    "\n",
    "    return gen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFgen = {}\n",
    "CIgen = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate =  cold\n",
      "Set =  train\n",
      "Set =  valid\n",
      "Set =  test\n",
      "Climate =  hot\n",
      "Set =  train\n",
      "Set =  valid\n",
      "Set =  test\n",
      "Climate =  both\n",
      "Set =  train\n",
      "Set =  valid\n",
      "Set =  test\n"
     ]
    }
   ],
   "source": [
    "for iclimate,clim in enumerate(climate_str):\n",
    "    print('Climate = ',clim)\n",
    "    BFgen[clim] = {}\n",
    "    CIgen[clim] = {}\n",
    "    \n",
    "    for iset,st in enumerate(set_str):\n",
    "        print('Set = ',st)\n",
    "        \n",
    "        BFgen[clim][st] = Generator_singleDS(path_array[clim][iset])\n",
    "        CIgen[clim][st] = Generator_singleDS(path_array[clim][iset],rescaling='CI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layer_max = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(inp,N_layer):\n",
    "    if N_layer>0:\n",
    "        densout = Dense(128, activation='linear')(inp)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    else: dense_out = Dense(120, activation='linear')(inp)\n",
    "    for i in range (N_layer-1):\n",
    "        densout = Dense(128, activation='linear')(densout)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    if N_layer>0: dense_out = Dense(120, activation='linear')(densout)\n",
    "    return tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_models = {}\n",
    "CI_models = {}\n",
    "inp_BF = {}\n",
    "inp_CI = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for ilayer in range(N_layer_max):\n",
    "    \n",
    "    print(ilayer)\n",
    "    \n",
    "    inp_BF[ilayer] = Input(shape=(64,))\n",
    "    BF_models[ilayer] = NN_model(inp_BF[ilayer],ilayer)\n",
    "    \n",
    "    inp_CI[ilayer] = Input(shape=(64,))\n",
    "    CI_models[ilayer] = NN_model(inp_CI[ilayer],ilayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for ilayer in range(N_layer_max):\n",
    "    \n",
    "    print(ilayer)\n",
    "    \n",
    "    BF_models[ilayer].compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "    CI_models[ilayer].compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            valid_generator,valid_name = validation_set\n",
    "            results = self.model.evaluate_generator(generator=valid_generator)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,[results]):\n",
    "                valuename = valid_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on both datasets at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_BF = {}\n",
    "history_CI = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers =  0\n",
      "Epoch 1/20\n",
      " 3850/11518 [=========>....................] - ETA: 14:38 - loss: 555.3992"
     ]
    }
   ],
   "source": [
    "for ilayer in range(N_layer_max):\n",
    "    \n",
    "    print('Number of layers = ',ilayer)\n",
    "    \n",
    "    path_BF = path_data + '2022_04_18_BF_'+str(ilayer)+'.hdf5'\n",
    "    mcp_save = ModelCheckpoint(path_BF,save_best_only=True, monitor='val_loss', mode='min')\n",
    "    history_BF[ilayer] = BF_models[ilayer].fit_generator(BFgen['both']['train'],epochs=Nep, \n",
    "                                                         validation_data = BFgen['both']['valid'],\n",
    "                                                         callbacks=[mcp_save,earlyStopping])\n",
    "    \n",
    "    path_CI = path_data + '2022_04_18_CI_'+str(ilayer)+'.hdf5'\n",
    "    mcp_save = ModelCheckpoint(path_CI,save_best_only=True, monitor='val_loss', mode='min')\n",
    "    history_CI[ilayer] = CI_models[ilayer].fit_generator(CIgen['both']['train'],epochs=Nep, \n",
    "                                                         validation_data = CIgen['both']['valid'],\n",
    "                                                         callbacks=[mcp_save,earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = path_data + 'PKL_DATA/2022_04_18_Train_Both_Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'history_BF':history_BF,'history_CI':history_CI},\n",
    "            open(path_history,'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
