{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/18/2019 \n",
    "- We will train unconstrained (U), loss-constrained (L) and architecture-constrained (A) neural networks trained on the 8-column +0K experiment and validated on the same experiment. The loss-constrained networks will have varying architectures to see how MSE and energy conservation performances vary with the importance given to each in the loss function.  \n",
    "  \n",
    "Notebook 009 follows the notebook 005 that predicts:\n",
    "***\n",
    "[PHQ, PHCLDLIQ, PHCLDICE, TPHYSTND, QRL, QRS, DTVKE, FSNT, FSNS, FLNT, FLNS, PRECT, PRECTEND, PRECST, PRECSTEN] as a function of:  \n",
    "[QBP, QCBP, QIBP, TBP, VBP, Qdt_adiabatic, QCdt_adiabatic, QIdt_adiabatic, Tdt_adiabatic, Vdt_adiabatic, PS, SOLIN, SHFLX, LHFLX] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load modules and create training/validation data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics.model_diagnostics import ModelDiagnostics\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "limit_mem()\n",
    "TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/'\n",
    "PREFIX = '8col009_01_'\n",
    "%cd /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/18/2019 - Used preprocessed data calculated in notebook 009 @ https://github.com/tbeucler/CBRAIN-CAM/blob/master/notebooks/tbeucler_devlog/009_Generalization_Climate_Change_8col.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = load_pickle('./nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "in_vars = load_pickle('./nn_config/scale_dicts/009_Wm2_in_vars.pkl')\n",
    "out_vars = load_pickle('./nn_config/scale_dicts/009_Wm2_out_vars.pkl')\n",
    "dP = load_pickle('./nn_config/scale_dicts/009_Wm2_dP.pkl')\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'train_shuffle.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'valid.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Build neural networks in a loop\n",
    "tgb - 4/18/2019 - https://stackoverflow.com/questions/52320059/creating-a-new-sequential-model-inside-a-for-loop-using-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  0  and NN is  {}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 304)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               156160    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 218)               111834    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 218)               0         \n",
      "=================================================================\n",
      "Total params: 1,318,618\n",
      "Trainable params: 1,318,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "NN is  None\n",
      "41376/41376 [==============================] - 286s 7ms/step - loss: 190.6616 - mean_squared_error: 190.6617\n",
      "41376/41376 [==============================] - 696s 17ms/step - loss: 207.5845 - mean_squared_error: 207.5837 - val_loss: 190.6616 - val_mean_squared_error: 190.6617\n",
      "NN saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/NNL0.h5\n",
      "alpha =  0.01  and NN is  {}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 304)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               156160    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 218)               111834    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 218)               0         \n",
      "=================================================================\n",
      "Total params: 1,318,618\n",
      "Trainable params: 1,318,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "NN is  None\n",
      " 5892/41376 [===>..........................] - ETA: 4:07 - loss: 189.2093 - mean_squared_error: 185.1844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1117bc32edc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 4) Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# 5) Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m           mode='test')\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1254\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_eval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha_array = [0,0.01,0.25,0.5,0.75,0.99,1] # Loop over weight given to MSE and conservation constraints\n",
    "Nep = 1\n",
    "\n",
    "for alpha in alpha_array:\n",
    "    NN = {}\n",
    "    print('alpha = ',str(alpha),' and NN is ',NN)\n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph):\n",
    "        \n",
    "        # 1) Create model\n",
    "        # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "        inpU = Input(shape=(304,))\n",
    "        densout = Dense(512, activation='linear')(inpU)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "        for i in range (4):\n",
    "            densout = Dense(512, activation='linear')(densout)\n",
    "            densout = LeakyReLU(alpha=0.3)(densout)\n",
    "        densout = Dense(218, activation='linear')(densout)\n",
    "        out_layer = LeakyReLU(alpha=0.3)(densout)\n",
    "        NN = tf.keras.models.Model(inpU, out_layer)\n",
    "        print('NN is ',NN.summary())\n",
    "        \n",
    "        # 2) Define loss\n",
    "        al = alpha/4 # Weight given to each residual\n",
    "        Loss = WeakLoss(inpU, inp_div=train_gen.input_transform.div,\n",
    "                            inp_sub=train_gen.input_transform.sub,\n",
    "                            norm_q=scale_dict['PHQ'],\n",
    "                            hyai=hyai, hybi=hybi, name='loss',\n",
    "                            alpha_mass=al, alpha_ent=al,\n",
    "                            alpha_lw=al, alpha_sw=al)\n",
    "        \n",
    "        # 3) Compile model\n",
    "        NN.compile(tf.keras.optimizers.RMSprop(), loss=Loss, metrics=[mse])\n",
    "        \n",
    "        # 4) Train model\n",
    "        NN.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)\n",
    "        \n",
    "        # 5) Save model\n",
    "        path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'.h5'\n",
    "        NN.save(path)\n",
    "        print('NN saved in ',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for architecture-constrained network\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph):\n",
    "\n",
    "    # 1) Create model\n",
    "    # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "    inpC = Input(shape=(304,))\n",
    "    densout = Dense(512, activation='linear')(inpC)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    for i in range (4):\n",
    "        densout = Dense(512, activation='linear')(densout)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    densout = Dense(214, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    surfout = SurRadLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, densout])\n",
    "    massout = MassConsLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, surfout])\n",
    "    enthout = EntConsLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, massout])\n",
    "    NNA = tf.keras.models.Model(inpC, enthout)\n",
    "    print(NNA.summary())\n",
    "\n",
    "    # 2) Compile model\n",
    "    NNA.compile(tf.keras.optimizers.RMSprop(), loss=mse, metrics=[mse])\n",
    "\n",
    "    # 3) Train model\n",
    "    NNA.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)\n",
    "\n",
    "    # 4) Save model\n",
    "    path = TRAINDIR+'HDF5_DATA/NNA.h5'\n",
    "    NNA.save(path)\n",
    "    print('NN saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Calculate statistics and residuals in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_fn = '/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\n",
    "data_fn = '/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\n",
    "dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}\n",
    "\n",
    "for alpha in alpha_array:\n",
    "    \n",
    "    # 1) Load model\n",
    "    path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'.h5'\n",
    "    NN = load_model(path,custom_objects=dict_lay)\n",
    "    \n",
    "    # 2) Define model diagnostics object\n",
    "    md = ModelDiagnostics(C_009,config_fn,data_fn)\n",
    "    \n",
    "    # 3) Calculate statistics and save in pickle file\n",
    "    md.compute_stats()\n",
    "    path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'md.pkl'\n",
    "    pickle.dump(md.stats,open(path,'wb'))\n",
    "    print('Stats are saved in ',path)\n",
    "    \n",
    "    # 4) Calculate budget residuals and save in pickle file\n",
    "    md.compute_res()\n",
    "    path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'res.pkl'\n",
    "    pickle.dump(md.res,open(path,'wb'))\n",
    "    print('Budget residuals are saved in ',path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
