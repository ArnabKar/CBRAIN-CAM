{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n",
      "/nfspool-0/home/tbeucler/CBRAIN-CAM\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "\n",
    "# [https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed] If using more than 1 GPU at at time\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "import xarray as xr\n",
    "\n",
    "#TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/'\n",
    "PREFIX = '8col009_01_'\n",
    "#%cd /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM\n",
    "%cd /export/home/tbeucler/CBRAIN-CAM\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "#limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom generator (all outputs minus the residual ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom generator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking as argument the *output* indices it will not be trained on **out_cut_off** (var_cut_off refers to the *input* indices it is not trained on). **out_cut_off** will be formatted as a dictionary with int entries corresponding to the single index to exclude from the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_var_idxs_outputcutoff(ds, var_list, out_cut_off=None):\n",
    "    \"\"\"\n",
    "    To be used on stacked variable dimension. Returns indices array\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray dataset\n",
    "    var_list: list of variables\n",
    "    Returns\n",
    "    -------\n",
    "    var_idxs: indices array\n",
    "    \"\"\"\n",
    "    if out_cut_off is None:\n",
    "        var_idxs = np.concatenate([np.where(ds.var_names == v)[0] for v in var_list])\n",
    "    else:\n",
    "        idxs_list = []\n",
    "        for v in var_list:\n",
    "            i = np.where(ds.var_names == v)[0]\n",
    "            if v in out_cut_off.keys():\n",
    "                i = np.delete(i,out_cut_off[v])\n",
    "            idxs_list.append(i)\n",
    "        var_idxs = np.concatenate(idxs_list)\n",
    "    return var_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictNormalizer_outputcutoff(object):\n",
    "    \"\"\"Normalizer that takes a conversion dictionary as input. Simply scales by factors in dict.\"\"\"\n",
    "    def __init__(self, norm_ds, var_list, dic=None,out_cut_off=None):\n",
    "        if dic is None: dic = conversion_dict\n",
    "        var_idxs = return_var_idxs_outputcutoff(norm_ds, var_list, out_cut_off=out_cut_off)\n",
    "        var_names = norm_ds.var_names[var_idxs].copy()\n",
    "        scale = []\n",
    "        for v in var_list:\n",
    "            s = np.atleast_1d(dic[v])\n",
    "            # Modification below: Delete scaling factor for outputs\n",
    "            # that have been cut off via out_cut_off \n",
    "            if v in out_cut_off.keys(): s = np.delete(s,out_cut_off[v])\n",
    "            scale.append(s)\n",
    "        self.scale = np.concatenate(scale).astype('float32')\n",
    "        self.transform_arrays = {\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_outputcutoff(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    Data generator class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=1024, shuffle=True, xarray=False, var_cut_off=None,\n",
    "                out_cut_off=None):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn, self.norm_fn = data_fn, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_dataset(data_fn)\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "\n",
    "        # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.vars.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        # Get input and output variable indices\n",
    "        self.input_idxs = return_var_idxs(self.data_ds, input_vars, var_cut_off)\n",
    "        self.output_idxs = return_var_idxs_outputcutoff(self.data_ds, output_vars, out_cut_off=out_cut_off)\n",
    "        self.n_inputs, self.n_outputs = len(self.input_idxs), len(self.output_idxs)\n",
    "\n",
    "        # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer_outputcutoff(self.norm_ds, output_vars, output_transform,\n",
    "                                                                out_cut_off=out_cut_off)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "        if not xarray:\n",
    "            self.data_ds.close()\n",
    "            self.data_ds = h5py.File(data_fn, 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom generator and compare to standard generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the manuscript's purposes, we will choose the lowest levels as the residuals for direct comparison with the reference ACnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILEQ = '8col009_01_train.nc'\n",
    "VALIDFILEQ = '8col009_01_valid.nc'\n",
    "NORMFILEQ = '8col009_01_norm.nc'\n",
    "TESTFILEQ = '8col009_01_test.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dictQ = load_pickle('./nn_config/scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_varsQ = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_varsQ = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FSNS', 'FLNT', 'FLNS', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=8192,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_custom = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_custom = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FLNT', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 29\n",
    "out_cut_off_low['TPHYSTND'] = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PHQ': 29, 'TPHYSTND': 29}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cut_off_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+TRAINFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+VALIDFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+TESTFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where notebook 061 starts separating from [https://github.com/tbeucler/CBRAIN-CAM/blob/master/notebooks/tbeucler_devlog/060_Constraints_Post_Processing_vs_ACNet.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostics: Load instead of Train Neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "\n",
    "#NNarray = ['UCnet_214_1.hdf5','JNNC.h5']\n",
    "#FlagINC = [1,0] # Flags if the NN is missing outputs that must be calculate post-processing\n",
    "#NNname = ['UCnet_{214}','ACnet'] # TODO: Add UCnet_NL\n",
    "\n",
    "NNarray = ['UCnet_214_q18T28.hdf5','UCnet_214_q3T26.hdf5',\n",
    "           'UCnet_214_q4T4.hdf5','UCnet_214_q5T5.hdf5',\n",
    "           'UCnet_214_q8T4.hdf5']\n",
    "FlagINC = [1,1,1,1,1]\n",
    "NNname = ['UCnet_{q18T28}','UCnet_{q3T26}','UCnet_{q4T4}','UCnet_{q5T5}','UCnet_{q8T4}']\n",
    "Mcutoff = [18,3,4,5,8]\n",
    "Ecutoff = [28,26,4,5,4]\n",
    "\n",
    "dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN name is  UCnet_214_q18T28.hdf5\n",
      "NN name is  UCnet_214_q3T26.hdf5\n",
      "NN name is  UCnet_214_q4T4.hdf5\n",
      "NN name is  UCnet_214_q5T5.hdf5\n",
      "NN name is  UCnet_214_q8T4.hdf5\n"
     ]
    }
   ],
   "source": [
    "NN = {}; md = {};\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs)\n",
    "    path = path_HDF5+NNs\n",
    "    NN[NNs] = load_model(path,custom_objects=dict_lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = test_gen_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 5\n",
    "inp = gen[sample][0]\n",
    "truth = gen[sample][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4663, shape=(1024, 214), dtype=float32, numpy=\n",
       "array([[-8.6733829e-03, -3.2105796e-02,  6.1838023e-02, ...,\n",
       "        -7.1555565e+01, -2.6726155e+00, -1.8707504e+01],\n",
       "       [ 7.8717116e-03, -8.9696702e-04,  6.5471850e-02, ...,\n",
       "        -3.7608677e+01, -1.3191630e+00, -3.3747487e+00],\n",
       "       [ 2.4816066e-02,  7.1093719e-04,  1.4633943e-01, ...,\n",
       "         1.2512340e+01, -4.3135433e+00, -3.3440247e+00],\n",
       "       ...,\n",
       "       [-9.5730033e-03, -8.7316651e-03, -1.4988841e-01, ...,\n",
       "        -1.4162773e+00, -1.7971500e+00, -2.1660233e+00],\n",
       "       [-1.4475256e-02, -3.4214780e-03, -1.5505002e-01, ...,\n",
       "        -1.9624866e+00, -1.7847141e+00, -2.0972819e+00],\n",
       "       [-1.4197432e-02, -9.6753072e-03, -1.3704923e-01, ...,\n",
       "        -3.3366709e+00, -1.4479843e+00, -3.1109478e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN[NNs].predict_on_batch(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical constraints layer in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dP_tilde(PS, PS_div, PS_sub, norm_q, hyai, hybi):\n",
    "    \"\"\"\n",
    "    Computes dP_tilde in Tom's notation.\n",
    "    PS is the normalized pressure as it is used in the input.\n",
    "    PS_mult and PS_add are the corresponding values to unnormalize PS.\n",
    "    WARNING: Really not sure about norm_q\n",
    "    \"\"\"\n",
    "    PS = PS * PS_div + PS_sub\n",
    "    P = P0 * hyai + PS[:, None] * hybi\n",
    "    dP = P[:, 1:] - P[:, :-1]\n",
    "    dP_norm = norm_q * G / L_V   # Why L_V?\n",
    "    dP_tilde = dP / dP_norm\n",
    "    return dP_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SurRadLayer_np(inp,prior,inp_sub,inp_div,norm_q):\n",
    "    # Define variable indices here\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "    # Output\n",
    "    QRL_idx = slice(118, 148)  # Odd numbers because residuals\n",
    "    QRS_idx = slice(148, 178)  # for Q and T are still missing\n",
    "    FSNT_idx = 208\n",
    "    FLNT_idx = 209\n",
    "    \n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(\n",
    "        inp[:, PS_idx],\n",
    "        inp_div[PS_idx], inp_sub[PS_idx],\n",
    "        norm_q, hyai, hybi\n",
    "    )\n",
    "\n",
    "    # 2. Compute radiative integrals\n",
    "    SWINT = np.sum(dP_tilde * prior[:, QRS_idx], axis=1)\n",
    "    LWINT = np.sum(dP_tilde * prior[:, QRL_idx], axis=1)\n",
    "\n",
    "    # 3. Infer surface fluxes from residual\n",
    "    FSNS = prior[:, FSNT_idx] - SWINT\n",
    "    FLNS = prior[:, FLNT_idx] + LWINT\n",
    "\n",
    "    # 4. Concatenate output vector\n",
    "    post = np.concatenate((\n",
    "        prior[:, :FLNT_idx], FSNS[:, None],\n",
    "        prior[:, FLNT_idx][:, None], FLNS[:, None],\n",
    "        prior[:, (FLNT_idx + 1):]\n",
    "    ), axis=1)\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MassConsLayer_choice_np(inp,prior,lvl_choice,inp_sub,inp_div,norm_q):\n",
    "    # Define variable indices here\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "    LHFLX_idx = 303\n",
    "    # Output\n",
    "    PHQbef_idx = slice(0, lvl_choice) # Indices before the residual\n",
    "    PHCLDLIQ_idx = slice(29, 59)\n",
    "    PHCLDICE_idx = slice(59, 89)\n",
    "    PRECT_idx = 212\n",
    "    PRECTEND_idx = 213\n",
    "    \n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(\n",
    "        inp[:, PS_idx],\n",
    "        inp_div[PS_idx], inp_sub[PS_idx],\n",
    "        norm_q, hyai, hybi\n",
    "    )\n",
    "\n",
    "    # 2. Compute vertical cloud water integral\n",
    "    CLDINT = np.sum(dP_tilde *\n",
    "                   (prior[:, PHCLDLIQ_idx] + prior[:, PHCLDICE_idx]),\n",
    "                   axis=1)\n",
    "\n",
    "    # 3. Compute water vapor integral minus the water vapor residual\n",
    "    # Careful with handling the pressure vector since it is not aligned\n",
    "    # with the prior water vapor vector\n",
    "    VAPINT = np.sum(dP_tilde[:, PHQbef_idx] * prior[:, PHQbef_idx], 1) +\\\n",
    "    np.sum(dP_tilde[:, lvl_choice+1:30] * prior[:, lvl_choice:29], 1)\n",
    "\n",
    "    # 4. Compute forcing (see Tom's note for details, I am just copying)\n",
    "    LHFLX = (inp[:, LHFLX_idx] * inp_div[LHFLX_idx] +\n",
    "             inp_sub[LHFLX_idx])\n",
    "    PREC = prior[:, PRECT_idx] + prior[:, PRECTEND_idx]\n",
    "\n",
    "    # 5. Compute water vapor tendency at level lvl_choice as residual\n",
    "    PHQ_LVL = (LHFLX - PREC - CLDINT - VAPINT) / dP_tilde[:, lvl_choice]\n",
    "\n",
    "    # 6. Concatenate output vector\n",
    "    post = np.concatenate([\n",
    "        prior[:, PHQbef_idx], PHQ_LVL[:, None],\n",
    "        prior[:, lvl_choice:]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EntConsLayer_choice_np(inp,prior,lvl_choice,inp_sub,inp_div,norm_q):\n",
    "    # Define variable indices here\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "    SHFLX_idx = 302\n",
    "    LHFLX_idx = 303\n",
    "\n",
    "    # Output\n",
    "    PHQ_idx = slice(0, 30)\n",
    "    PHCLDLIQ_idx = slice(30, 60)\n",
    "    Tbef_idx = slice(90, 90+lvl_choice)\n",
    "    DTVKE_idx = slice(179, 209)\n",
    "    FSNT_idx = 209\n",
    "    FSNS_idx = 210\n",
    "    FLNT_idx = 211\n",
    "    FLNS_idx = 212\n",
    "    PRECT_idx = 213\n",
    "    PRECTEND_idx = 214\n",
    "    PRECST_idx = 215\n",
    "    PRECSTEND_idx = 216\n",
    "    \n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(\n",
    "        inp[:, PS_idx],\n",
    "        inp_div[PS_idx], inp_sub[PS_idx],\n",
    "        norm_q, hyai, hybi\n",
    "    )\n",
    "\n",
    "    # 2. Compute net energy input from phase change and precipitation\n",
    "    PHAS = L_I / L_V * (\n",
    "            (prior[:, PRECST_idx] + prior[:, PRECSTEND_idx]) -\n",
    "            (prior[:, PRECT_idx] + prior[:, PRECTEND_idx])\n",
    "    )\n",
    "\n",
    "    # 3. Compute net energy input from radiation, SHFLX and TKE\n",
    "    RAD = (prior[:, FSNT_idx] - prior[:, FSNS_idx] -\n",
    "           prior[:, FLNT_idx] + prior[:, FLNS_idx])\n",
    "    SHFLX = (inp[:, SHFLX_idx] * inp_div[SHFLX_idx] +\n",
    "             inp_sub[SHFLX_idx])\n",
    "    KEDINT = np.sum(dP_tilde * prior[:, DTVKE_idx], 1)\n",
    "\n",
    "    # 4. Compute tendency of vapor due to phase change\n",
    "    LHFLX = (inp[:, LHFLX_idx] * inp_div[LHFLX_idx] +\n",
    "             inp_sub[LHFLX_idx])\n",
    "    VAPINT = np.sum(dP_tilde * prior[:, PHQ_idx], 1)\n",
    "    SPDQINT = (VAPINT - LHFLX) * L_S / L_V\n",
    "\n",
    "    # 5. Same for cloud liquid water tendency\n",
    "    SPDQCINT = np.sum(dP_tilde * prior[:, PHCLDLIQ_idx], 1) * L_I / L_V\n",
    "\n",
    "    # 6. And the same for T but remember residual is still missing\n",
    "    DTINT = np.sum(dP_tilde[:, :lvl_choice] *\\\n",
    "                  prior[:, Tbef_idx], 1) +\\\n",
    "    K.sum(dP_tilde[:, lvl_choice+1:30] *\\\n",
    "         prior[:, 90+lvl_choice:119], 1)\n",
    "\n",
    "    # 7. Compute DT30 as residual\n",
    "    DT_LVL = (\n",
    "                   PHAS + RAD + SHFLX + KEDINT - SPDQINT - SPDQCINT - DTINT\n",
    "           ) / dP_tilde[:, lvl_choice]\n",
    "\n",
    "    # 8. Concatenate output vector\n",
    "    post = np.concatenate([\n",
    "        prior[:, :(90+lvl_choice)], DT_LVL[:, None], \\\n",
    "        prior[:, (90+lvl_choice):]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_res_diagno(inp_div,inp_sub,norm_q,inp,pred):\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "    LHFLX_idx = 303\n",
    "\n",
    "    # Output\n",
    "    PHQ_idx = slice(0, 30)\n",
    "    PHCLDLIQ_idx = slice(30, 60)\n",
    "    PHCLDICE_idx = slice(60, 90)\n",
    "    PRECT_idx = 214\n",
    "    PRECTEND_idx = 215\n",
    "\n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(inp[:, PS_idx],  inp_div[PS_idx], inp_sub[PS_idx], norm_q, hyai, hybi)\n",
    "\n",
    "    # 2. Compute water integral\n",
    "    WATINT = np.sum(dP_tilde *(pred[:, PHQ_idx] + pred[:, PHCLDLIQ_idx] + pred[:, PHCLDICE_idx]), axis=1)\n",
    "#     print('PHQ',np.mean(np.sum(dP_tilde*pred[:,PHQ_idx],axis=1)))\n",
    "#     print('PHCLQ',np.mean(np.sum(dP_tilde*pred[:,PHCLDLIQ_idx],axis=1)))\n",
    "#     print('PHICE',np.mean(np.sum(dP_tilde*pred[:,PHCLDICE_idx],axis=1)))\n",
    "\n",
    "    # 3. Compute latent heat flux and precipitation forcings\n",
    "    LHFLX = inp[:, LHFLX_idx] * inp_div[LHFLX_idx] + inp_sub[LHFLX_idx]\n",
    "    PREC = pred[:, PRECT_idx] + pred[:, PRECTEND_idx]\n",
    "\n",
    "    # 4. Compute water mass residual\n",
    "#     print('LHFLX',np.mean(LHFLX))\n",
    "#     print('PREC',np.mean(PREC))\n",
    "#     print('WATINT',np.mean(WATINT))\n",
    "    WATRES = LHFLX - PREC - WATINT\n",
    "    #print('WATRES',np.mean(WATRES))\n",
    "\n",
    "    return np.square(WATRES)\n",
    "\n",
    "def ent_res_diagno(inp_div,inp_sub,norm_q,inp,pred):\n",
    "\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "    SHFLX_idx = 302\n",
    "    LHFLX_idx = 303\n",
    "\n",
    "    # Output\n",
    "    PHQ_idx = slice(0, 30)\n",
    "    PHCLDLIQ_idx = slice(30, 60)\n",
    "    PHCLDICE_idx = slice(60, 90)\n",
    "    TPHYSTND_idx = slice(90, 120)\n",
    "    DTVKE_idx = slice(180, 210)\n",
    "    FSNT_idx = 210\n",
    "    FSNS_idx = 211\n",
    "    FLNT_idx = 212\n",
    "    FLNS_idx = 213\n",
    "    PRECT_idx = 214\n",
    "    PRECTEND_idx = 215\n",
    "    PRECST_idx = 216\n",
    "    PRECSTEND_idx = 217\n",
    "\n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(inp[:, PS_idx],  inp_div[PS_idx], inp_sub[PS_idx], norm_q, hyai, hybi)\n",
    "\n",
    "    # 2. Compute net energy input from phase change and precipitation\n",
    "    PHAS = L_I / L_V * (\n",
    "            (pred[:, PRECST_idx] + pred[:, PRECSTEND_idx]) -\n",
    "            (pred[:, PRECT_idx] + pred[:, PRECTEND_idx])\n",
    "    )\n",
    "\n",
    "    # 3. Compute net energy input from radiation, SHFLX and TKE\n",
    "    RAD = (pred[:, FSNT_idx] - pred[:, FSNS_idx] -\n",
    "           pred[:, FLNT_idx] + pred[:, FLNS_idx])\n",
    "    SHFLX = (inp[:, SHFLX_idx] * inp_div[SHFLX_idx] +\n",
    "             inp_sub[SHFLX_idx])\n",
    "    KEDINT = np.sum(dP_tilde * pred[:, DTVKE_idx], 1)\n",
    "\n",
    "    # 4. Compute tendency of vapor due to phase change\n",
    "    LHFLX = (inp[:, LHFLX_idx] * inp_div[LHFLX_idx] +\n",
    "             inp_sub[LHFLX_idx])\n",
    "    VAPINT = np.sum(dP_tilde * pred[:, PHQ_idx], 1)\n",
    "    SPDQINT = (VAPINT - LHFLX) * L_S / L_V\n",
    "\n",
    "    # 5. Same for cloud liquid water tendency\n",
    "    SPDQCINT = np.sum(dP_tilde * pred[:, PHCLDLIQ_idx], 1) * L_I / L_V\n",
    "\n",
    "    # 6. And the same for T but remember residual is still missing\n",
    "    DTINT = np.sum(dP_tilde * pred[:, TPHYSTND_idx], 1)\n",
    "\n",
    "    # 7. Compute enthalpy residual\n",
    "    ENTRES = SPDQINT + SPDQCINT + DTINT - RAD - SHFLX - PHAS - KEDINT\n",
    "\n",
    "    return np.square(ENTRES)\n",
    "\n",
    "def lw_res_diagno(inp_div,inp_sub,norm_q,inp,pred):\n",
    "\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "\n",
    "    # Output\n",
    "    QRL_idx = slice(120, 150)\n",
    "    FLNS_idx = 213\n",
    "    FLNT_idx = 212\n",
    "\n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(inp[:, PS_idx],  inp_div[PS_idx], inp_sub[PS_idx], norm_q, hyai, hybi)\n",
    "\n",
    "    # 2. Compute longwave integral\n",
    "    LWINT = np.sum(dP_tilde *pred[:, QRL_idx], axis=1)\n",
    "\n",
    "    # 3. Compute net longwave flux from lw fluxes at top and bottom\n",
    "    LWNET = pred[:, FLNS_idx] - pred[:, FLNT_idx]\n",
    "\n",
    "    # 4. Compute water mass residual\n",
    "    LWRES = LWINT-LWNET\n",
    "\n",
    "    return np.square(LWRES)\n",
    "\n",
    "def sw_res_diagno(inp_div,inp_sub,norm_q,inp,pred):\n",
    "\n",
    "    # Input\n",
    "    PS_idx = 300\n",
    "\n",
    "    # Output\n",
    "    QRS_idx = slice(150, 180)\n",
    "    FSNS_idx = 211\n",
    "    FSNT_idx = 210\n",
    "\n",
    "    # 1. Compute dP_tilde\n",
    "    dP_tilde = compute_dP_tilde(inp[:, PS_idx],  inp_div[PS_idx], inp_sub[PS_idx], norm_q, hyai, hybi)\n",
    "\n",
    "    # 2. Compute longwave integral\n",
    "    SWINT = np.sum(dP_tilde *pred[:, QRS_idx], axis=1)\n",
    "\n",
    "    # 3. Compute net longwave flux from lw fluxes at top and bottom\n",
    "    SWNET = pred[:, FSNT_idx] - pred[:, FSNS_idx]\n",
    "\n",
    "    # 4. Compute water mass residual\n",
    "    SWRES = SWINT-SWNET\n",
    "\n",
    "    return np.square(SWRES)\n",
    "\n",
    "def tot_res_diagno(inp_div,inp_sub,norm_q,inp,pred):\n",
    "    return 0.25*(mass_res_diagno(inp_div,inp_sub,norm_q,inp,pred)+\\\n",
    "                ent_res_diagno(inp_div,inp_sub,norm_q,inp,pred)+\\\n",
    "                lw_res_diagno(inp_div,inp_sub,norm_q,inp,pred)+\\\n",
    "                sw_res_diagno(inp_div,inp_sub,norm_q,inp,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Can we close the budgets using UCnet's predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One batch using the custom generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = test_gen_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 5\n",
    "inp = gen[sample][0]\n",
    "truth = gen[sample][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = NN[NNs].predict_on_batch(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.6733829e-03, -3.2105796e-02,  6.1838023e-02, ...,\n",
       "        -7.1555565e+01, -2.6726155e+00, -1.8707504e+01],\n",
       "       [ 7.8717116e-03, -8.9696702e-04,  6.5471850e-02, ...,\n",
       "        -3.7608677e+01, -1.3191630e+00, -3.3747487e+00],\n",
       "       [ 2.4816066e-02,  7.1093719e-04,  1.4633943e-01, ...,\n",
       "         1.2512340e+01, -4.3135433e+00, -3.3440247e+00],\n",
       "       ...,\n",
       "       [-9.5730033e-03, -8.7316651e-03, -1.4988841e-01, ...,\n",
       "        -1.4162773e+00, -1.7971500e+00, -2.1660233e+00],\n",
       "       [-1.4475256e-02, -3.4214780e-03, -1.5505002e-01, ...,\n",
       "        -1.9624866e+00, -1.7847141e+00, -2.0972819e+00],\n",
       "       [-1.4197432e-02, -9.6753072e-03, -1.3704923e-01, ...,\n",
       "        -3.3366709e+00, -1.4479843e+00, -3.1109478e+00]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = (pred.numpy()-truth)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 214)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.43607"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use the q generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = test_genQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 250\n",
    "inpQ = gen[sample][0]\n",
    "truthQ = gen[sample][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 29\n",
    "out_cut_off_low['TPHYSTND'] = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array(NN[NNs].predict_on_batch(inpQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "postrad = SurRadLayer_np(inpQ,prior,gen.input_transform.sub,gen.input_transform.div,gen.output_transform.scale[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmas = MassConsLayer_choice_np(inpQ,postrad,out_cut_off_low['PHQ'],\n",
    "                                  gen.input_transform.sub,gen.input_transform.div,\n",
    "                                  gen.output_transform.scale[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "postent = EntConsLayer_choice_np(inpQ,postmas,out_cut_off_low['TPHYSTND'],\n",
    "                                 gen.input_transform.sub,gen.input_transform.div,\n",
    "                                 gen.output_transform.scale[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_postprocessing(output_preprocessing,inp,inp_sub,inp_div,out_scale_PHQ,lvl_choice_mas,lvl_choice_ent):\n",
    "    \n",
    "    return EntConsLayer_choice_np(inp,\n",
    "                                  MassConsLayer_choice_np(inp,\n",
    "                                                          SurRadLayer_np(inp,\n",
    "                                                                         output_preprocessing,\n",
    "                                                                         inp_sub,\n",
    "                                                                         inp_div,\n",
    "                                                                         out_scale_PHQ),\n",
    "                                                          lvl_choice_mas,\n",
    "                                                          inp_sub,inp_div,out_scale_PHQ),\n",
    "                                  lvl_choice_ent,inp_sub,inp_div,out_scale_PHQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = output_postprocessing(prior,inpQ,gen.input_transform.sub,gen.input_transform.div,\n",
    "                             gen.output_transform.scale[:30],\n",
    "                             out_cut_off_low['PHQ'],\n",
    "                             out_cut_off_low['TPHYSTND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 218)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 218)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = ((post-truthQ)**2).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.65526"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='test': gen = test_genQ\n",
    "elif dataset=='valid': gen = valid_genQ\n",
    "elif dataset=='train': gen = train_genQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRES = {}\n",
    "\n",
    "TRES = {}\n",
    "PSUM = {}\n",
    "TSUM = {}\n",
    "PSQSUM = {}\n",
    "TSQSUM = {}\n",
    "SSE = {}\n",
    "\n",
    "PRED_MEAN = {}\n",
    "TRUE_MEAN = {}\n",
    "TRUE_SQMEAN = {}\n",
    "    \n",
    "MSE_SSE = {}\n",
    "TRUE_VAR = {}\n",
    "R2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spl= 5747                   \r"
     ]
    }
   ],
   "source": [
    "spl = 0\n",
    "while gen[spl][0].size>0: #spl is sample number\n",
    "    \n",
    "    print('spl=',spl,'                  ',end='\\r')\n",
    "    \n",
    "    inp = gen[spl][0]\n",
    "    truth = gen[spl][1]\n",
    "    \n",
    "    for iNNs,NNs in enumerate(NNarray):\n",
    "\n",
    "        prior = np.array(NN[NNs].predict_on_batch(inp))\n",
    "        if FlagINC[iNNs]:\n",
    "            pred = output_postprocessing(prior,inp,gen.input_transform.sub,\n",
    "                                         gen.input_transform.div,\n",
    "                                         gen.output_transform.scale[:30],\n",
    "                                         Mcutoff[iNNs],Ecutoff[iNNs])\n",
    "        else: pred=prior\n",
    "\n",
    "        se = (pred-truth)**2\n",
    "        tresid = tot_res_diagno(gen.input_transform.div,\n",
    "                                gen.input_transform.sub,\n",
    "                                gen.output_transform.scale[:30],\n",
    "                                inp,pred)\n",
    "        \n",
    "        if spl==0: \n",
    "            TRES[NNs] = tresid;\n",
    "            PSUM[NNs] = pred; \n",
    "            TSUM[NNs] = truth;\n",
    "            PSQSUM[NNs] = pred**2; \n",
    "            TSQSUM[NNs] = truth**2;\n",
    "            SSE[NNs] = (truth-pred)**2\n",
    "        else: \n",
    "            TRES[NNs] = np.concatenate((TRES[NNs],tresid),axis=0); \n",
    "            PSUM[NNs] = PSUM[NNs] + pred; \n",
    "            TSUM[NNs] = TSUM[NNs] + truth;\n",
    "            PSQSUM[NNs] = PSQSUM[NNs] + pred**2; \n",
    "            TSQSUM[NNs] = TSQSUM[NNs] + truth**2;\n",
    "            SSE[NNs] = SSE[NNs] + (truth-pred)**2\n",
    "        \n",
    "    spl += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNs= UCnet_214_q18T28.hdf5\n",
      "NNs= UCnet_214_q3T26.hdf5\n",
      "NNs= UCnet_214_q4T4.hdf5\n",
      "NNs= UCnet_214_q5T5.hdf5\n",
      "NNs= UCnet_214_q8T4.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DFS-L/DATA/pritchard/tbeucler/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/DFS-L/DATA/pritchard/tbeucler/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "for iNNs,NNs in enumerate(NNarray):\n",
    "    print('NNs=',NNs)\n",
    "    \n",
    "    PRED_MEAN[NNs] = PSUM[NNs]/spl\n",
    "    TRUE_MEAN[NNs] = TSUM[NNs]/spl\n",
    "    \n",
    "    TRUE_SQMEAN[NNs] = TSQSUM[NNs]/spl\n",
    "    \n",
    "    MSE_SSE[NNs] = SSE[NNs]/spl\n",
    "    TRUE_VAR[NNs] = TRUE_SQMEAN[NNs] - (TRUE_MEAN[NNs]**2)\n",
    "    R2[NNs] = 1-(MSE_SSE[NNs]/TRUE_VAR[NNs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = open(pathPKL+'2021_01_09_'+dataset+'_custom_'+'_061.pkl','wb')\n",
    "S = {\"TRES\":TRES,\"MSE_SSE\":MSE_SSE,\"NNarray\":NNarray,\n",
    "    \"R2\":R2}\n",
    "pickle.dump(S,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360.319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
