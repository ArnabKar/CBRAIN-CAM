{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 10/5/2020 - Rescaling the outputs and vertical interpolation after training the NN with rescaled inputs.  \n",
    "Heavily draws from [https://github.com/ankitesh97/CBRAIN-CAM/blob/climate_invar_classification/notebooks/ankitesh-devlog/10_Regression_New_Scalings.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "#import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "#import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "from cbrain.imports import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.normalization import *\n",
    "import h5py\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from cbrain.climate_invariant import *\n",
    "import yaml\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "coor = xr.open_dataset(\"/DFS-L/DATA/pritchard/tbeucler/SPCAM/sp8fbp_minus4k/sp8fbp_minus4k.cam2.h2.0000-01-01-00000.nc\",\\\n",
    "                    decode_times=False)\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();\n",
    "path = '/export/nfs0home/tbeucler/CBRAIN-CAM/cbrain/'\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz = 20\n",
    "lw = 4\n",
    "plt.rc('text',usetex=False)\n",
    "plt.rc('font',size=fz)\n",
    "plt.rc('font',**{'family':'serif','serif':['Computer Modern Roman']}, size=fz)\n",
    "mpl.rcParams['lines.linewidth'] = lw\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = load_pickle('/export/nfs0home/tbeucler/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "scale_dict['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'\n",
    "BASE_DIR = '/DFS-L/DATA/pritchard/ankitesg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = f\"{BASE_DIR}data/{TRAINFILE_RH}\",\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = f\"{BASE_DIR}data/{NORMFILE_RH}\",\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For T-TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_TNS = 'CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'CI_TNS_M4K_NORM_norm.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = f\"{BASE_DIR}data/{TRAINFILE_TNS}\",\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = f\"{BASE_DIR}data/{NORMFILE_TNS}\",\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this won't be used just to show we can use it overall\n",
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGeneratorClimInv(\n",
    "    data_fn = f\"{BASE_DIR}data/{TRAINFILE}\",\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = f\"{BASE_DIR}data/{NORMFILE}\",\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans = True,t2tns_trans=True,\n",
    "    lhflx_trans=True,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = f\"{BASE_DIR}data/{VALIDFILE}\",\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = f\"{BASE_DIR}data/{NORMFILE}\",\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans = True,t2tns_trans=True,\n",
    "    lhflx_trans=True,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator with TTNS and LHFLX transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGeneratorClimInv(\n",
    "    data_fn = f\"{BASE_DIR}data/{TRAINFILE}\",\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = f\"{BASE_DIR}data/{NORMFILE}\",\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans = True,t2tns_trans=True,\n",
    "    lhflx_trans=True,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = f\"{BASE_DIR}data/{VALIDFILE}\",\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = f\"{BASE_DIR}data/{NORMFILE}\",\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans = True,t2tns_trans=True,\n",
    "    lhflx_trans=True,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build diagnostics object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_HDF5 = '/DFS-L/DATA/pritchard/ankitesg/models/' # Path to NN weights\n",
    "config_file = 'CI_SP_M4K_CONFIG.yml' # Configuration file\n",
    "data_file = ['CI_SP_M4K_valid.nc','CI_SP_P4K_valid.nc']  # Validation/test data sets\n",
    "NNarray = ['CI01_RH_TNS_LHQsat.hdf5', 'RH_TNSV2_LHV2.hdf5','RH.hdf5', 'RH_TNS.hdf5'] # NN to evaluate \n",
    "NNname = ['RH-TNS-LH','RH-TNSV2-LHV2','RH', 'RH-TNS'] # Name of NNs for plotting\n",
    "dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer,\n",
    "            'QV2RH':QV2RH,'T2TmTNS':T2TmTNS,'eliq':eliq,'eice':eice,'esat':esat,'qv':qv,'RH':RH,\n",
    "           'reverseInterpLayer':reverseInterpLayer,'ScaleOp':ScaleOp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of different variables\n",
    "PHQ_idx = slice(0,30)\n",
    "TPHYSTND_idx = slice(30,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '$TRAINDIR/HDF5_DATA'\n",
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n",
      "NN name is  CI01_RH_TNS_LHQsat.hdf5\n",
      "data name is  CI_SP_M4K_valid.nc\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/export/nfs0home/ankitesg/CBrain_project/PrepData/CI_SP_M4K_CONFIG.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d68ee12c24bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                      \u001b[0mrh_trans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrh_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2tns_trans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt2tns_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                      \u001b[0mlhflx_trans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlhflx_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                                      model=model,exp=exp)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         md[NNs][data[6:-3]] = ModelDiagnostics(NN[NNs],\n",
      "\u001b[0;32m/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/climate_invariant.py\u001b[0m in \u001b[0;36mload_climate_model\u001b[0;34m(dict_lay, config_fn, data_fn, lev, hyam, hybm, TRAINDIR, inp_subRH, inp_divRH, inp_subTNS, inp_divTNS, nlat, nlon, nlev, ntime, rh_trans, t2tns_trans, lhflx_trans, scaling, interpolate, model, pos_model, neg_model, train_gen_RH_pos, train_gen_RH_neg, train_gen_TNS_pos, train_gen_TNS_neg, exp)\u001b[0m\n\u001b[1;32m   1092\u001b[0m                     \u001b[0mpos_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                     \u001b[0mtrain_gen_RH_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_gen_RH_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_gen_RH_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_gen_RH_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m                     train_gen_TNS_pos=train_gen_TNS_pos,train_gen_TNS_neg=train_gen_TNS_neg,exp=exp)\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/climate_invariant.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dict_lay, data_fn, config_fn, lev, hyam, hybm, TRAINDIR, nlat, nlon, nlev, ntime, inp_subRH, inp_divRH, inp_subTNS, inp_divTNS, rh_trans, t2tns_trans, lhflx_trans, scaling, interpolate, model, pos_model, neg_model, train_gen_RH_pos, train_gen_RH_neg, train_gen_TNS_pos, train_gen_TNS_neg, exp)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0mout_scale_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/export/nfs0home/ankitesg/CBrain_project/PrepData/CI_SP_M4K_CONFIG.yml'"
     ]
    }
   ],
   "source": [
    "NN = {}; md = {};\n",
    "%cd $TRAINDIR/HDF5_DATA\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs)\n",
    "    path = path_HDF5+NNs\n",
    "    \n",
    "    rh_trans=False\n",
    "    t2tns_trans=False\n",
    "    lhflx_trans=False\n",
    "    scaling=False\n",
    "    interpolate=False\n",
    "    model = path\n",
    "    exp=None\n",
    "    pos_model=None\n",
    "    neg_model=None\n",
    "    if 'RH' in NNs:\n",
    "        rh_trans=True\n",
    "    if 'TNS' in NNs:\n",
    "        t2tns_trans=True\n",
    "    if 'LH' in NNs:\n",
    "        lhflx_trans=True\n",
    "    if 'V2' in NNs:\n",
    "        if \"LHV2\":\n",
    "            exp = {\"LHFLX\":True}\n",
    "        else:\n",
    "            exp = {\"LHFLX\":False}\n",
    "        \n",
    "    if 'Scal' in NNs:\n",
    "        pos,neg = NNs.split('*')\n",
    "        pos_model = path_HDF5+pos\n",
    "        neg_model = path_HDF5+neg\n",
    "        model = None\n",
    "        scaling=True\n",
    "    if 'Interp' in NNs or 'Vert' in NNs:\n",
    "        interpolate=True\n",
    "        \n",
    "    md[NNs] = {}\n",
    "    for j,data in enumerate(data_file):\n",
    "        print('data name is ',data)\n",
    "        \n",
    "        NN[NNs] = load_climate_model(dict_lay,'/export/nfs0home/ankitesg/CBrain_project/PrepData/'+config_file,\n",
    "                                     '/DFS-L/DATA/pritchard/ankitesg/data/'+data,\n",
    "                                     lev=lev,hyam=hyam,hybm=hybm,TRAINDIR='',\n",
    "                                     inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "                                     inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "                                     rh_trans=rh_trans,t2tns_trans=t2tns_trans,\n",
    "                                     lhflx_trans=lhflx_trans,scaling=scaling,interpolate=interpolate,\n",
    "                                     model=model,exp=exp)\n",
    "        \n",
    "        md[NNs][data[6:-3]] = ModelDiagnostics(NN[NNs],\n",
    "                                                '/export/nfs0home/ankitesg/CBrain_project/PrepData/'+config_file,\n",
    "                                                '/DFS-L/DATA/pritchard/ankitesg/data/'+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tropical profile tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_ind = np.arange(26,40)\n",
    "iinis = [200]\n",
    "\n",
    "diagno = {} # Diagnostics structure\n",
    "diagno['truth'] = {} # Diagnostics structure for the truth\n",
    "diagno['truth_pos'] = {} # Diagnostics structure for the truth pos\n",
    "diagno['truth_neg'] = {} # Diagnostics structure for the truth neg\n",
    "diagno['MSE'] = {}\n",
    "truth_done = {}\n",
    "\n",
    "for j,data in enumerate(data_file):\n",
    "    truth_done[data[6:-3]] = False\n",
    "\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('i=',i,'& NNs=',NNs,'         ')\n",
    "    diagno[NNs] = {} # Diagnostics structure for each NN\n",
    "    diagno[NNs]['MSE'] = {}\n",
    "    for j,data in enumerate(data_file):\n",
    "        diagno[NNs][data[6:-3]]={}\n",
    "        diagno[NNs]['MSE'][data[6:-3]]={}\n",
    "        if i==0: \n",
    "            diagno['truth'][data[6:-3]]={}\n",
    "            diagno['truth_pos'][data[6:-3]]={}\n",
    "            diagno['truth_neg'][data[6:-3]]={}\n",
    "        for iini in iinis:\n",
    "            \n",
    "            print('j=',j,'& iini=',iini,'& data=',data,'         ',end='\\r'),\n",
    "            \n",
    "            iend = iini+47\n",
    "            diagno[NNs][data[6:-3]][iini] = {} # Diagnostics structure for each data file\n",
    "            if i==0: \n",
    "                diagno['truth'][data[6:-3]][iini] = {}\n",
    "                diagno['truth_pos'][data[6:-3]][iini] = {}\n",
    "                diagno['truth_neg'][data[6:-3]][iini] = {}\n",
    "                \n",
    "            for itime in tqdm(np.arange(iini,iend)):\n",
    "                # Get input, prediction and truth from NN\n",
    "                inp, p, truth = md[NNs][data[6:-3]].get_inp_pred_truth(itime)  # [lat, lon, var, lev]\n",
    "                truth_geo =  md[NNs][data[6:-3]].reshape_ngeo(truth)[:,:,:]\n",
    "\n",
    "                ## only if the scaling is true\n",
    "                if NN[NNs].scaling==True:\n",
    "                    X, _ = md[NNs][data[6:-3]].valid_gen[itime]\n",
    "                    mask, pos_op, neg_op = md[NNs][data[6:-3]].model.predict_on_batch_seperate(X.values)\n",
    "                    mask_reshaped = md[NNs][data[6:-3]].reshape_ngeo(mask)[lat_ind,:,:]\n",
    "                    mask = mask_reshaped.flatten()\n",
    "                    neg_mask = np.logical_not(mask)\n",
    "                    ## get the truth only once.\n",
    "                p = np.array(p)\n",
    "                p_geo = md[NNs][data[6:-3]].reshape_ngeo(p)[:,:,:]\n",
    "                # Get convective heating and moistening for each NN\n",
    "                if itime==iini:\n",
    "                    if i==0:\n",
    "                        diagno['truth'][data[6:-3]][iini]['PHQ'] = md[NNs][data[6:-3]].reshape_ngeo(truth[:,PHQ_idx])[lat_ind,:,:,np.newaxis]\n",
    "                        diagno['truth'][data[6:-3]][iini]['TPHYSTND'] = md[NNs][data[6:-3]].reshape_ngeo(truth[:,TPHYSTND_idx])[lat_ind,:,:,np.newaxis]\n",
    "                    ##if scaling is true and the truth array is not filled\n",
    "                    if NN[NNs].scaling==True and truth_done[data[6:-3]]==False:\n",
    "                        diagno['truth_pos'][data[6:-3]][iini]['PHQ_pos'] = md[NNs][data[6:-3]].reshape_ngeo(truth[:,PHQ_idx])[lat_ind,:,:].reshape(-1,30)[mask]\n",
    "                        diagno['truth_pos'][data[6:-3]][iini]['TPHYSTND_pos'] = md[NNs][data[6:-3]].reshape_ngeo(truth[:,TPHYSTND_idx])[lat_ind,:,:].reshape(-1,30)[mask]\n",
    "                        diagno['truth_neg'][data[6:-3]][iini]['PHQ_neg'] = md[NNs][data[6:-3]].reshape_ngeo(truth[:,PHQ_idx])[lat_ind,:,:].reshape(-1,30)[neg_mask]\n",
    "                        diagno['truth_neg'][data[6:-3]][iini]['TPHYSTND_neg'] = md[NNs][data[6:-3]].reshape_ngeo(truth[:,TPHYSTND_idx])[lat_ind,:,:].reshape(-1,30)[neg_mask]\n",
    "                        truth_done[data[6:-3]] = True\n",
    "                                                    \n",
    "                    diagno[NNs][data[6:-3]][iini]['PHQ'] = md[NNs][data[6:-3]].reshape_ngeo(p[:,PHQ_idx])[lat_ind,:,:,np.newaxis]\n",
    "                    diagno[NNs][data[6:-3]][iini]['TPHYSTND'] = md[NNs][data[6:-3]].reshape_ngeo(p[:,TPHYSTND_idx])[lat_ind,:,:,np.newaxis]\n",
    "                    \n",
    "#                     print(NNs)\n",
    "#                     print(\"----------------------\")\n",
    "                    diagno[NNs]['MSE'][data[6:-3]] = np.mean((truth_geo-p_geo)**2,axis=(1,2))\n",
    "#                     print(truth_geo)\n",
    "#                     print(p_geo)\n",
    "#                     print(diagno[NNs]['MSE'][data[6:-3]])\n",
    "#                     print(\"----------------------\")\n",
    "                    if NN[NNs].scaling==True:\n",
    "                        diagno[NNs][data[6:-3]][iini]['PHQ_pos'] = md[NNs][data[6:-3]].reshape_ngeo(p[:,PHQ_idx])[lat_ind,:,:].reshape(-1,30)[mask]\n",
    "                        diagno[NNs][data[6:-3]][iini]['TPHYSTND_pos'] = md[NNs][data[6:-3]].reshape_ngeo(p[:,TPHYSTND_idx])[lat_ind,:,:].reshape(-1,30)[mask]\n",
    "                        diagno[NNs][data[6:-3]][iini]['PHQ_neg'] = md[NNs][data[6:-3]].reshape_ngeo(p[:,PHQ_idx])[lat_ind,:,:].reshape(-1,30)[neg_mask]\n",
    "                        diagno[NNs][data[6:-3]][iini]['TPHYSTND_neg'] = md[NNs][data[6:-3]].reshape_ngeo(p[:,TPHYSTND_idx])[lat_ind,:,:].reshape(-1,30)[neg_mask]\n",
    "\n",
    "                else:\n",
    "                    diagno[NNs]['MSE'][data[6:-3]] = np.concatenate((diagno[NNs]['MSE'][data[6:-3]],\n",
    "                           np.mean((truth_geo-p_geo)**2,axis=(1,2))),axis=0)\n",
    "                    for istr,field in enumerate(['PHQ','TPHYSTND']):\n",
    "                        if field=='PHQ': ind_field = PHQ_idx\n",
    "                        elif field=='TPHYSTND': ind_field = TPHYSTND_idx\n",
    "                        diagno[NNs][data[6:-3]][iini][field] = np.concatenate((diagno[NNs][data[6:-3]][iini][field],\n",
    "                                                             md[NNs][data[6:-3]].\\\n",
    "                                                             reshape_ngeo(p[:,ind_field])[lat_ind,:,:,np.newaxis]),\n",
    "                                                            axis=3)\n",
    "                        if NN[NNs].scaling==True:\n",
    "                            diagno[NNs][data[6:-3]][iini][field+'_pos'] = np.concatenate((diagno[NNs][data[6:-3]][iini][field+'_pos'],\n",
    "                                         md[NNs][data[6:-3]].\\\n",
    "                                         reshape_ngeo(p[:,ind_field])[lat_ind,:,:].reshape(-1,30)[mask]),\n",
    "                                        axis=0)\n",
    "\n",
    "                            diagno[NNs][data[6:-3]][iini][field+'_neg'] = np.concatenate((diagno[NNs][data[6:-3]][iini][field+'_neg'],\n",
    "                                         md[NNs][data[6:-3]].\\\n",
    "                                         reshape_ngeo(p[:,ind_field])[lat_ind,:,:].reshape(-1,30)[neg_mask]),\n",
    "                                        axis=0)\n",
    "                        if i==0:\n",
    "\n",
    "                            diagno['truth'][data[6:-3]][iini][field] = np.concatenate((diagno['truth'][data[6:-3]][iini][field],\n",
    "                                                                     md[NNs][data[6:-3]].\\\n",
    "                                                                     reshape_ngeo(truth[:,ind_field])[lat_ind,:,:,np.newaxis]),\n",
    "                                                                    axis=3)\n",
    "                            \n",
    "                            if NN[NNs].scaling==True:\n",
    "                                diagno['truth_pos'][data[6:-3]][iini][field+'_pos'] = np.concatenate((diagno['truth_pos'][data[6:-3]][iini][field+'_pos'],\n",
    "                                             md[NNs][data[6:-3]].\\\n",
    "                                             reshape_ngeo(truth[:,ind_field])[lat_ind,:,:].reshape(-1,30)[mask]),\n",
    "                                            axis=0)\n",
    "                                diagno['truth_neg'][data[6:-3]][iini][field+'_neg'] = np.concatenate((diagno['truth_neg'][data[6:-3]][iini][field+'_neg'],\n",
    "                                         md[NNs][data[6:-3]].\\\n",
    "                                         reshape_ngeo(truth[:,ind_field])[lat_ind,:,:].reshape(-1,30)[neg_mask]),\n",
    "                                        axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_file[0][6:-3]\n",
    "plt.figure(figsize=(30,15))\n",
    "iini = iinis[0]\n",
    "plt.subplot(1,2,1)\n",
    "print(NNname)\n",
    "\n",
    "plt.axvline(x=0,c='lightgray')\n",
    "for iNN,NNs in enumerate(NNarray):\n",
    "    plt.plot(np.mean(diagno[NNs][data][iini]['PHQ'],axis=(0,1,3)),lev,label=NNname[iNN])\n",
    "plt.plot(np.mean(diagno['truth'][data][iini]['PHQ'],axis=(0,1,3)),lev,label='Truth',color='k')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Convective moistening ($\\mathrm{W\\ m^{-2}}$)')\n",
    "plt.ylabel('Pressure (hPa)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('data= '+data+' '+ '--- iini = '+str(iini))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axvline(x=0,c='lightgray')\n",
    "for iNN,NNs in enumerate(NNarray):\n",
    "    plt.plot(np.mean(diagno[NNs][data][iini]['TPHYSTND'],axis=(0,1,3)),lev,label=NNname[iNN])\n",
    "plt.plot(np.mean(diagno['truth'][data][iini]['TPHYSTND'],axis=(0,1,3)),lev,label='Truth',color='k')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Convective heating ($\\mathrm{W\\ m^{-2}}$)')\n",
    "plt.title('data= '+data+' '+ '--- iini = '+str(iini))\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_file[1][6:-3]\n",
    "plt.figure(figsize=(30,15))\n",
    "iini = iinis[0]\n",
    "plt.subplot(1,2,1)\n",
    "plt.axvline(x=0,c='lightgray')\n",
    "for iNN,NNs in enumerate(NNarray):\n",
    "    plt.plot(np.mean(diagno[NNs][data][iini]['PHQ'],axis=(0,1,3)),lev,label=NNname[iNN])\n",
    "plt.plot(np.mean(diagno['truth'][data][iini]['PHQ'],axis=(0,1,3)),lev,label='Truth',color='k')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Convective moistening ($\\mathrm{W\\ m^{-2}}$)')\n",
    "plt.ylabel('Pressure (hPa)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('data= '+data+' '+ '--- iini = '+str(iini))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axvline(x=0,c='lightgray')\n",
    "for iNN,NNs in enumerate(NNarray):\n",
    "    plt.plot(np.mean(diagno[NNs][data][iini]['TPHYSTND'],axis=(0,1,3)),lev,label=NNname[iNN])\n",
    "plt.plot(np.mean(diagno['truth'][data][iini]['TPHYSTND'],axis=(0,1,3)),lev,label='Truth',color='k')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Convective heating ($\\mathrm{W\\ m^{-2}}$)')\n",
    "plt.title('data= '+data+' '+ '--- iini = '+str(iini))\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_HDF5 = '/DFS-L/DATA/pritchard/ankitesg/models/' # Path to NN weights\n",
    "config_file = 'CI_SP_M4K_CONFIG.yml' # Configuration file\n",
    "data_file = ['CI_SP_M4K_valid.nc','CI_SP_P4K_valid.nc']  # Validation/test data sets\n",
    "NNarray = ['BF.hdf5','CI01_RH_TNS_LHQsat.hdf5', 'RH_TNSV2_LHV2.hdf5','RH.hdf5', 'RH_TNS.hdf5'] # NN to evaluate \n",
    "NNname = ['BF','RH-TNS-LH','RH-TNSV2-LHV2','RH', 'RH-TNS'] # Name of NNs for plotting\n",
    "dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer,\n",
    "            'QV2RH':QV2RH,'T2TmTNS':T2TmTNS,'eliq':eliq,'eice':eice,'esat':esat,'qv':qv,'RH':RH,\n",
    "           'reverseInterpLayer':reverseInterpLayer,'ScaleOp':ScaleOp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define default values\n",
    "\n",
    "\n",
    "NN = {}; md = {};\n",
    "%cd $TRAINDIR/HDF5_DATA\n",
    "for i,NNs in enumerate(NNarray):\n",
    "    print('NN name is ',NNs)\n",
    "    path = path_HDF5+NNs\n",
    "    \n",
    "    rh_trans=False\n",
    "    t2tns_trans=False\n",
    "    lhflx_trans=False\n",
    "    scaling=False\n",
    "    interpolate=False\n",
    "    model = path\n",
    "    exp=None\n",
    "    pos_model=None\n",
    "    neg_model=None\n",
    "    if 'RH' in NNs:\n",
    "        rh_trans=True\n",
    "    if 'TNS' in NNs:\n",
    "        t2tns_trans=True\n",
    "    if 'LH' in NNs:\n",
    "        lhflx_trans=True\n",
    "    if 'V2' in NNs:\n",
    "        if \"LHV2\":\n",
    "            exp = {\"LHFLX\":True}\n",
    "        else:\n",
    "            exp = {\"LHFLX\":False}\n",
    "        \n",
    "    if 'Scal' in NNs:\n",
    "        pos,neg = NNs.split('*')\n",
    "        pos_model = path_HDF5+pos\n",
    "        neg_model = path_HDF5+neg\n",
    "        model = None\n",
    "        scaling=True\n",
    "    if 'Interp' in NNs or 'Vert' in NNs:\n",
    "        interpolate=True\n",
    "        \n",
    "    md[NNs] = {}\n",
    "    for j,data in enumerate(data_file):\n",
    "        print('data name is ',data)\n",
    "        \n",
    "        NN[NNs] = load_climate_model(dict_lay,'/export/nfs0home/ankitesg/CBrain_project/PrepData/'+config_file,\n",
    "                                     '/DFS-L/DATA/pritchard/ankitesg/data/'+data,\n",
    "                                     lev=lev,hyam=hyam,hybm=hybm,TRAINDIR='',\n",
    "                                     inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "                                     inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "                                     rh_trans=rh_trans,t2tns_trans=t2tns_trans,\n",
    "                                     lhflx_trans=lhflx_trans,scaling=scaling,interpolate=interpolate,\n",
    "                                     model=model,exp=exp)\n",
    "        \n",
    "        md[NNs][data[6:-3]] = ModelDiagnostics(NN[NNs],\n",
    "                                                '/export/nfs0home/ankitesg/CBrain_project/PrepData/'+config_file,\n",
    "                                                '/DFS-L/DATA/pritchard/ankitesg/data/'+data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 30\n",
    "data = data_file[0]\n",
    "t_random = np.random.choice(np.linspace(0,md[NNs][data[6:-3]].valid_gen.n_batches-1,\n",
    "                                        md[NNs][data[6:-3]].valid_gen.n_batches),\n",
    "                            size=((Nt,)),replace=False).astype('int')\n",
    "\n",
    "MSE = {}\n",
    "VAR = {}\n",
    "diagno = {}\n",
    "diagno['truth'] = {}\n",
    "diagno['pred'] = {}\n",
    "\n",
    "for iar,itime in enumerate(t_random):\n",
    "    print('iar=',iar,'/',Nt-1,' & itime',itime)\n",
    "    for i,NNs in enumerate(NNarray):\n",
    "        if iar==0: MSE[NNs] = {}; VAR[NNs] = {}\n",
    "        for j,data in enumerate(data_file):\n",
    "            #print('j=',j,'data=',data)\n",
    "\n",
    "            inp, p, truth = md[NNs][data[6:-3]].get_inp_pred_truth(itime)  # [lat, lon, var, lev]\n",
    "\n",
    "            t_geo = md[NNs][data[6:-3]].reshape_ngeo(truth)[:,:,:]\n",
    "            if tf.is_tensor(p): p_geo = md[NNs][data[6:-3]].reshape_ngeo(p.numpy())[:,:,:]\n",
    "            else: p_geo = md[NNs][data[6:-3]].reshape_ngeo(p)[:,:,:]\n",
    "\n",
    "            if iar==0: \n",
    "                MSE[NNs][data[6:-3]] = np.mean((t_geo-p_geo)**2,axis=(1,2))\n",
    "                VAR[NNs][data[6:-3]] = np.var(p_geo,axis=(1,2))\n",
    "            else: \n",
    "                MSE[NNs][data[6:-3]] = np.concatenate((MSE[NNs][data[6:-3]],\n",
    "                                                       np.mean((t_geo-p_geo)**2,axis=(1,2))),axis=0)\n",
    "                VAR[NNs][data[6:-3]] = np.concatenate((VAR[NNs][data[6:-3]],\n",
    "                                                       np.var(p_geo,axis=(1,2))),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(NNname):\n",
    "    print(f\"{name} MSE={np.sqrt(MSE[NNarray[i]]['M4K_valid'].mean())} STD={np.sqrt(MSE[NNarray[i]]['M4K_valid'].std())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(NNname):\n",
    "    print(f\"{name} MSE={np.sqrt(MSE[NNarray[i]]['P4K_valid'].mean())} STD={np.sqrt(MSE[NNarray[i]]['P4K_valid'].std())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentile interpolation idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_path = '/export/nfs0home/ankitesg/data/percentile_data_bin_size_1000.pkl'\n",
    "perc_array_m4k = load_pickle(perc_path)['Percentile']['M4K']\n",
    "perc_array_p4k = load_pickle(perc_path)['Percentile']['P4K']\n",
    "PERC_BINS = np.linspace(0,100,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model RH.hdf5\n",
    "md_name = 'RH.hdf5'\n",
    "model = md[md_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_physical_to_percentile(p, PERC):\n",
    "    ans = []\n",
    "    var_dict = {\"PHQ\":PHQ_idx, \"TPHYSTND\":TPHYSTND_idx}\n",
    "    for var in var_dict.keys():\n",
    "        p_var = p[:,var_dict[var]]\n",
    "        perc_var = PERC[var]\n",
    "        lev_preds = []\n",
    "        for ilev in range(30):\n",
    "            lev_preds.append(np.interp(p_var[:,ilev],perc_var[ilev],PERC_BINS))\n",
    "        p_percentile = np.stack(lev_preds, axis=1)\n",
    "        ans.append(p_percentile)\n",
    "    return np.concatenate(ans,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_percentile_to_physical(perc_val, PERC):\n",
    "    ans = []\n",
    "    var_dict = {\"PHQ\":PHQ_idx, \"TPHYSTND\":TPHYSTND_idx}\n",
    "    for var in var_dict.keys():\n",
    "        perc_var = perc_val[:,var_dict[var]]\n",
    "        perc = PERC[var]\n",
    "        lev_preds = []\n",
    "        for ilev in range(30):\n",
    "            lev_preds.append(np.interp(perc_var[:,ilev],PERC_BINS,perc[ilev]))\n",
    "        p = np.stack(lev_preds, axis=1)\n",
    "        ans.append(p)\n",
    "        \n",
    "    return np.concatenate(ans,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_ind = np.arange(26,40)\n",
    "iinis = 200\n",
    "iend = iini+47\n",
    "\n",
    "vals = {}\n",
    "truth_vals = {}\n",
    "for itime in tqdm(np.arange(iini,iend)):\n",
    "    inp, p, truth = model['P4K_valid'].get_inp_pred_truth(itime)  \n",
    "    p = p.numpy()\n",
    "    percentil_space = convert_from_physical_to_percentile(p, perc_array_m4k)\n",
    "    val = convert_from_percentile_to_physical(percentil_space,perc_array_p4k)\n",
    "    if itime==iini:\n",
    "        truth_vals['PHQ'] = model['P4K_valid'].reshape_ngeo(truth[:,PHQ_idx])[lat_ind,:,:,np.newaxis]\n",
    "        truth_vals['TPHYSTND'] = model['P4K_valid'].reshape_ngeo(truth[:,TPHYSTND_idx])[lat_ind,:,:,np.newaxis]\n",
    "        vals['PHQ'] = model['P4K_valid'].reshape_ngeo(val[:,PHQ_idx])[lat_ind,:,:,np.newaxis]\n",
    "        vals['TPHYSTND'] = model['P4K_valid'].reshape_ngeo(val[:,TPHYSTND_idx])[lat_ind,:,:,np.newaxis]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        vals['PHQ']  = np.concatenate((vals['PHQ'],\n",
    "                                      model['P4K_valid'].\\\n",
    "                                     reshape_ngeo(p[:,PHQ_idx])[lat_ind,:,:,np.newaxis]),\n",
    "                                    axis=3)\n",
    "        vals['TPHYSTND']  = np.concatenate((vals['TPHYSTND'],\n",
    "                                      model['P4K_valid'].\\\n",
    "                                     reshape_ngeo(p[:,TPHYSTND_idx])[lat_ind,:,:,np.newaxis]),\n",
    "                                    axis=3)\n",
    "        \n",
    "        truth_vals['PHQ'] = np.concatenate((truth_vals['PHQ'],\n",
    "                                                     model['P4K_valid'].\\\n",
    "                                                     reshape_ngeo(truth[:,PHQ_idx])[lat_ind,:,:,np.newaxis]),\n",
    "                                                    axis=3)\n",
    "        \n",
    "        truth_vals['TPHYSTND'] = np.concatenate((truth_vals['TPHYSTND'],\n",
    "                                             model['P4K_valid'].\\\n",
    "                                             reshape_ngeo(truth[:,TPHYSTND_idx])[lat_ind,:,:,np.newaxis]),\n",
    "                                            axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_file[1][6:-3]\n",
    "name = 'RH'\n",
    "plt.figure(figsize=(30,15))\n",
    "iini = 200\n",
    "plt.subplot(1,2,1)\n",
    "plt.axvline(x=0,c='lightgray')\n",
    "plt.plot(np.mean(vals['PHQ'],axis=(0,1,3)),lev,label=name)\n",
    "plt.plot()\n",
    "plt.plot(np.mean(diagno['RH.hdf5'][data][iini]['PHQ'],axis=(0,1,3)),lev,label=f'{name}-OG')\n",
    "plt.plot(np.mean(truth_vals['PHQ'],axis=(0,1,3)),lev,label='Truth',color='k')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Convective moistening ($\\mathrm{W\\ m^{-2}}$)')\n",
    "plt.ylabel('Pressure (hPa)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('data= '+data+' '+ '--- iini = '+str(iini))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axvline(x=0,c='lightgray')\n",
    "plt.plot(np.mean(vals['TPHYSTND'],axis=(0,1,3)),lev,label=name)\n",
    "plt.plot(np.mean(diagno['RH.hdf5'][data][iini]['TPHYSTND'],axis=(0,1,3)),lev,label=f'{name}-OG')\n",
    "plt.plot(np.mean(truth_vals['TPHYSTND'],axis=(0,1,3)),lev,label='Truth',color='k')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Convective heating ($\\mathrm{W\\ m^{-2}}$)')\n",
    "plt.title('data= '+data+' '+ '--- iini = '+str(iini))\n",
    "plt.gca().invert_yaxis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
