{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n",
      "/nfspool-0/home/tbeucler/CBRAIN-CAM\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "\n",
    "#TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/'\n",
    "PREFIX = '8col009_01_'\n",
    "#%cd /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM\n",
    "%cd /export/home/tbeucler/CBRAIN-CAM\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "#limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom generator (all outputs minus the residual ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom generator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking as argument the *output* indices it will not be trained on **out_cut_off** (var_cut_off refers to the *input* indices it is not trained on). **out_cut_off** will be formatted as a dictionary with int entries corresponding to the single index to exclude from the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function cbrain.utils.return_var_idxs(ds, var_list, var_cut_off=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_var_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_var_idxs_outputcutoff(ds, var_list, out_cut_off=None):\n",
    "    \"\"\"\n",
    "    To be used on stacked variable dimension. Returns indices array\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray dataset\n",
    "    var_list: list of variables\n",
    "    Returns\n",
    "    -------\n",
    "    var_idxs: indices array\n",
    "    \"\"\"\n",
    "    if out_cut_off is None:\n",
    "        var_idxs = np.concatenate([np.where(ds.var_names == v)[0] for v in var_list])\n",
    "    else:\n",
    "        idxs_list = []\n",
    "        for v in var_list:\n",
    "            i = np.where(ds.var_names == v)[0]\n",
    "            if v in out_cut_off.keys():\n",
    "                i = np.delete(i,out_cut_off[v])\n",
    "            idxs_list.append(i)\n",
    "        var_idxs = np.concatenate(idxs_list)\n",
    "    return var_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictNormalizer_outputcutoff(object):\n",
    "    \"\"\"Normalizer that takes a conversion dictionary as input. Simply scales by factors in dict.\"\"\"\n",
    "    def __init__(self, norm_ds, var_list, dic=None,out_cut_off=None):\n",
    "        if dic is None: dic = conversion_dict\n",
    "        var_idxs = return_var_idxs_outputcutoff(norm_ds, var_list, out_cut_off=out_cut_off)\n",
    "        var_names = norm_ds.var_names[var_idxs].copy()\n",
    "        scale = []\n",
    "        for v in var_list:\n",
    "            s = np.atleast_1d(dic[v])\n",
    "            # Modification below: Delete scaling factor for outputs\n",
    "            # that have been cut off via out_cut_off \n",
    "            if v in out_cut_off.keys(): s = np.delete(s,out_cut_off[v])\n",
    "            scale.append(s)\n",
    "        self.scale = np.concatenate(scale).astype('float32')\n",
    "        self.transform_arrays = {\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_outputcutoff(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    Data generator class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=1024, shuffle=True, xarray=False, var_cut_off=None,\n",
    "                out_cut_off=None):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn, self.norm_fn = data_fn, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_dataset(data_fn)\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "\n",
    "        # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.vars.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        # Get input and output variable indices\n",
    "        self.input_idxs = return_var_idxs(self.data_ds, input_vars, var_cut_off)\n",
    "        self.output_idxs = return_var_idxs_outputcutoff(self.data_ds, output_vars, out_cut_off=out_cut_off)\n",
    "        self.n_inputs, self.n_outputs = len(self.input_idxs), len(self.output_idxs)\n",
    "\n",
    "        # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer_outputcutoff(self.norm_ds, output_vars, output_transform,\n",
    "                                                                out_cut_off=out_cut_off)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "        if not xarray:\n",
    "            self.data_ds.close()\n",
    "            self.data_ds = h5py.File(data_fn, 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom generator and compare to standard generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the manuscript's purposes, we will choose the lowest levels as the residuals for direct comparison with the reference ACnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILEQ = '8col009_01_train.nc'\n",
    "VALIDFILEQ = '8col009_01_valid.nc'\n",
    "NORMFILEQ = '8col009_01_norm.nc'\n",
    "TESTFILEQ = '8col009_01_test.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dictQ = load_pickle('./nn_config/scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_varsQ = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_varsQ = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FSNS', 'FLNT', 'FLNS', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_custom = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_custom = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FLNT', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 29\n",
    "out_cut_off_low['TPHYSTND'] = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PHQ': 29, 'TPHYSTND': 29}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cut_off_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+TRAINFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+VALIDFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+TESTFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214 = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'UCnet_214_1'\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCnet_214.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41376/41376 [==============================] - 3373s 82ms/step - loss: 181.6153 - val_loss: 169.9962\n",
      "Epoch 2/10\n",
      "41376/41376 [==============================] - 5031s 122ms/step - loss: 170.8664 - val_loss: 174.2553\n",
      "Epoch 3/10\n",
      "41376/41376 [==============================] - 4971s 120ms/step - loss: 166.1586 - val_loss: 167.2636\n",
      "Epoch 4/10\n",
      "41376/41376 [==============================] - 5081s 123ms/step - loss: 163.3579 - val_loss: 162.2961\n",
      "Epoch 5/10\n",
      "41376/41376 [==============================] - 4938s 119ms/step - loss: 161.5349 - val_loss: 162.5542\n",
      "Epoch 6/10\n",
      "41376/41376 [==============================] - 5280s 128ms/step - loss: 159.8486 - val_loss: 173.2181\n",
      "Epoch 7/10\n",
      "41376/41376 [==============================] - 4974s 120ms/step - loss: 158.5363 - val_loss: 157.1120\n",
      "Epoch 8/10\n",
      "41376/41376 [==============================] - 4997s 121ms/step - loss: 157.2909 - val_loss: 157.3662\n",
      "Epoch 9/10\n",
      "41376/41376 [==============================] - 4868s 118ms/step - loss: 156.4750 - val_loss: 160.8959\n",
      "Epoch 10/10\n",
      "41376/41376 [==============================] - 4801s 116ms/step - loss: 155.7042 - val_loss: 158.6559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae7e79f160>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "UCnet_214.fit_generator(train_gen_custom, epochs=Nep, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train other NNs corresponding to the five NNs with optimized $\\beta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q8T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 8\n",
    "out_cut_off_low['TPHYSTND'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PHQ': 8, 'TPHYSTND': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cut_off_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214_q8T4 = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCnet_214_q8T4.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'UCnet_214_q8T4'\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCnet_214_q8T4.load_weights(path_HDF5+name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "41376/41376 [==============================] - 4416s 107ms/step - loss: 157.9035 - val_loss: 158.9887\n",
      "Epoch 2/5\n",
      "41376/41376 [==============================] - 3548s 86ms/step - loss: 156.9145 - val_loss: 164.0828\n",
      "Epoch 3/5\n",
      "41376/41376 [==============================] - 4317s 104ms/step - loss: 156.3070 - val_loss: 160.4873\n",
      "Epoch 4/5\n",
      "41376/41376 [==============================] - 3639s 88ms/step - loss: 155.3187 - val_loss: 153.5644\n",
      "Epoch 5/5\n",
      "41376/41376 [==============================] - 4191s 101ms/step - loss: 154.9518 - val_loss: 158.3485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f787c3dcb38>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCnet_214_q8T4.fit_generator(train_gen_custom, epochs=5, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "                        callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = Input(shape=(304,))\n",
    "# densout = Dense(512, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (4):\n",
    "#     densout = Dense(512, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# out = Dense(214, activation='linear')(densout)\n",
    "# UCnet_214_q8T4 = tf.keras.models.Model(inp, out)\n",
    "\n",
    "# name = 'UCnet_214_q8T4'\n",
    "# path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "# mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# UCnet_214_q8T4.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "\n",
    "# UCnet_214_q8T4.fit_generator(train_gen_custom, epochs=Nep, \n",
    "#                         validation_data=valid_gen_custom,\\\n",
    "#                         callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q3T26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 3\n",
    "out_cut_off_low['TPHYSTND'] = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41376/41376 [==============================] - 4213s 102ms/step - loss: 206.9813 - val_loss: 178.2350\n",
      "Epoch 2/10\n",
      "41376/41376 [==============================] - 4278s 103ms/step - loss: 174.7543 - val_loss: 180.5182\n",
      "Epoch 3/10\n",
      "41376/41376 [==============================] - 4243s 103ms/step - loss: 167.9307 - val_loss: 167.4441\n",
      "Epoch 4/10\n",
      "41376/41376 [==============================] - 3697s 89ms/step - loss: 164.3355 - val_loss: 174.1542\n",
      "Epoch 5/10\n",
      "41376/41376 [==============================] - 3825s 92ms/step - loss: 162.1696 - val_loss: 159.8734\n",
      "Epoch 6/10\n",
      "41376/41376 [==============================] - 3868s 93ms/step - loss: 160.4272 - val_loss: 160.4866\n",
      "Epoch 7/10\n",
      "41376/41376 [==============================] - 3637s 88ms/step - loss: 159.0526 - val_loss: 163.9074\n",
      "Epoch 8/10\n",
      "41376/41376 [==============================] - 3555s 86ms/step - loss: 157.9185 - val_loss: 159.7694\n",
      "Epoch 9/10\n",
      "41376/41376 [==============================] - 3612s 87ms/step - loss: 156.8869 - val_loss: 165.4429\n",
      "Epoch 10/10\n",
      "41376/41376 [==============================] - 3400s 82ms/step - loss: 156.0188 - val_loss: 159.7260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f787c0c2cf8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214_q3T26 = tf.keras.models.Model(inp, out)\n",
    "\n",
    "name = 'UCnet_214_q3T26'\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "UCnet_214_q3T26.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "\n",
    "UCnet_214_q3T26.fit_generator(train_gen_custom, epochs=Nep, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "                        callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q4T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 4\n",
    "out_cut_off_low['TPHYSTND'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.engine.input_layer.Input(shape=None, batch_size=None, name=None, dtype=None, sparse=False, tensor=None, ragged=False, **kwargs)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214_q4T4 = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41376/41376 [==============================] - 1902s 46ms/step - loss: 206.8369 - val_loss: 175.4781\n",
      "Epoch 2/10\n",
      "41376/41376 [==============================] - 2434s 59ms/step - loss: 174.4162 - val_loss: 170.0619\n",
      "Epoch 3/10\n",
      "41376/41376 [==============================] - 2245s 54ms/step - loss: 168.2906 - val_loss: 161.8403\n",
      "Epoch 4/10\n",
      "41376/41376 [==============================] - 2186s 53ms/step - loss: 164.6291 - val_loss: 160.8909\n",
      "Epoch 5/10\n",
      "41376/41376 [==============================] - 2055s 50ms/step - loss: 162.2459 - val_loss: 170.4000\n",
      "Epoch 6/10\n",
      "41376/41376 [==============================] - 2152s 52ms/step - loss: 160.5727 - val_loss: 161.7583\n",
      "Epoch 7/10\n",
      "41376/41376 [==============================] - 2048s 49ms/step - loss: 158.9995 - val_loss: 157.2331\n",
      "Epoch 8/10\n",
      "41376/41376 [==============================] - 1981s 48ms/step - loss: 157.9674 - val_loss: 159.9227\n",
      "Epoch 9/10\n",
      "41376/41376 [==============================] - 1946s 47ms/step - loss: 157.0697 - val_loss: 156.6100\n",
      "Epoch 10/10\n",
      "41376/41376 [==============================] - 2638s 64ms/step - loss: 156.3126 - val_loss: 158.5905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1b8031e438>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214_q4T4 = tf.keras.models.Model(inp, out)\n",
    "\n",
    "name = 'UCnet_214_q4T4'\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "UCnet_214_q4T4.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "\n",
    "UCnet_214_q4T4.fit_generator(train_gen_custom, epochs=Nep, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "                        callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q5T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 5\n",
    "out_cut_off_low['TPHYSTND'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41376/41376 [==============================] - 1853s 45ms/step - loss: 208.1494 - val_loss: 177.1035\n",
      "Epoch 2/10\n",
      "41376/41376 [==============================] - 2613s 63ms/step - loss: 174.5985 - val_loss: 172.5894\n",
      "Epoch 3/10\n",
      "41376/41376 [==============================] - 2964s 72ms/step - loss: 168.2515 - val_loss: 167.3098\n",
      "Epoch 4/10\n",
      "41376/41376 [==============================] - 3063s 74ms/step - loss: 164.6474 - val_loss: 180.5366\n",
      "Epoch 5/10\n",
      "41376/41376 [==============================] - 3032s 73ms/step - loss: 162.3271 - val_loss: 160.0432\n",
      "Epoch 6/10\n",
      "41376/41376 [==============================] - 3263s 79ms/step - loss: 160.5940 - val_loss: 156.8561\n",
      "Epoch 7/10\n",
      "41376/41376 [==============================] - 3361s 81ms/step - loss: 159.2423 - val_loss: 165.6498\n",
      "Epoch 8/10\n",
      "41376/41376 [==============================] - 2760s 67ms/step - loss: 157.9129 - val_loss: 177.1692\n",
      "Epoch 9/10\n",
      "41376/41376 [==============================] - 3338s 81ms/step - loss: 157.0045 - val_loss: 157.2837\n",
      "Epoch 10/10\n",
      "41376/41376 [==============================] - 3478s 84ms/step - loss: 156.0235 - val_loss: 155.4518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1b802c67f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214_q5T5 = tf.keras.models.Model(inp, out)\n",
    "\n",
    "name = 'UCnet_214_q5T5'\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "UCnet_214_q5T5.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "\n",
    "UCnet_214_q5T5.fit_generator(train_gen_custom, epochs=Nep, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "                        callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q18T28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 18\n",
    "out_cut_off_low['TPHYSTND'] = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41376/41376 [==============================] - 2515s 61ms/step - loss: 207.3434 - val_loss: 176.6790\n",
      "Epoch 2/10\n",
      "41376/41376 [==============================] - 2864s 69ms/step - loss: 174.2243 - val_loss: 178.8526\n",
      "Epoch 3/10\n",
      "41376/41376 [==============================] - 2396s 58ms/step - loss: 167.8845 - val_loss: 165.2047\n",
      "Epoch 4/10\n",
      "41376/41376 [==============================] - 2501s 60ms/step - loss: 164.5342 - val_loss: 163.8657\n",
      "Epoch 5/10\n",
      "41376/41376 [==============================] - 2565s 62ms/step - loss: 162.1137 - val_loss: 159.6605\n",
      "Epoch 6/10\n",
      "41376/41376 [==============================] - 2683s 65ms/step - loss: 160.3366 - val_loss: 158.0646\n",
      "Epoch 7/10\n",
      "41376/41376 [==============================] - 2429s 59ms/step - loss: 158.9218 - val_loss: 159.1966\n",
      "Epoch 8/10\n",
      "41376/41376 [==============================] - 2123s 51ms/step - loss: 157.8893 - val_loss: 157.3681\n",
      "Epoch 9/10\n",
      "41376/41376 [==============================] - 2467s 60ms/step - loss: 157.0358 - val_loss: 157.2084\n",
      "Epoch 10/10\n",
      "41376/41376 [==============================] - 1954s 47ms/step - loss: 156.0582 - val_loss: 154.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1b80221ac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214_q18T28 = tf.keras.models.Model(inp, out)\n",
    "\n",
    "name = 'UCnet_214_q18T28'\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "UCnet_214_q18T28.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "\n",
    "UCnet_214_q18T28.fit_generator(train_gen_custom, epochs=Nep, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "                        callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360.319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
