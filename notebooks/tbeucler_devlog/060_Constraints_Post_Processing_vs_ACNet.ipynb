{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n",
      "/nfspool-0/home/tbeucler/CBRAIN-CAM\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "\n",
    "#TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/'\n",
    "PREFIX = '8col009_01_'\n",
    "#%cd /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM\n",
    "%cd /export/home/tbeucler/CBRAIN-CAM\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "#limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom generator (all outputs minus the residual ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom generator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking as argument the *output* indices it will not be trained on **out_cut_off** (var_cut_off refers to the *input* indices it is not trained on). **out_cut_off** will be formatted as a dictionary with int entries corresponding to the single index to exclude from the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function cbrain.utils.return_var_idxs(ds, var_list, var_cut_off=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_var_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_var_idxs_outputcutoff(ds, var_list, out_cut_off=None):\n",
    "    \"\"\"\n",
    "    To be used on stacked variable dimension. Returns indices array\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray dataset\n",
    "    var_list: list of variables\n",
    "    Returns\n",
    "    -------\n",
    "    var_idxs: indices array\n",
    "    \"\"\"\n",
    "    if out_cut_off is None:\n",
    "        var_idxs = np.concatenate([np.where(ds.var_names == v)[0] for v in var_list])\n",
    "    else:\n",
    "        idxs_list = []\n",
    "        for v in var_list:\n",
    "            i = np.where(ds.var_names == v)[0]\n",
    "            if v in out_cut_off.keys():\n",
    "                i = np.delete(i,out_cut_off[v])\n",
    "            idxs_list.append(i)\n",
    "        var_idxs = np.concatenate(idxs_list)\n",
    "    return var_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictNormalizer_outputcutoff(object):\n",
    "    \"\"\"Normalizer that takes a conversion dictionary as input. Simply scales by factors in dict.\"\"\"\n",
    "    def __init__(self, norm_ds, var_list, dic=None,out_cut_off=None):\n",
    "        if dic is None: dic = conversion_dict\n",
    "        var_idxs = return_var_idxs_outputcutoff(norm_ds, var_list, out_cut_off=out_cut_off)\n",
    "        var_names = norm_ds.var_names[var_idxs].copy()\n",
    "        scale = []\n",
    "        for v in var_list:\n",
    "            s = np.atleast_1d(dic[v])\n",
    "            # Modification below: Delete scaling factor for outputs\n",
    "            # that have been cut off via out_cut_off \n",
    "            if v in out_cut_off.keys(): s = np.delete(s,out_cut_off[v])\n",
    "            scale.append(s)\n",
    "        self.scale = np.concatenate(scale).astype('float32')\n",
    "        self.transform_arrays = {\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_outputcutoff(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    Data generator class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "                 norm_fn=None, input_transform=None, output_transform=None,\n",
    "                 batch_size=1024, shuffle=True, xarray=False, var_cut_off=None,\n",
    "                out_cut_off=None):\n",
    "        # Just copy over the attributes\n",
    "        self.data_fn, self.norm_fn = data_fn, norm_fn\n",
    "        self.input_vars, self.output_vars = input_vars, output_vars\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "        # Open datasets\n",
    "        self.data_ds = xr.open_dataset(data_fn)\n",
    "        if norm_fn is not None: self.norm_ds = xr.open_dataset(norm_fn)\n",
    "\n",
    "        # Compute number of samples and batches\n",
    "        self.n_samples = self.data_ds.vars.shape[0]\n",
    "        self.n_batches = int(np.floor(self.n_samples) / self.batch_size)\n",
    "\n",
    "        # Get input and output variable indices\n",
    "        self.input_idxs = return_var_idxs(self.data_ds, input_vars, var_cut_off)\n",
    "        self.output_idxs = return_var_idxs_outputcutoff(self.data_ds, output_vars, out_cut_off=out_cut_off)\n",
    "        self.n_inputs, self.n_outputs = len(self.input_idxs), len(self.output_idxs)\n",
    "\n",
    "        # Initialize input and output normalizers/transformers\n",
    "        if input_transform is None:\n",
    "            self.input_transform = Normalizer()\n",
    "        elif type(input_transform) is tuple:\n",
    "            self.input_transform = InputNormalizer(\n",
    "                self.norm_ds, input_vars, input_transform[0], input_transform[1], var_cut_off)\n",
    "        else:\n",
    "            self.input_transform = input_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        if output_transform is None:\n",
    "            self.output_transform = Normalizer()\n",
    "        elif type(output_transform) is dict:\n",
    "            self.output_transform = DictNormalizer_outputcutoff(self.norm_ds, output_vars, output_transform,\n",
    "                                                                out_cut_off=out_cut_off)\n",
    "        else:\n",
    "            self.output_transform = output_transform  # Assume an initialized normalizer is passed\n",
    "\n",
    "        # Now close the xarray file and load it as an h5 file instead\n",
    "        # This significantly speeds up the reading of the data...\n",
    "        if not xarray:\n",
    "            self.data_ds.close()\n",
    "            self.data_ds = h5py.File(data_fn, 'r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "\n",
    "        # Normalize\n",
    "        X = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(self.n_batches)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom generator and compare to standard generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the manuscript's purposes, we will choose the lowest levels as the residuals for direct comparison with the reference ACnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILEQ = '8col009_01_train.nc'\n",
    "VALIDFILEQ = '8col009_01_valid.nc'\n",
    "NORMFILEQ = '8col009_01_norm.nc'\n",
    "TESTFILEQ = '8col009_01_test.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dictQ = load_pickle('./nn_config/scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_varsQ = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_varsQ = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FSNS', 'FLNT', 'FLNS', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+VALIDFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genQ = DataGenerator(\n",
    "    data_fn = TRAINDIR+TESTFILEQ,\n",
    "    input_vars = in_varsQ,\n",
    "    output_vars = out_varsQ,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_custom = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_custom = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FLNT', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cut_off_low = {}\n",
    "out_cut_off_low['PHQ'] = 29\n",
    "out_cut_off_low['TPHYSTND'] = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PHQ': 29, 'TPHYSTND': 29}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cut_off_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+TRAINFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+VALIDFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_custom = DataGenerator_outputcutoff(\n",
    "    data_fn = TRAINDIR+TESTFILEQ,\n",
    "    input_vars = in_vars_custom,\n",
    "    output_vars = out_vars_custom,\n",
    "    norm_fn = TRAINDIR+NORMFILEQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dictQ,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    out_cut_off=out_cut_off_low\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "out = Dense(214, activation='linear')(densout)\n",
    "UCnet_214 = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'UCnet_214_1'\n",
    "path_HDF5 = '/local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCnet_214.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 6096/41376 [===>..........................] - ETA: 25:00 - loss: 295.2595"
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "UCnet_214.fit_generator(train_gen_custom, epochs=Nep, \n",
    "                        validation_data=valid_gen_custom,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360.319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
