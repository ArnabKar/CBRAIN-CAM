{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/8/2019 \n",
    "- This script aims at predicting convective heating/moistening, longwave cooling, shortwave heating and microphysical tendencies based on the 8 column SPCAM simulation, with the goal of testing generalization abilities of different network architectures.  \n",
    "- Notably, we would like to compare the generalization abilities of the conserving network (C), the unconstrained network (U) and the weakly-constrained network (W) with climate change (trained & validated on 0K and then statistics on the +4K simulation).  \n",
    "- Another aspect is the \"data greediness\" of each network. Do we need less data when we train with constraints?\n",
    "Notebook 009 follows the notebook 005 that predicts:\n",
    "***\n",
    "[PHQ, PHCLDLIQ, PHCLDICE, TPHYSTND, QRL, QRS, DTVKE, FSNT, FSNS, FLNT, FLNS, PRECT, PRECTEND, PRECST, PRECSTEN] as a function of:  \n",
    "[QBP, QCBP, QIBP, TBP, VBP, Qdt_adiabatic, QCdt_adiabatic, QIdt_adiabatic, Tdt_adiabatic, Vdt_adiabatic, PS, SOLIN, SHFLX, LHFLX] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preprocess all the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/Tom.Beucler/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM\n"
     ]
    }
   ],
   "source": [
    "#!ln -s /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/cbrain \\\n",
    "#/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain\n",
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "limit_mem()\n",
    "TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/'\n",
    "PREFIX = '8col009_01_'\n",
    "%cd /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/8/2019 - We change the pre-processing configuration file \"8col_rad_tbeucler_local.yml\" & because we now train on the 8 column data, we change the prefix to 8col009_01_train.  \n",
    "Follows the April 4th, 2019 commit of https://github.com/raspstephan/CBRAIN-CAM/blob/master/quickstart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python preprocessing.py -c pp_config/8col_rad_tbeucler_local.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/10/2019 - Process future climate data (+4K experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t/Tom.Beucler/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "04/11/2019 04:14:50 AM Preprocess training dataset\n",
      "04/11/2019 04:14:50 AM Start preprocessing file /local/Tom.Beucler/SPCAM_PHYS/8col009_02_train.nc\n",
      "04/11/2019 04:14:50 AM Reading input files\n",
      "04/11/2019 04:14:50 AM Crop levels\n",
      "04/11/2019 04:14:50 AM Create stacked dataarray\n",
      "These time steps are cut: []\n",
      "04/11/2019 04:14:51 AM Stack and reshape dataarray\n",
      "Traceback (most recent call last):\n",
      "  File \"preprocessing.py\", line 79, in <module>\n",
      "    main(args)\n",
      "  File \"preprocessing.py\", line 29, in main\n",
      "    preprocess(args.in_dir, args.in_fns, args.out_dir, args.out_fn, args.vars)\n",
      "  File \"/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/cbrain/preprocessing/convert_dataset.py\", line 191, in preprocess\n",
      "    da = reshape_da(da).reset_index('sample')\n",
      "  File \"/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/cbrain/preprocessing/convert_dataset.py\", line 165, in reshape_da\n",
      "    da = da.stack(sample=('time', 'lat', 'lon'))\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/xarray/core/dataarray.py\", line 1311, in stack\n",
      "    ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/xarray/core/dataset.py\", line 2405, in stack\n",
      "    result = result._stack_once(dims, new_dim)\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/xarray/core/dataset.py\", line 2355, in _stack_once\n",
      "    stacked_var = exp_var.stack(**{new_dim: dims})\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/xarray/core/variable.py\", line 1240, in stack\n",
      "    result = result._stack_once(dims, new_dim)\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/xarray/core/variable.py\", line 1205, in _stack_once\n",
      "    new_data = reordered.data.reshape(new_shape)\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/dask/array/core.py\", line 1411, in reshape\n",
      "    return reshape(self, shape)\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/dask/array/reshape.py\", line 184, in reshape\n",
      "    inchunks, outchunks = reshape_rechunk(x.shape, shape, x.chunks)\n",
      "  File \"/home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/dask/array/reshape.py\", line 61, in reshape_rechunk\n",
      "    raise ValueError(\"Shapes not compatible\")\n",
      "ValueError: Shapes not compatible\n"
     ]
    }
   ],
   "source": [
    "!python preprocessing.py -c pp_config/8col_rad_tbeucler_p4K.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/9/2019 - Follows method adapted by Stephan in his quickstart to define inputs/outputs/normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define inputs and outputs\n",
    "in_vars = ['QBP', 'QCBP', 'QIBP', 'TBP', 'VBP', \n",
    "           'Qdt_adiabatic', 'QCdt_adiabatic', 'QIdt_adiabatic', 'Tdt_adiabatic', 'Vdt_adiabatic',\n",
    "           'PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ', 'PHCLDLIQ', 'PHCLDICE', 'TPHYSTND', 'QRL', 'QRS', 'DTVKE', \n",
    "            'FSNT', 'FSNS', 'FLNT', 'FLNS', 'PRECT', 'PRECTEND', 'PRECST', 'PRECSTEN']\n",
    "\n",
    "# 2) Calculates pressure and pressure increments\n",
    "# Takes representative value for PS since purpose is normalization\n",
    "PS = 1e5; P0 = 1e5;\n",
    "P = P0*hyai+PS*hybi; # Total pressure [Pa]\n",
    "dP = P[1:]-P[:-1]; # Differential pressure [Pa]\n",
    "dt = 30*60; # timestep\n",
    "\n",
    "# 3) Define the scaling dictionary \n",
    "scale_dict = {\n",
    "    'PHQ': L_V*dP/G, \n",
    "    'PHCLDLIQ': L_V*dP/G, \n",
    "    'PHCLDICE': L_V*dP/G, \n",
    "    'TPHYSTND': C_P*dP/G, \n",
    "    'QRL': C_P*dP/G, \n",
    "    'QRS': C_P*dP/G, \n",
    "    'DTVKE': C_P*dP/(G*dt), \n",
    "    'FSNT': 1, \n",
    "    'FSNS': 1, \n",
    "    'FLNT': 1, \n",
    "    'FLNS': 1, \n",
    "    'PRECT': RHO_L*L_V, \n",
    "    'PRECTEND': 1e-3*RHO_L*L_V, \n",
    "    'PRECST': RHO_L*L_V, \n",
    "    'PRECSTEN': 1e-3*RHO_L*L_V\n",
    "}\n",
    "\n",
    "# 4) Save dictionary as pickle file for loading in teh training script\n",
    "save_pickle('./nn_config/scale_dicts/009_Wm2_scaling.pkl', scale_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create data generator and produce data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'train_shuffle.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 304), (1024, 218))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = train_gen[0]; X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'valid.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 304), (1024, 218))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = valid_gen[0]; X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Build neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Conserving Network (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpC = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inpC)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(214, activation='linear')(densout)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "surfout = SurRadLayer(\n",
    "    inp_div=train_gen.input_transform.div,\n",
    "    inp_sub=train_gen.input_transform.sub,\n",
    "    norm_q=scale_dict['PHQ'],\n",
    "    hyai=hyai, hybi=hybi\n",
    ")([inpC, densout])\n",
    "massout = MassConsLayer(\n",
    "    inp_div=train_gen.input_transform.div,\n",
    "    inp_sub=train_gen.input_transform.sub,\n",
    "    norm_q=scale_dict['PHQ'],\n",
    "    hyai=hyai, hybi=hybi\n",
    ")([inpC, surfout])\n",
    "enthout = EntConsLayer(\n",
    "    inp_div=train_gen.input_transform.div,\n",
    "    inp_sub=train_gen.input_transform.sub,\n",
    "    norm_q=scale_dict['PHQ'],\n",
    "    hyai=hyai, hybi=hybi\n",
    ")([inpC, massout])\n",
    "C_009 = tf.keras.models.Model(inpC, enthout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Unconstrained Network (U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconstrained model with 5 dense layers\n",
    "inpU = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inpU)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(218, activation='linear')(densout)\n",
    "out_layer = LeakyReLU(alpha=0.3)(densout)\n",
    "U_009 = tf.keras.models.Model(inpU, out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Weakly-constrained Network (W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weakly-constrained models with 5 dense layers\n",
    "# alpha=0.01\n",
    "inp001 = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp001)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(218, activation='linear')(densout)\n",
    "out = LeakyReLU(alpha=0.3)(densout)\n",
    "W001_009 = tf.keras.models.Model(inp001, out)\n",
    "# alpha=0.5\n",
    "inp05 = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp05)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(218, activation='linear')(densout)\n",
    "out = LeakyReLU(alpha=0.3)(densout)\n",
    "W05_009 = tf.keras.models.Model(inp05, out)\n",
    "# alpha=0.99\n",
    "inp099 = Input(shape=(304,))\n",
    "densout = Dense(512, activation='linear')(inp099)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (4):\n",
    "    densout = Dense(512, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "densout = Dense(218, activation='linear')(densout)\n",
    "out = LeakyReLU(alpha=0.3)(densout)\n",
    "W099_009 = tf.keras.models.Model(inp099, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Compile and train the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Define losses\n",
    "tgb - 4/10/2019 - Trying to solve the following error:  \n",
    "  \n",
    "You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,304]\n",
    "\t [[{{node input_2}} = Placeholder[dtype=DT_FLOAT, shape=[?,304], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
    "\t [[{{node ConstantFoldingCtrl/loss/ent_cons_layer_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0/_310}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_972_C...d/Switch_0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]  \n",
    "  \n",
    "when trying to train several networks in a row.\n",
    "tgb - 4/10/2019 - First, understand how to access normalization  \n",
    "  \n",
    "tgb - 4/10/2019 - Found the problem: The weak loss function tracks a given input layer, and that has to be different for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Open the file containing the normalization of the targets\n",
    "# ds = xr.open_dataset(TRAINDIR + PREFIX + 'norm.nc')\n",
    "# # 2) Open the pickle files containing the pressure converters\n",
    "# with open(os.path.join('/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/cbrain', 'hyai_hybi.pkl'), 'rb') as f:\n",
    "#             hyai, hybi = pickle.load(f)\n",
    "# print(ds)\n",
    "# # 3) Define input transform and save it\n",
    "# inp_transform = InputNormalizer(ds, in_vars, 'mean', 'maxrs')\n",
    "# print(inp_transform)\n",
    "# inp_div = inp_transform.div\n",
    "# inp_sub = inp_transform.sub\n",
    "# norm_q=scale_dict['PHQ']\n",
    "# print('inp_div.shape=',inp_div.shape)\n",
    "# print('inp_sub.shape=',inp_sub.shape)\n",
    "# print('hyai.shape=',hyai.shape)\n",
    "# print('hybi.shape=',hybi.shape)\n",
    "# print('norm_q.shape=',norm_q.shape)\n",
    "\n",
    "# ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W05_loss = WeakLoss(inp05, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                     norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, name='W05_loss')\n",
    "W001_loss = WeakLoss(inp001, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                     norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, alpha_mass=5e-3, alpha_ent=5e-3, name = 'W001_loss')\n",
    "W099_loss = WeakLoss(inp099, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                     norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, alpha_mass=0.99/2, alpha_ent=0.99/2, name = 'W099_loss')\n",
    "masC_loss = WeakLoss(inpC, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                      norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, alpha_mass=1, alpha_ent=0, name='masC_loss')\n",
    "entC_loss = WeakLoss(inpC, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                     norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, alpha_mass=0, alpha_ent=1, name='entC_loss')\n",
    "masU_loss = WeakLoss(inpU, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                      norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, alpha_mass=1, alpha_ent=0, name='masU_loss')\n",
    "entU_loss = WeakLoss(inpU, inp_div=train_gen.input_transform.div, inp_sub=train_gen.input_transform.sub,\n",
    "                     norm_q=scale_dict['PHQ'], hyai=hyai, hybi=hybi, alpha_mass=0, alpha_ent=1, name='entU_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Compile the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1) Use mean square error as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_009.compile(tf.keras.optimizers.RMSprop(), loss=mse, metrics=[mse, masC_loss, entC_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_009.compile(tf.keras.optimizers.RMSprop(), loss=mse, metrics=[mse, masU_loss, entU_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2) Use custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mass_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a5c687ce7ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW001_009\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW001_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmass_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW001_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mass_loss' is not defined"
     ]
    }
   ],
   "source": [
    "W001_009.compile(tf.keras.optimizers.RMSprop(), loss=W001_loss, metrics=[mass_loss, ent_loss, mse, W001_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W05_009.compile(tf.keras.optimizers.RMSprop(), loss=W05_loss, metrics=[mass_loss, ent_loss, mse, W05_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W099_009.compile(tf.keras.optimizers.RMSprop(), loss=W099_loss, metrics=[mass_loss, ent_loss, mse, W099_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1) Train compiled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nep = 10\n",
    "# hC_009 = C_009.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)\n",
    "# hU_009 = U_009.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2) Save models in .h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd $TRAINDIR/HDF5_DATA\n",
    "# !pwd\n",
    "# C_009.save('C_009.h5')\n",
    "# U_009.save('U_009.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W001_rad_5dens.save('W001_rad2_5dens.h5')\n",
    "# W05_rad_5dens.save('W05_rad2_5dens.h5')\n",
    "# W099_rad_5dens.save('W099_rad2_5dens.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3) Load models if already trained\n",
    "tgb - 2/11/2019 - OPTIONAL: Load the models if already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $TRAINDIR/HDF5_DATA\n",
    "!ls\n",
    "C_009.load_weights('C_009.h5')\n",
    "U_009.load_weights('U_009.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4) Loss and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for index in range (2):\n",
    "    if index==0: hdict = hC_009.history; colo = 'bo'; col = 'b'; lab = 'C';\n",
    "    elif index==1: hdict = hU_009.history; colo = 'ro'; col = 'r'; lab = 'U';\n",
    "    elif index==2: hdict = hW001_rad_5dens.history; colo = 'go'; col = 'g'; lab = 'W001';\n",
    "    elif index==3: hdict = hW05_rad_5dens.history; colo = 'co'; col = 'c'; lab = 'W05';\n",
    "    elif index==4: hdict = hW099_rad_5dens.history; colo = 'mo'; col = 'm'; lab = 'W099';\n",
    "        \n",
    "    \n",
    "    train_loss_values = hdict['loss']\n",
    "    valid_loss_values = hdict['val_loss']\n",
    "    epochs = range(1, len(train_loss_values) + 1)\n",
    "\n",
    "    ax.plot(epochs, train_loss_values, colo, label=lab+' Train')\n",
    "    ax.plot(epochs, valid_loss_values, col, label=lab+' Valid')\n",
    "\n",
    "#plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim((0, 125))\n",
    "# https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot\n",
    "# for legend at the right place\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=5, fancybox=True, shadow=True); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Compare R2 plot in present and future climate\n",
    "tgb - 4/10/2019 - Use the new quickstart from Stephan at https://github.com/raspstephan/CBRAIN-CAM/blob/master/quickstart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbrain.model_diagnostics.model_diagnostics import ModelDiagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno -101] NetCDF: HDF error: b'/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: [<function _open_netcdf4_group at 0x149ca259f268>, ('/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc', CombinedLock([<SerializableLock: fec7c7db-4d8e-4630-a51f-a2d5b7d39942>, <SerializableLock: adb23ca1-8b05-4dd0-9c57-66a541e81df3>])), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('group', None), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e145f6b1566b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelDiagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_009\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/model_diagnostics/model_diagnostics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config_fn, data_fn, nlat, nlon, nlev, ntime)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngeo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mxarray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/data_generator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_fn, input_vars, output_vars, norm_fn, input_transform, output_transform, batch_size, shuffle, xarray)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Open datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'netcdf4'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             store = backends.NetCDF4DataStore.open(\n\u001b[0;32m--> 320\u001b[0;31m                 filename_or_obj, group=group, lock=lock, **backend_kwargs)\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScipyDataStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbackend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    353\u001b[0m             kwargs=dict(group=group, clobber=clobber, diskless=diskless,\n\u001b[1;32m    354\u001b[0m                         persist=persist, format=format))\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, lock, autoclose)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0;31m# ensure file doesn't get overriden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_open_netcdf4_group\u001b[0;34m(filename, lock, mode, group, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -101] NetCDF: HDF error: b'/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'"
     ]
    }
   ],
   "source": [
    "config_fn = '/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\n",
    "data_fn = '/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\n",
    "md = ModelDiagnostics(C_009,config_fn,data_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(config_fn, 'r') as f:\n",
    "            config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vars': ['QBP',\n",
       "  'QCBP',\n",
       "  'QIBP',\n",
       "  'TBP',\n",
       "  'VBP',\n",
       "  'PS',\n",
       "  'SOLIN',\n",
       "  'SHFLX',\n",
       "  'LHFLX',\n",
       "  'PHQ',\n",
       "  'PHCLDLIQ',\n",
       "  'PHCLDICE',\n",
       "  'TPHYSTND',\n",
       "  'QRL',\n",
       "  'QRS',\n",
       "  'DTVKE',\n",
       "  'FSNT',\n",
       "  'FSNS',\n",
       "  'FLNT',\n",
       "  'FLNS',\n",
       "  'PRECT',\n",
       "  'PRECTEND',\n",
       "  'PRECST',\n",
       "  'PRECSTEN',\n",
       "  'Qdt_adiabatic',\n",
       "  'QCdt_adiabatic',\n",
       "  'QIdt_adiabatic',\n",
       "  'Tdt_adiabatic',\n",
       "  'Vdt_adiabatic'],\n",
       " 'in_dir': '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/',\n",
       " 'in_fns': 'AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-*-0*-00000.nc',\n",
       " 'out_dir': '/local/Tom.Beucler/SPCAM_PHYS/',\n",
       " 'out_fn': '8col009_01_train.nc',\n",
       " 'val_in_fns': 'AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0001-*-0*-00000.nc',\n",
       " 'val_out_fn': '8col009_01_valid.nc',\n",
       " 'norm_fn': '8col009_01_norm.nc'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
