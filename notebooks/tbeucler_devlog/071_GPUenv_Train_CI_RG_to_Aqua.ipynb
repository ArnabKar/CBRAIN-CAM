{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 6/3/2021 - Repeat for additional training from RG to aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/27/2021 - Recycling notebook [065] for additional training from warm to cold climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2021 - Recycling this notebook but fitting in percentile space (no scale_dict, use output in percentile units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2020\n",
    "- Adapting Ankitesh's notebook that builds and train a \"brute-force\" network to David Walling's hyperparameter search  \n",
    "- Adding the option to choose between aquaplanet and real-geography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfspool-0/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "from cbrain.climate_invariant import *\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[2], True)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "\n",
    "# Comet path below\n",
    "# coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "#                     decode_times=False)\n",
    "\n",
    "# GP path below\n",
    "path_0K = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/fluxbypass_aqua/'\n",
    "coor = xr.open_dataset(path_0K+\"AndKua_aqua_SPCAM3.0_sp_fbp_f4.cam2.h1.0000-09-02-00000.nc\")\n",
    "\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet path below\n",
    "# TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "# path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "\n",
    "# GP path below\n",
    "TRAINDIR = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "path = '/export/nfs0home/tbeucler/CBRAIN-CAM/cbrain/'\n",
    "path_nnconfig = '/export/nfs0home/tbeucler/CBRAIN-CAM/nn_config/'\n",
    "\n",
    "# Load hyam and hybm to calculate pressure field in SPCAM\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "\n",
    "# Scale dictionary to convert the loss to W/m2\n",
    "scale_dict = load_pickle(path_nnconfig+'scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data generator class for the climate-invariant network. Calculates the physical rescalings needed to make the NN climate-invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP paths below\n",
    "#path_aquaplanet = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/SPCAM_PHYS/'\n",
    "#path_realgeography = ''\n",
    "\n",
    "# GP /fast paths below\n",
    "path_aquaplanet = '/fast/tbeucler/climate_invariant/aquaplanet/'\n",
    "\n",
    "# Comet paths below\n",
    "# path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "# path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography/'\n",
    "\n",
    "path = path_aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling_2.pkl')\n",
    "scale_dict_RH = scale_dict.copy()\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','QRL','QRS']\n",
    "\n",
    "# New GP path below\n",
    "TRAINFILE_RH = '2021_01_24_O3_small_shuffle.nc'\n",
    "NORMFILE_RH = '2021_02_01_NORM_O3_RH_small.nc'\n",
    "    \n",
    "# Comet/Ankitesh path below\n",
    "# TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "# NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "# VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using QSATdeficit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the norm file for this generator as we are solely using it as an input to determine the right normalization for the combined generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New GP path below\n",
    "TRAINFILE_QSATdeficit = '2021_02_01_O3_QSATdeficit_small_shuffle.nc'\n",
    "NORMFILE_QSATdeficit = '2021_02_01_NORM_O3_QSATdeficit_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars_QSATdeficit = ['QSATdeficit','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "# if path==path_realgeography: out_vars_RH = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "# elif path==path_aquaplanet: out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "if path==path_aquaplanet: out_vars_QSATdeficit = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_QSATdeficit = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_QSATdeficit,\n",
    "    input_vars = in_vars_QSATdeficit,\n",
    "    output_vars = out_vars_QSATdeficit,\n",
    "    norm_fn = path+NORMFILE_QSATdeficit,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_TNS = '2021_02_01_O3_TfromNS_small_shuffle.nc'\n",
    "NORMFILE_TNS = '2021_02_01_NORM_O3_TfromNS_small.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using BCONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','BCONS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_BCONS = '2021_02_01_O3_BCONS_small_shuffle.nc'\n",
    "NORMFILE_BCONS = '2021_02_01_NORM_O3_BCONS_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BCONS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_BCONS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_BCONS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using BMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','BMSE','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_BMSE = '2021_06_16_BMSE_small_shuffle.nc'\n",
    "NORMFILE_BMSE = '2021_06_16_NORM_BMSE_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_BMSE = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_BMSE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_BMSE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','T_NSto220','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_T_NSto220 = '2021_03_31_O3_T_NSto220_small.nc'\n",
    "NORMFILE_T_NSto220 = '2021_03_31_NORM_O3_T_NSto220_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_T_NSto220 = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_T_NSto220,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_T_NSto220,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsDELQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsDELQ = '2021_02_01_O3_LHF_nsDELQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsDELQ = '2021_02_01_NORM_O3_LHF_nsDELQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsDELQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsDELQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsDELQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHF_nsQ']\n",
    "if path==path_aquaplanet: out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "elif path==path_realgeography: out_vars = ['PTEQ','PTTEND','FSNT','FSNS','FLNT','FLNS']\n",
    "\n",
    "TRAINFILE_LHF_nsQ = '2021_02_01_O3_LHF_nsQ_small_shuffle.nc'\n",
    "NORMFILE_LHF_nsQ = '2021_02_01_NORM_O3_LHF_nsQ_small.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_LHF_nsQ = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_LHF_nsQ,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_LHF_nsQ,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=8192,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Combined (latest flexible version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "#if path==path_aquaplanet: out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In physical space\n",
    "# TRAINFILE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "# VALIDFILE = '2021_03_18_O3_VALID_M4K.nc'\n",
    "# TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "# TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_M4K_shuffle.nc'\n",
    "# TRAINFILE = '2021_03_18_O3_TRAIN_P4K_shuffle.nc'\n",
    "# VALIDFILE = '2021_03_18_O3_VALID_P4K.nc'\n",
    "# TESTFILE_DIFFCLIMATE = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'\n",
    "# TESTFILE_DIFFGEOG = '2021_04_18_RG_TRAIN_P4K_shuffle.nc'\n",
    "\n",
    "# In percentile space\n",
    "#TRAINFILE = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'\n",
    "#TRAINFILE = '2021_01_24_O3_small_shuffle.nc'\n",
    "#VALIDFILE = '2021_04_09_PERC_VALID_M4K.nc'\n",
    "#TESTFILE = '2021_04_09_PERC_TEST_P4K.nc'\n",
    "\n",
    "# Change to real-geography\n",
    "TRAINFILE = '2021_04_18_RG_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_18_RG_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_18_RG_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old data generator by Ankitesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved flexible data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add callback class to track loss on multiple sets during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [https://stackoverflow.com/questions/47731935/using-multiple-validation-sets-with-keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_diffgeog_gen_CI[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_gen_CI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-67e1c513bfda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gen_CI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_gen_CI' is not defined"
     ]
    }
   ],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(test_gen_CI[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            valid_generator,valid_name = validation_set\n",
    "            #tf.print('Results')\n",
    "            results = self.model.evaluate_generator(generator=valid_generator)\n",
    "            #tf.print(results)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,[results]):\n",
    "                #tf.print(metric,result)\n",
    "                valuename = valid_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models tracking losses across climates/geography (Warm to Cold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR or Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_MLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17475/17475 [==============================] - 2128s 122ms/step - loss: 223.2982 - val_loss: 209.1084\n",
      "Epoch 2/20\n",
      "17475/17475 [==============================] - 1993s 114ms/step - loss: 206.1793 - val_loss: 202.5698\n",
      "Epoch 3/20\n",
      "17475/17475 [==============================] - 1962s 112ms/step - loss: 202.0670 - val_loss: 200.2679\n",
      "Epoch 4/20\n",
      "17475/17475 [==============================] - 2004s 115ms/step - loss: 200.4173 - val_loss: 199.2042\n",
      "Epoch 5/20\n",
      "17475/17475 [==============================] - 1795s 103ms/step - loss: 199.5865 - val_loss: 198.6079\n",
      "Epoch 6/20\n",
      "17475/17475 [==============================] - 1834s 105ms/step - loss: 199.0867 - val_loss: 198.2228\n",
      "Epoch 7/20\n",
      "17475/17475 [==============================] - 1756s 101ms/step - loss: 198.7466 - val_loss: 197.9413\n",
      "Epoch 8/20\n",
      "17475/17475 [==============================] - 1764s 101ms/step - loss: 198.4952 - val_loss: 197.7299\n",
      "Epoch 9/20\n",
      "17475/17475 [==============================] - 1742s 100ms/step - loss: 198.2982 - val_loss: 197.5603\n",
      "Epoch 10/20\n",
      "17475/17475 [==============================] - 1714s 98ms/step - loss: 198.1368 - val_loss: 197.4183\n",
      "Epoch 11/20\n",
      "17475/17475 [==============================] - 1691s 97ms/step - loss: 198.0011 - val_loss: 197.2978\n",
      "Epoch 12/20\n",
      "17475/17475 [==============================] - 1730s 99ms/step - loss: 197.8842 - val_loss: 197.1907\n",
      "Epoch 13/20\n",
      "17475/17475 [==============================] - 1735s 99ms/step - loss: 197.7817 - val_loss: 197.0969\n",
      "Epoch 14/20\n",
      "17475/17475 [==============================] - 1768s 101ms/step - loss: 197.6906 - val_loss: 197.0133\n",
      "Epoch 15/20\n",
      "17475/17475 [==============================] - 1642s 94ms/step - loss: 197.6087 - val_loss: 196.9359\n",
      "Epoch 16/20\n",
      "17475/17475 [==============================] - 1689s 97ms/step - loss: 197.5346 - val_loss: 196.8692\n",
      "Epoch 17/20\n",
      "17475/17475 [==============================] - 1679s 96ms/step - loss: 197.4667 - val_loss: 196.8039\n",
      "Epoch 18/20\n",
      "17475/17475 [==============================] - 1748s 100ms/step - loss: 197.4043 - val_loss: 196.7457\n",
      "Epoch 19/20\n",
      "17475/17475 [==============================] - 1683s 96ms/step - loss: 197.3468 - val_loss: 196.6939\n",
      "Epoch 20/20\n",
      "17475/17475 [==============================] - 1745s 100ms/step - loss: 197.2935 - val_loss: 196.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd76032c438>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [223.29819108212627,\n",
       "  206.17931607094957,\n",
       "  202.06698953478462,\n",
       "  200.41729842331958,\n",
       "  199.5864607345734,\n",
       "  199.0867494526919,\n",
       "  198.7466087007318,\n",
       "  198.4952149397996,\n",
       "  198.2981907031669,\n",
       "  198.13680471963295,\n",
       "  198.00114111467835,\n",
       "  197.88416899892565,\n",
       "  197.78171910538353,\n",
       "  197.6905891734984,\n",
       "  197.60869615808576,\n",
       "  197.53455407749772,\n",
       "  197.46667948579582,\n",
       "  197.40427822653317,\n",
       "  197.34682512755387,\n",
       "  197.29348995530725],\n",
       " 'val_loss': [209.10836235297427,\n",
       "  202.5698359221757,\n",
       "  200.26789018753772,\n",
       "  199.20420995709551,\n",
       "  198.60789161147306,\n",
       "  198.2228022414659,\n",
       "  197.94130699648878,\n",
       "  197.7298761761274,\n",
       "  197.56025337557594,\n",
       "  197.41827944819678,\n",
       "  197.29783793022364,\n",
       "  197.19073267313203,\n",
       "  197.0968571834537,\n",
       "  197.01333940519626,\n",
       "  196.93592210914272,\n",
       "  196.86923351151407,\n",
       "  196.80388644042444,\n",
       "  196.74574342937768,\n",
       "  196.69393973791207,\n",
       "  196.64264546384797],\n",
       " 'trainM4K_loss': [508.3030777732838,\n",
       "  541.4444600161223,\n",
       "  569.1960853255767,\n",
       "  589.0296021555627,\n",
       "  600.9225598458876,\n",
       "  607.7910872605396,\n",
       "  612.7966834054928,\n",
       "  615.5331468416795,\n",
       "  617.7945896953097,\n",
       "  617.8358476359923,\n",
       "  618.6470007411536,\n",
       "  619.93780263134,\n",
       "  620.2214828539236,\n",
       "  619.5565992712804,\n",
       "  620.1751068001721,\n",
       "  619.4726823719991,\n",
       "  620.6530673737315,\n",
       "  620.5418869101507,\n",
       "  620.8600531084445,\n",
       "  618.6187442090821],\n",
       " 'trainP4K_RG_loss': [315.1275108034028,\n",
       "  306.02964993354493,\n",
       "  302.7924061964154,\n",
       "  301.36354976918346,\n",
       "  300.6994297976791,\n",
       "  300.32674739550833,\n",
       "  300.02999172618064,\n",
       "  299.8727835356972,\n",
       "  299.7227831531014,\n",
       "  299.70644898902293,\n",
       "  299.63537130804605,\n",
       "  299.5363968168412,\n",
       "  299.5081463121751,\n",
       "  299.51246971611266,\n",
       "  299.4845111606477,\n",
       "  299.50079840110146,\n",
       "  299.37336879067175,\n",
       "  299.3525838770786,\n",
       "  299.30723076045547,\n",
       "  299.3978516859693]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_MLR_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17475/17475 [==============================] - 4801s 275ms/step - loss: 218.2582 - val_loss: 206.6561\n",
      "Epoch 2/20\n",
      "17475/17475 [==============================] - 4839s 277ms/step - loss: 205.4828 - val_loss: 203.2764\n",
      "Epoch 3/20\n",
      "17475/17475 [==============================] - 5009s 287ms/step - loss: 203.6851 - val_loss: 202.4985\n",
      "Epoch 4/20\n",
      "17475/17475 [==============================] - 5029s 288ms/step - loss: 203.1824 - val_loss: 202.2358\n",
      "Epoch 5/20\n",
      "17475/17475 [==============================] - 4948s 283ms/step - loss: 202.9802 - val_loss: 202.1054\n",
      "Epoch 6/20\n",
      "17475/17475 [==============================] - 5029s 288ms/step - loss: 202.8587 - val_loss: 202.0129\n",
      "Epoch 7/20\n",
      "17475/17475 [==============================] - 5037s 288ms/step - loss: 202.7717 - val_loss: 201.9339\n",
      "Epoch 8/20\n",
      "17475/17475 [==============================] - 5083s 291ms/step - loss: 202.7050 - val_loss: 201.8816\n",
      "Epoch 9/20\n",
      "17475/17475 [==============================] - 4938s 283ms/step - loss: 202.6513 - val_loss: 201.8366\n",
      "Epoch 10/20\n",
      "17475/17475 [==============================] - 4902s 281ms/step - loss: 202.6071 - val_loss: 201.7962\n",
      "Epoch 11/20\n",
      "17475/17475 [==============================] - 4810s 275ms/step - loss: 202.5698 - val_loss: 201.7614\n",
      "Epoch 12/20\n",
      "17475/17475 [==============================] - 4834s 277ms/step - loss: 202.5376 - val_loss: 201.7326\n",
      "Epoch 13/20\n",
      "17475/17475 [==============================] - 4669s 267ms/step - loss: 202.5095 - val_loss: 201.7058\n",
      "Epoch 14/20\n",
      "17475/17475 [==============================] - 4829s 276ms/step - loss: 202.4849 - val_loss: 201.6850\n",
      "Epoch 15/20\n",
      "17475/17475 [==============================] - 5189s 297ms/step - loss: 202.4628 - val_loss: 201.6643\n",
      "Epoch 16/20\n",
      "17475/17475 [==============================] - 5033s 288ms/step - loss: 202.4431 - val_loss: 201.6478\n",
      "Epoch 17/20\n",
      "17475/17475 [==============================] - 5036s 288ms/step - loss: 202.4255 - val_loss: 201.6315\n",
      "Epoch 18/20\n",
      "17475/17475 [==============================] - 5408s 309ms/step - loss: 202.4093 - val_loss: 201.6172\n",
      "Epoch 19/20\n",
      "17475/17475 [==============================] - 4854s 278ms/step - loss: 202.3944 - val_loss: 201.6063\n",
      "Epoch 20/20\n",
      "17475/17475 [==============================] - 5241s 300ms/step - loss: 202.3808 - val_loss: 201.5890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd760189780>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [218.2581651400019,\n",
       "  205.48280193105106,\n",
       "  203.68514866033507,\n",
       "  203.18237631343464,\n",
       "  202.98020391941753,\n",
       "  202.85874441864493,\n",
       "  202.77167292560802,\n",
       "  202.70498869321548,\n",
       "  202.65131649339318,\n",
       "  202.60713455483977,\n",
       "  202.56977194331745,\n",
       "  202.53760723676123,\n",
       "  202.5094735172037,\n",
       "  202.4848617636663,\n",
       "  202.4628487063341,\n",
       "  202.44313306554704,\n",
       "  202.42550102615903,\n",
       "  202.4092505904431,\n",
       "  202.39442062618053,\n",
       "  202.3807780253529],\n",
       " 'val_loss': [206.65605477831053,\n",
       "  203.27636524336873,\n",
       "  202.49851598343966,\n",
       "  202.23578283896603,\n",
       "  202.10541704060523,\n",
       "  202.01288744313862,\n",
       "  201.93390759076513,\n",
       "  201.88157322350148,\n",
       "  201.83661594047055,\n",
       "  201.7962018164988,\n",
       "  201.7614174140199,\n",
       "  201.7325615101652,\n",
       "  201.70581223865776,\n",
       "  201.68497617162177,\n",
       "  201.6642625505832,\n",
       "  201.64784240875463,\n",
       "  201.63148081221465,\n",
       "  201.61716875657504,\n",
       "  201.60630836585048,\n",
       "  201.58895601439033],\n",
       " 'trainM4K_loss': [467.2247095634668,\n",
       "  461.3193834436332,\n",
       "  459.8182357067714,\n",
       "  459.313672507181,\n",
       "  459.00253475947784,\n",
       "  458.8507153058359,\n",
       "  458.70949816353163,\n",
       "  458.6026163317854,\n",
       "  458.54861833345905,\n",
       "  458.463548910553,\n",
       "  458.4174236824789,\n",
       "  458.4175588137909,\n",
       "  458.35010420334015,\n",
       "  458.3178175068038,\n",
       "  458.2645447528277,\n",
       "  458.2526146255952,\n",
       "  458.2127605755851,\n",
       "  458.2257696018028,\n",
       "  458.15663517462167,\n",
       "  458.1972447490556],\n",
       " 'trainP4K_RG_loss': [312.7888750810552,\n",
       "  309.21135521829285,\n",
       "  308.45019160312256,\n",
       "  308.15041267989847,\n",
       "  307.92074368227316,\n",
       "  307.8104717124148,\n",
       "  307.81804119520126,\n",
       "  307.7790652296779,\n",
       "  307.7624402663053,\n",
       "  307.7749713222731,\n",
       "  307.7559493433003,\n",
       "  307.8304557396236,\n",
       "  307.87240666824823,\n",
       "  307.87027710935973,\n",
       "  307.9295333223762,\n",
       "  307.9384225857591,\n",
       "  307.96438895383005,\n",
       "  308.04519066708764,\n",
       "  308.0609583986325,\n",
       "  308.13484315098856]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17475/17475 [==============================] - 18421s 1s/step - loss: 218.0461 - val_loss: 207.1461\n",
      "Epoch 2/20\n",
      "17475/17475 [==============================] - 18188s 1s/step - loss: 205.9641 - val_loss: 203.6876\n",
      "Epoch 3/20\n",
      "17475/17475 [==============================] - 18256s 1s/step - loss: 204.0529 - val_loss: 202.8154\n",
      "Epoch 4/20\n",
      "17475/17475 [==============================] - 18751s 1s/step - loss: 203.4758 - val_loss: 202.5104\n",
      "Epoch 5/20\n",
      "17475/17475 [==============================] - 18296s 1s/step - loss: 203.2369 - val_loss: 202.3486\n",
      "Epoch 6/20\n",
      "17475/17475 [==============================] - 21564s 1s/step - loss: 203.0920 - val_loss: 202.2289\n",
      "Epoch 7/20\n",
      "17475/17475 [==============================] - 22303s 1s/step - loss: 202.9877 - val_loss: 202.1445\n",
      "Epoch 8/20\n",
      "17475/17475 [==============================] - 23600s 1s/step - loss: 202.9073 - val_loss: 202.0769\n",
      "Epoch 9/20\n",
      "17475/17475 [==============================] - 19634s 1s/step - loss: 202.8423 - val_loss: 202.0198\n",
      "Epoch 10/20\n",
      " 3037/17475 [====>.........................] - ETA: 1:18:11 - loss: 202.8107"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BMSE'\n",
    "train_gen_T = train_gen_BMSE\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_17_RG2AQ_MLR_RH_BMSE_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17475/17475 [==============================] - 7823s 448ms/step - loss: 218.6248 - val_loss: 207.5182\n",
      "Epoch 2/20\n",
      "17475/17475 [==============================] - 7842s 449ms/step - loss: 206.4347 - val_loss: 204.1745\n",
      "Epoch 3/20\n",
      "17475/17475 [==============================] - 7848s 449ms/step - loss: 204.5239 - val_loss: 203.2511\n",
      "Epoch 4/20\n",
      "17475/17475 [==============================] - 7876s 451ms/step - loss: 203.9010 - val_loss: 202.9040\n",
      "Epoch 5/20\n",
      "17475/17475 [==============================] - 8182s 468ms/step - loss: 203.6362 - val_loss: 202.7189\n",
      "Epoch 6/20\n",
      "17475/17475 [==============================] - 7877s 451ms/step - loss: 203.4783 - val_loss: 202.5984\n",
      "Epoch 7/20\n",
      "17475/17475 [==============================] - 7652s 438ms/step - loss: 203.3663 - val_loss: 202.5089\n",
      "Epoch 8/20\n",
      "17475/17475 [==============================] - 7738s 443ms/step - loss: 203.2803 - val_loss: 202.4378\n",
      "Epoch 9/20\n",
      "17475/17475 [==============================] - 8199s 469ms/step - loss: 203.2112 - val_loss: 202.3782\n",
      "Epoch 10/20\n",
      "17475/17475 [==============================] - 7898s 452ms/step - loss: 203.1539 - val_loss: 202.3236\n",
      "Epoch 11/20\n",
      "17475/17475 [==============================] - 8183s 468ms/step - loss: 203.1050 - val_loss: 202.2811\n",
      "Epoch 12/20\n",
      "17475/17475 [==============================] - 7966s 456ms/step - loss: 203.0627 - val_loss: 202.2451\n",
      "Epoch 13/20\n",
      "17475/17475 [==============================] - 7827s 448ms/step - loss: 203.0251 - val_loss: 202.2103\n",
      "Epoch 14/20\n",
      "17475/17475 [==============================] - 8691s 497ms/step - loss: 202.9916 - val_loss: 202.1824\n",
      "Epoch 15/20\n",
      "17475/17475 [==============================] - 7914s 453ms/step - loss: 202.9616 - val_loss: 202.1552\n",
      "Epoch 16/20\n",
      "17475/17475 [==============================] - 8123s 465ms/step - loss: 202.9341 - val_loss: 202.1260\n",
      "Epoch 17/20\n",
      "17475/17475 [==============================] - 7927s 454ms/step - loss: 202.9089 - val_loss: 202.1056\n",
      "Epoch 18/20\n",
      "17475/17475 [==============================] - 8437s 483ms/step - loss: 202.8859 - val_loss: 202.0788\n",
      "Epoch 19/20\n",
      "17475/17475 [==============================] - 7892s 452ms/step - loss: 202.8644 - val_loss: 202.0592\n",
      "Epoch 20/20\n",
      "17475/17475 [==============================] - 7846s 449ms/step - loss: 202.8444 - val_loss: 202.0406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80105457f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [218.62480420463245,\n",
       "  206.4347062977803,\n",
       "  204.52392505039984,\n",
       "  203.9010091800717,\n",
       "  203.6362360806663,\n",
       "  203.4782627237781,\n",
       "  203.36631828864756,\n",
       "  203.28025687330953,\n",
       "  203.2112263901346,\n",
       "  203.1539153904294,\n",
       "  203.10499415494513,\n",
       "  203.06270979154093,\n",
       "  203.02506522903116,\n",
       "  202.9916461600766,\n",
       "  202.96155006768876,\n",
       "  202.9340913925935,\n",
       "  202.90893200177152,\n",
       "  202.8859141948738,\n",
       "  202.86438482063522,\n",
       "  202.8444212723324],\n",
       " 'val_loss': [207.51815063432903,\n",
       "  204.17448116209715,\n",
       "  203.25114826142362,\n",
       "  202.90400858735833,\n",
       "  202.7188857770273,\n",
       "  202.59835368035007,\n",
       "  202.50888405424672,\n",
       "  202.43775859368887,\n",
       "  202.37821883456732,\n",
       "  202.32363199738816,\n",
       "  202.28110546455875,\n",
       "  202.2450924809228,\n",
       "  202.21026459285972,\n",
       "  202.1824489214765,\n",
       "  202.15515127353913,\n",
       "  202.12603882599967,\n",
       "  202.1055721578748,\n",
       "  202.07881618952717,\n",
       "  202.0591738114473,\n",
       "  202.0405952060683],\n",
       " 'trainM4K_loss': [468.98039655213364,\n",
       "  462.9665679154512,\n",
       "  461.0744581021157,\n",
       "  460.3280278763887,\n",
       "  459.9907705622988,\n",
       "  459.776495879996,\n",
       "  459.58277856654877,\n",
       "  459.4774312197531,\n",
       "  459.36795686604466,\n",
       "  459.29758016649066,\n",
       "  459.1984890926072,\n",
       "  459.1248694301709,\n",
       "  459.06707542583155,\n",
       "  459.0224439349741,\n",
       "  458.9524345465621,\n",
       "  458.8930531933175,\n",
       "  458.8639636527349,\n",
       "  458.84298187037564,\n",
       "  458.82086892582316,\n",
       "  458.778331641114],\n",
       " 'trainP4K_RG_loss': [313.0756036937599,\n",
       "  309.2738399992471,\n",
       "  308.1123182373259,\n",
       "  307.6823615954969,\n",
       "  307.54469768022744,\n",
       "  307.4534770146201,\n",
       "  307.43228749803296,\n",
       "  307.49399116358467,\n",
       "  307.48966432847294,\n",
       "  307.58718663285356,\n",
       "  307.61598240275583,\n",
       "  307.6516464718267,\n",
       "  307.73495590970384,\n",
       "  307.81924709791025,\n",
       "  307.8523666612347,\n",
       "  307.930086353459,\n",
       "  308.0240071702902,\n",
       "  308.1196157061654,\n",
       "  308.21666690901264,\n",
       "  308.26398447483524]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In percentile space\n",
    "TRAINFILE = '2021_04_24_RG_PERC_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_24_RG_PERC_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_24_RG_PERC_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_LOGI_PERC_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(valid_gen_CI[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_LOGI_PERC_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "#if path==path_aquaplanet: out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']\n",
    "out_vars = ['PHQ','TPHYSTND','QRL','QRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In physical space\n",
    "TRAINFILE = '2021_04_18_RG_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_18_RG_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_18_RG_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_03_18_O3_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling=None,\n",
    "                               Tscaling=None,\n",
    "                               LHFscaling=None,\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=None,\n",
    "                               inp_div_Qscaling=None,\n",
    "                               inp_sub_Tscaling=None,\n",
    "                               inp_div_Tscaling=None,\n",
    "                               inp_sub_LHFscaling=None,\n",
    "                               inp_div_LHFscaling=None,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling=None,\n",
    "                                       Tscaling=None,\n",
    "                                       LHFscaling=None,\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=None,\n",
    "                                       inp_div_Qscaling=None,\n",
    "                                       inp_sub_Tscaling=None,\n",
    "                                       inp_div_Tscaling=None,\n",
    "                                       inp_sub_LHFscaling=None,\n",
    "                                       inp_div_LHFscaling=None,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_NN_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_27_RG2AQ_NN_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Rescaling (T=BMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BMSE'\n",
    "train_gen_T = train_gen_BMSE\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=scale_dict,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=scale_dict,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_17_RG2AQ_NN_RH_BMSE_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainM4K'),(test_diffgeog_gen_CI,'trainP4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17475/17475 [==============================] - 8122s 465ms/step - loss: 134.9978 - val_loss: 126.3949\n",
      "Epoch 2/20\n",
      "17475/17475 [==============================] - 8445s 483ms/step - loss: 122.8299 - val_loss: 121.8215\n",
      "Epoch 3/20\n",
      "17475/17475 [==============================] - 8157s 467ms/step - loss: 119.6971 - val_loss: 120.2678\n",
      "Epoch 4/20\n",
      "17475/17475 [==============================] - 7998s 458ms/step - loss: 117.9063 - val_loss: 119.7701\n",
      "Epoch 5/20\n",
      "17475/17475 [==============================] - 8100s 463ms/step - loss: 116.7361 - val_loss: 117.4521\n",
      "Epoch 6/20\n",
      "17475/17475 [==============================] - 8411s 481ms/step - loss: 115.8742 - val_loss: 117.1238\n",
      "Epoch 7/20\n",
      "17475/17475 [==============================] - 8116s 464ms/step - loss: 115.2347 - val_loss: 117.2021\n",
      "Epoch 8/20\n",
      "17475/17475 [==============================] - 8341s 477ms/step - loss: 114.6971 - val_loss: 115.9422\n",
      "Epoch 9/20\n",
      "17475/17475 [==============================] - 8180s 468ms/step - loss: 114.2607 - val_loss: 115.4521\n",
      "Epoch 10/20\n",
      "17475/17475 [==============================] - 7903s 452ms/step - loss: 113.8906 - val_loss: 117.3917\n",
      "Epoch 11/20\n",
      "17475/17475 [==============================] - 8347s 478ms/step - loss: 113.5708 - val_loss: 115.3285\n",
      "Epoch 12/20\n",
      "17475/17475 [==============================] - 8316s 476ms/step - loss: 113.2750 - val_loss: 115.5554\n",
      "Epoch 13/20\n",
      "17475/17475 [==============================] - 8533s 488ms/step - loss: 113.0305 - val_loss: 114.9608\n",
      "Epoch 14/20\n",
      "17475/17475 [==============================] - 8135s 466ms/step - loss: 112.8041 - val_loss: 114.7127\n",
      "Epoch 15/20\n",
      "17475/17475 [==============================] - 8728s 499ms/step - loss: 112.6197 - val_loss: 114.4643\n",
      "Epoch 16/20\n",
      "17475/17475 [==============================] - 8880s 508ms/step - loss: 112.4299 - val_loss: 114.5193\n",
      "Epoch 17/20\n",
      "17475/17475 [==============================] - 8116s 464ms/step - loss: 112.2541 - val_loss: 114.3922\n",
      "Epoch 18/20\n",
      "17475/17475 [==============================] - 8505s 487ms/step - loss: 112.1061 - val_loss: 114.4916\n",
      "Epoch 19/20\n",
      "17475/17475 [==============================] - 8261s 473ms/step - loss: 111.9497 - val_loss: 114.3208\n",
      "Epoch 20/20\n",
      "17475/17475 [==============================] - 8123s 465ms/step - loss: 111.8236 - val_loss: 113.8437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80104ce128>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [134.99783191686365,\n",
       "  122.82994899984422,\n",
       "  119.69707677620163,\n",
       "  117.90625671866967,\n",
       "  116.73612362011649,\n",
       "  115.8742336547153,\n",
       "  115.23471641267659,\n",
       "  114.69714245579273,\n",
       "  114.26067125486884,\n",
       "  113.89064075650063,\n",
       "  113.57080129074949,\n",
       "  113.27497019930118,\n",
       "  113.03048709809353,\n",
       "  112.80413435820005,\n",
       "  112.61971642970357,\n",
       "  112.42988497230628,\n",
       "  112.25405974686913,\n",
       "  112.10605054426944,\n",
       "  111.9497353667558,\n",
       "  111.82356473254202],\n",
       " 'val_loss': [126.3948931279544,\n",
       "  121.82149847022454,\n",
       "  120.26777086555362,\n",
       "  119.770068317135,\n",
       "  117.45208934500153,\n",
       "  117.12383197964517,\n",
       "  117.20214453079159,\n",
       "  115.942207797191,\n",
       "  115.45211229610852,\n",
       "  117.39169296755811,\n",
       "  115.32847655502341,\n",
       "  115.55537472611674,\n",
       "  114.96080533711184,\n",
       "  114.71271322902521,\n",
       "  114.46426118236755,\n",
       "  114.51930383679522,\n",
       "  114.39215046463777,\n",
       "  114.49157941559012,\n",
       "  114.32075181216129,\n",
       "  113.84373935279245],\n",
       " 'trainM4K_loss': [307.98299969368907,\n",
       "  303.3875200245957,\n",
       "  299.96885998639937,\n",
       "  301.6731856081721,\n",
       "  294.518084222578,\n",
       "  298.9345974587371,\n",
       "  295.8685500736264,\n",
       "  297.81215728803426,\n",
       "  291.54327530365646,\n",
       "  293.1265811379887,\n",
       "  299.01569233862966,\n",
       "  296.46196639730863,\n",
       "  301.9590532836041,\n",
       "  298.4496665550745,\n",
       "  299.6739900340679,\n",
       "  293.6031569845175,\n",
       "  299.7514466144087,\n",
       "  302.3569957849942,\n",
       "  303.6749214619867,\n",
       "  304.9083929137747],\n",
       " 'trainP4K_RG_loss': [384.6195917064934,\n",
       "  340.7476247796418,\n",
       "  325.4930553883392,\n",
       "  349.84031030489643,\n",
       "  324.1350283814007,\n",
       "  320.5718735438044,\n",
       "  311.6156313790694,\n",
       "  334.60826459256367,\n",
       "  334.91809060834805,\n",
       "  369.4523448278391,\n",
       "  352.74638632449614,\n",
       "  357.3488353392092,\n",
       "  340.30528339028956,\n",
       "  343.51703894579236,\n",
       "  347.0616906456369,\n",
       "  344.15831586357865,\n",
       "  334.8345898884745,\n",
       "  361.6405914566297,\n",
       "  368.7618994867663,\n",
       "  335.3275078867723]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=T-TNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars=['PHQPERC','TPHYSTNDPERC','QRLPERC','QRSPERC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINFILE = '2021_01_24_O3_TRAIN_shuffle.nc'\n",
    "NORMFILE = '2021_01_24_NORM_O3_small.nc'\n",
    "# VALIDFILE = '2021_01_24_O3_VALID.nc'\n",
    "# GENTESTFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "# In percentile space\n",
    "TRAINFILE = '2021_04_24_RG_PERC_TRAIN_M4K_shuffle.nc'\n",
    "VALIDFILE = '2021_04_24_RG_PERC_VALID_M4K.nc'\n",
    "TESTFILE_DIFFCLIMATE = '2021_04_24_RG_PERC_TRAIN_P4K_shuffle.nc'\n",
    "TESTFILE_DIFFGEOG = '2021_04_09_PERC_TRAIN_M4K_shuffle.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'TfromNS'\n",
    "train_gen_T = train_gen_TNS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_NN_PERC_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BCONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BCONS'\n",
    "train_gen_T = train_gen_BCONS\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_03_RG2AQ_NN_PERC_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Rescaling (T=BMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tscaling_name = 'BMSE'\n",
    "train_gen_T = train_gen_BMSE\n",
    "\n",
    "train_gen_CI = DataGeneratorCI(data_fn = path+TRAINFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "valid_gen_CI = DataGeneratorCI(data_fn = path+VALIDFILE,\n",
    "                               input_vars=in_vars,\n",
    "                               output_vars=out_vars,\n",
    "                               norm_fn=path+NORMFILE,\n",
    "                               input_transform=('mean', 'maxrs'),\n",
    "                               output_transform=None,\n",
    "                               batch_size=8192,\n",
    "                               shuffle=True,\n",
    "                               xarray=False,\n",
    "                               var_cut_off=None, \n",
    "                               Qscaling='RH',\n",
    "                               Tscaling=Tscaling_name,\n",
    "                               LHFscaling='LHF_nsDELQ',\n",
    "                               SHFscaling=None,\n",
    "                               output_scaling=False,\n",
    "                               interpolate=False,\n",
    "                               hyam=hyam,hybm=hybm,\n",
    "                               inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                               inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                               inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                               inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                               inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                               inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                               inp_sub_SHFscaling=None,\n",
    "                               inp_div_SHFscaling=None,\n",
    "                               lev=None, interm_size=40,\n",
    "                               lower_lim=6,is_continous=True,Tnot=5,\n",
    "                               epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffclimate_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFCLIMATE,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')\n",
    "\n",
    "test_diffgeog_gen_CI = DataGeneratorCI(data_fn = path+TESTFILE_DIFFGEOG,\n",
    "                                       input_vars=in_vars,\n",
    "                                       output_vars=out_vars,\n",
    "                                       norm_fn=path+NORMFILE,\n",
    "                                       input_transform=('mean', 'maxrs'),\n",
    "                                       output_transform=None,\n",
    "                                       batch_size=8192,\n",
    "                                       shuffle=True,\n",
    "                                       xarray=False,\n",
    "                                       var_cut_off=None, \n",
    "                                       Qscaling='RH',\n",
    "                                       Tscaling=Tscaling_name,\n",
    "                                       LHFscaling='LHF_nsDELQ',\n",
    "                                       SHFscaling=None,\n",
    "                                       output_scaling=False,\n",
    "                                       interpolate=False,\n",
    "                                       hyam=hyam,hybm=hybm,\n",
    "                                       inp_sub_Qscaling=train_gen_RH.input_transform.sub,\n",
    "                                       inp_div_Qscaling=train_gen_RH.input_transform.div,\n",
    "                                       inp_sub_Tscaling=train_gen_T.input_transform.sub,\n",
    "                                       inp_div_Tscaling=train_gen_T.input_transform.div,\n",
    "                                       inp_sub_LHFscaling=train_gen_LHF_nsDELQ.input_transform.sub,\n",
    "                                       inp_div_LHFscaling=train_gen_LHF_nsDELQ.input_transform.div,\n",
    "                                       inp_sub_SHFscaling=None,\n",
    "                                       inp_div_SHFscaling=None,\n",
    "                                       lev=None, interm_size=40,\n",
    "                                       lower_lim=6,is_continous=True,Tnot=5,\n",
    "                                       epsQ=1e-3,epsT=1,mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 120)               15480     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid (TensorF [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_06_17_RG2AQ_NN_PERC_RH_BMSE_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_diffclimate_gen_CI,'trainP4K'),(test_diffgeog_gen_CI,'trainM4K_RG')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17475/17475 [==============================] - 8286s 474ms/step - loss: 1.4416e-04 - val_loss: 8.8477e-10\n",
      "Epoch 2/20\n",
      "17475/17475 [==============================] - 8285s 474ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 3/20\n",
      "17475/17475 [==============================] - 8446s 483ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 4/20\n",
      "17475/17475 [==============================] - 8913s 510ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 5/20\n",
      "17475/17475 [==============================] - 9194s 526ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 6/20\n",
      "17475/17475 [==============================] - 8596s 492ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 7/20\n",
      "17475/17475 [==============================] - 9178s 525ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 8/20\n",
      "17475/17475 [==============================] - 8895s 509ms/step - loss: 8.8980e-10 - val_loss: 8.8477e-10\n",
      "Epoch 9/20\n",
      "17474/17475 [============================>.] - ETA: 0s - loss: 8.8981e-10"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate-invariant (T,Q,PS,S0,SHF,LHF)->($\\dot{T}$,$\\dot{q}$,RADFLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ozone (T,Q,$O_{3}$,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(94,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_O3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_O3, epochs=Nep, validation_data=valid_gen_O3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Ozone (T,Q,S0,PS,LHF,SHF)$\\rightarrow$($\\dot{q}$,$\\dot{T}$,lw,sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_01_25_noO3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path_HDF5+save_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nep = 15\n",
    "# model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "#               callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_noO3, epochs=Nep, validation_data=valid_gen_noO3,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_MLR_PERC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 1184s 206ms/step - loss: 0.0510 - val_loss: 0.0429\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 1035s 180ms/step - loss: 0.0419 - val_loss: 0.0427\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 753s 131ms/step - loss: 0.0417 - val_loss: 0.0425\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 726s 126ms/step - loss: 0.0416 - val_loss: 0.0424\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 765s 133ms/step - loss: 0.0415 - val_loss: 0.0424\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 0.0415 - val_loss: 0.0424\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 756s 131ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 766s 133ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 763s 132ms/step - loss: 0.0415 - val_loss: 0.0423\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 729s 127ms/step - loss: 0.0414 - val_loss: 0.0424\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 774s 134ms/step - loss: 0.0414 - val_loss: 0.0422\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 744s 129ms/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 750s 130ms/step - loss: 0.0414 - val_loss: 0.0421\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 731s 127ms/step - loss: 0.0414 - val_loss: 0.0423\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 744s 129ms/step - loss: 0.0414 - val_loss: 0.0425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402db4240>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.050982531596102595,\n",
       "  0.04185105333621251,\n",
       "  0.04165498730630319,\n",
       "  0.04157832884143202,\n",
       "  0.04153471013858079,\n",
       "  0.041505846159183686,\n",
       "  0.04148416520251771,\n",
       "  0.04146769321144383,\n",
       "  0.04145484425892099,\n",
       "  0.04144392009125254,\n",
       "  0.04143447252735948,\n",
       "  0.04142640940075244,\n",
       "  0.04141932996618994,\n",
       "  0.04141429779437562,\n",
       "  0.04140815107314552],\n",
       " 'val_loss': [0.042909197750994055,\n",
       "  0.04265815539936504,\n",
       "  0.04253125871611655,\n",
       "  0.0423782235956507,\n",
       "  0.04237728513551328,\n",
       "  0.04235305504845943,\n",
       "  0.04230615223943935,\n",
       "  0.04227608388790575,\n",
       "  0.0423463198359328,\n",
       "  0.042378264729721796,\n",
       "  0.042240786968061286,\n",
       "  0.04227235703251836,\n",
       "  0.042130285002797335,\n",
       "  0.04226516442969454,\n",
       "  0.04249188080922277],\n",
       " 'testP4K_loss': [0.04237089483123066,\n",
       "  0.04204679281155743,\n",
       "  0.041950938805466936,\n",
       "  0.041866756795589787,\n",
       "  0.04182252665865533,\n",
       "  0.04181972140371545,\n",
       "  0.041768535935374836,\n",
       "  0.041756746478308786,\n",
       "  0.041807406456754515,\n",
       "  0.04179418787080238,\n",
       "  0.041728623833007836,\n",
       "  0.041724651345679334,\n",
       "  0.04165810040754197,\n",
       "  0.04172214069534936,\n",
       "  0.0418310413666763]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.050982531596102595,\n",
       "  0.04185105333621251,\n",
       "  0.04165498730630319,\n",
       "  0.04157832884143202,\n",
       "  0.04153471013858079,\n",
       "  0.041505846159183686,\n",
       "  0.04148416520251771,\n",
       "  0.04146769321144383,\n",
       "  0.04145484425892099,\n",
       "  0.04144392009125254,\n",
       "  0.04143447252735948,\n",
       "  0.04142640940075244,\n",
       "  0.04141932996618994,\n",
       "  0.04141429779437562,\n",
       "  0.04140815107314552],\n",
       " 'val_loss': [0.042909197750994055,\n",
       "  0.04265815539936504,\n",
       "  0.04253125871611655,\n",
       "  0.0423782235956507,\n",
       "  0.04237728513551328,\n",
       "  0.04235305504845943,\n",
       "  0.04230615223943935,\n",
       "  0.04227608388790575,\n",
       "  0.0423463198359328,\n",
       "  0.042378264729721796,\n",
       "  0.042240786968061286,\n",
       "  0.04227235703251836,\n",
       "  0.042130285002797335,\n",
       "  0.04226516442969454,\n",
       "  0.04249188080922277],\n",
       " 'testP4K_loss': [0.04237089483123066,\n",
       "  0.04204679281155743,\n",
       "  0.041950938805466936,\n",
       "  0.041866756795589787,\n",
       "  0.04182252665865533,\n",
       "  0.04181972140371545,\n",
       "  0.041768535935374836,\n",
       "  0.041756746478308786,\n",
       "  0.041807406456754515,\n",
       "  0.04179418787080238,\n",
       "  0.041728623833007836,\n",
       "  0.041724651345679334,\n",
       "  0.04165810040754197,\n",
       "  0.04172214069534936,\n",
       "  0.0418310413666763]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF Logistic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_1 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_Log_PERC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 794s 138ms/step - loss: 0.0492 - val_loss: 0.0427\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 761s 132ms/step - loss: 0.0411 - val_loss: 0.0412\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 842s 146ms/step - loss: 0.0405 - val_loss: 0.0410\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 0.0403 - val_loss: 0.0408\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 727s 126ms/step - loss: 0.0402 - val_loss: 0.0408\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 727s 126ms/step - loss: 0.0401 - val_loss: 0.0407\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 738s 128ms/step - loss: 0.0401 - val_loss: 0.0406\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 740s 129ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 769s 133ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 768s 133ms/step - loss: 0.0400 - val_loss: 0.0406\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 738s 128ms/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 687s 119ms/step - loss: 0.0399 - val_loss: 0.0406\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 639s 111ms/step - loss: 0.0399 - val_loss: 0.0405\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 749s 130ms/step - loss: 0.0399 - val_loss: 0.0405\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 0.0399 - val_loss: 0.0404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402d40668>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.04923291314722536,\n",
       "  0.04110610745569074,\n",
       "  0.04050010521521811,\n",
       "  0.040299923023502326,\n",
       "  0.040188955621450057,\n",
       "  0.040115493637423776,\n",
       "  0.040061687980442955,\n",
       "  0.04002036269023862,\n",
       "  0.039987151799029744,\n",
       "  0.03995977671317889,\n",
       "  0.03993697982862977,\n",
       "  0.03991729508678539,\n",
       "  0.0399002306710558,\n",
       "  0.03988536803773406,\n",
       "  0.03987165327315054],\n",
       " 'val_loss': [0.04265024852781968,\n",
       "  0.041217454327464445,\n",
       "  0.04097958452281418,\n",
       "  0.04079991503641219,\n",
       "  0.04078920084213819,\n",
       "  0.04070766954445322,\n",
       "  0.04064866158667609,\n",
       "  0.0406280841281429,\n",
       "  0.04056495156529776,\n",
       "  0.04057289374916206,\n",
       "  0.040560972740690364,\n",
       "  0.0405654035945343,\n",
       "  0.0404891143274493,\n",
       "  0.040506530972142435,\n",
       "  0.0404415049514598],\n",
       " 'testP4K_loss': [0.07411513028238494,\n",
       "  0.07390664614972449,\n",
       "  0.07520650478889554,\n",
       "  0.07573994763074626,\n",
       "  0.0764975735435722,\n",
       "  0.0768441669025755,\n",
       "  0.07727208656825946,\n",
       "  0.07749857328113179,\n",
       "  0.07786896164909984,\n",
       "  0.07816397840018766,\n",
       "  0.07837125465184781,\n",
       "  0.07873877331958073,\n",
       "  0.07878147320397179,\n",
       "  0.07903415168914706,\n",
       "  0.07906030325885516]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.04923291314722536,\n",
       "  0.04110610745569074,\n",
       "  0.04050010521521811,\n",
       "  0.040299923023502326,\n",
       "  0.040188955621450057,\n",
       "  0.040115493637423776,\n",
       "  0.040061687980442955,\n",
       "  0.04002036269023862,\n",
       "  0.039987151799029744,\n",
       "  0.03995977671317889,\n",
       "  0.03993697982862977,\n",
       "  0.03991729508678539,\n",
       "  0.0399002306710558,\n",
       "  0.03988536803773406,\n",
       "  0.03987165327315054],\n",
       " 'val_loss': [0.04265024852781968,\n",
       "  0.041217454327464445,\n",
       "  0.04097958452281418,\n",
       "  0.04079991503641219,\n",
       "  0.04078920084213819,\n",
       "  0.04070766954445322,\n",
       "  0.04064866158667609,\n",
       "  0.0406280841281429,\n",
       "  0.04056495156529776,\n",
       "  0.04057289374916206,\n",
       "  0.040560972740690364,\n",
       "  0.0405654035945343,\n",
       "  0.0404891143274493,\n",
       "  0.040506530972142435,\n",
       "  0.0404415049514598],\n",
       " 'testP4K_loss': [0.07411513028238494,\n",
       "  0.07390664614972449,\n",
       "  0.07520650478889554,\n",
       "  0.07573994763074626,\n",
       "  0.0764975735435722,\n",
       "  0.0768441669025755,\n",
       "  0.07727208656825946,\n",
       "  0.07749857328113179,\n",
       "  0.07786896164909984,\n",
       "  0.07816397840018766,\n",
       "  0.07837125465184781,\n",
       "  0.07873877331958073,\n",
       "  0.07878147320397179,\n",
       "  0.07903415168914706,\n",
       "  0.07906030325885516]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_08_NN6L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2542s 441ms/step - loss: 342.0934 - val_loss: 329.9304\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2701s 469ms/step - loss: 320.9061 - val_loss: 316.9567\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 2452s 426ms/step - loss: 311.2353 - val_loss: 309.6577\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 2254s 391ms/step - loss: 305.5034 - val_loss: 304.9600\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1731s 301ms/step - loss: 301.7379 - val_loss: 301.6836\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1117s 194ms/step - loss: 299.0327 - val_loss: 299.1967\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 816s 142ms/step - loss: 296.9427 - val_loss: 297.2431\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 713s 124ms/step - loss: 295.2764 - val_loss: 295.7120\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 694s 121ms/step - loss: 293.9242 - val_loss: 294.4352\n",
      "Epoch 10/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 292.8106 - val_loss: 293.3954\n",
      "Epoch 11/20\n",
      "5759/5759 [==============================] - 712s 124ms/step - loss: 291.8827 - val_loss: 292.5364\n",
      "Epoch 12/20\n",
      "5759/5759 [==============================] - 722s 125ms/step - loss: 291.1030 - val_loss: 291.8280\n",
      "Epoch 13/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 290.4395 - val_loss: 291.2006\n",
      "Epoch 14/20\n",
      "5759/5759 [==============================] - 699s 121ms/step - loss: 289.8687 - val_loss: 290.6818\n",
      "Epoch 15/20\n",
      "5759/5759 [==============================] - 702s 122ms/step - loss: 289.3730 - val_loss: 290.2150\n",
      "Epoch 16/20\n",
      "5759/5759 [==============================] - 703s 122ms/step - loss: 288.9378 - val_loss: 289.8255\n",
      "Epoch 17/20\n",
      "5759/5759 [==============================] - 684s 119ms/step - loss: 288.5535 - val_loss: 289.4663\n",
      "Epoch 18/20\n",
      "5759/5759 [==============================] - 694s 120ms/step - loss: 288.2098 - val_loss: 289.1482\n",
      "Epoch 19/20\n",
      "5759/5759 [==============================] - 691s 120ms/step - loss: 287.9012 - val_loss: 288.8622\n",
      "Epoch 20/20\n",
      "5759/5759 [==============================] - 700s 122ms/step - loss: 287.6209 - val_loss: 288.5948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee43236630>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [342.09338355590165,\n",
       "  320.9061308653948,\n",
       "  311.23525867438974,\n",
       "  305.5033564990667,\n",
       "  301.7378943407364,\n",
       "  299.03272853665186,\n",
       "  296.9426967053182,\n",
       "  295.27639429510043,\n",
       "  293.9242286688753,\n",
       "  292.8105507327525,\n",
       "  291.88271119160294,\n",
       "  291.10301101189106,\n",
       "  290.43950267433394,\n",
       "  289.868749243287,\n",
       "  289.3729577492583,\n",
       "  288.93778015867997,\n",
       "  288.5534648097085,\n",
       "  288.2098106308765,\n",
       "  287.90122331664634,\n",
       "  287.6208748516885],\n",
       " 'val_loss': [329.9303815202876,\n",
       "  316.9566871866662,\n",
       "  309.6576747567941,\n",
       "  304.96000983386693,\n",
       "  301.6835738202586,\n",
       "  299.1966588354587,\n",
       "  297.243100628537,\n",
       "  295.7119940864864,\n",
       "  294.43520978061343,\n",
       "  293.3954472839883,\n",
       "  292.5363609783128,\n",
       "  291.8279773312142,\n",
       "  291.20056871985776,\n",
       "  290.6817859751116,\n",
       "  290.2149793161854,\n",
       "  289.82551079177665,\n",
       "  289.4662798184087,\n",
       "  289.1482212480884,\n",
       "  288.8622394983756,\n",
       "  288.5948049347219],\n",
       " 'testP4K_loss': [755.1666537160246,\n",
       "  749.3020249286134,\n",
       "  764.911121189238,\n",
       "  790.1407622900099,\n",
       "  815.8143627241674,\n",
       "  838.4558800362604,\n",
       "  857.9830822118015,\n",
       "  876.1170735086364,\n",
       "  892.4915627105739,\n",
       "  907.936570900022,\n",
       "  922.2206510340221,\n",
       "  936.0840369663515,\n",
       "  948.2193597782177,\n",
       "  958.382844659429,\n",
       "  970.0905091218109,\n",
       "  979.5140667531757,\n",
       "  988.4868674547829,\n",
       "  996.0533862367234,\n",
       "  1002.38678037935,\n",
       "  1009.3211877889657]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH Logistic version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "# densout = Dense(128, activation='linear')(inp)\n",
    "# densout = LeakyReLU(alpha=0.3)(densout)\n",
    "# for i in range (6):\n",
    "#     densout = Dense(128, activation='linear')(densout)\n",
    "#     densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "dense_out = tf.keras.activations.sigmoid(dense_out)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_2 (Tenso [(None, 120)]             0         \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_15_Log_PERC_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = AdditionalValidationSets([(train_gen_CI,valid_gen_CI,test_gen_CI)])\n",
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5759/5759 [==============================] - 1103s 191ms/step - loss: 0.0467 - val_loss: 0.0405\n",
      "Epoch 2/15\n",
      "5759/5759 [==============================] - 1136s 197ms/step - loss: 0.0392 - val_loss: 0.0394\n",
      "Epoch 3/15\n",
      "5759/5759 [==============================] - 1123s 195ms/step - loss: 0.0389 - val_loss: 0.0394\n",
      "Epoch 4/15\n",
      "5759/5759 [==============================] - 1115s 194ms/step - loss: 0.0388 - val_loss: 0.0393\n",
      "Epoch 5/15\n",
      "5759/5759 [==============================] - 1128s 196ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 6/15\n",
      "5759/5759 [==============================] - 1082s 188ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 7/15\n",
      "5759/5759 [==============================] - 1136s 197ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 8/15\n",
      "5759/5759 [==============================] - 1069s 186ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 9/15\n",
      "5759/5759 [==============================] - 1093s 190ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 10/15\n",
      "5759/5759 [==============================] - 1121s 195ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 11/15\n",
      "5759/5759 [==============================] - 1114s 193ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 12/15\n",
      "5759/5759 [==============================] - 1146s 199ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 13/15\n",
      "5759/5759 [==============================] - 1111s 193ms/step - loss: 0.0387 - val_loss: 0.0392\n",
      "Epoch 14/15\n",
      "5759/5759 [==============================] - 1079s 187ms/step - loss: 0.0386 - val_loss: 0.0392\n",
      "Epoch 15/15\n",
      "5759/5759 [==============================] - 1139s 198ms/step - loss: 0.0386 - val_loss: 0.0392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff402f0acc0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 15\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.0467287344276843,\n",
       "  0.0392072251961502,\n",
       "  0.038853517252732575,\n",
       "  0.03877987418298528,\n",
       "  0.03874506841666439,\n",
       "  0.03872233596301302,\n",
       "  0.03870567677141188,\n",
       "  0.03869225124110984,\n",
       "  0.03868109595084236,\n",
       "  0.03867188213775712,\n",
       "  0.0386642090824872,\n",
       "  0.03865747418455789,\n",
       "  0.03865132474455802,\n",
       "  0.03864620624032156,\n",
       "  0.03864140413024914],\n",
       " 'val_loss': [0.040451113186371485,\n",
       "  0.03944033429256318,\n",
       "  0.03935017663365135,\n",
       "  0.03931213896163092,\n",
       "  0.03929163821896072,\n",
       "  0.03926565892478177,\n",
       "  0.039275243295873806,\n",
       "  0.039249983558271286,\n",
       "  0.0392355400208758,\n",
       "  0.03925549958671793,\n",
       "  0.03922620319762756,\n",
       "  0.03919682341832721,\n",
       "  0.03921325347639839,\n",
       "  0.039212426125584225,\n",
       "  0.03919467062651657],\n",
       " 'testP4K_loss': [0.06156855558834964,\n",
       "  0.05892425673790308,\n",
       "  0.05871588077863804,\n",
       "  0.058408490616755036,\n",
       "  0.05826237939559019,\n",
       "  0.05821150895383091,\n",
       "  0.058247450274676975,\n",
       "  0.05818161563383118,\n",
       "  0.05822653928238749,\n",
       "  0.05826851157272707,\n",
       "  0.05817507403042966,\n",
       "  0.058177342630420154,\n",
       "  0.05819879022675839,\n",
       "  0.05813914520759851,\n",
       "  0.05809993178835799]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   3/5759 [..............................] - ETA: 56:44 - loss: 362.5890  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QSATdeficit linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_QSATdeficit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfromNS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_TfromNS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCONS linear version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_BCONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8098s 1s/step - loss: 324.9880 - val_loss: 313.2678\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 12555s 2s/step - loss: 308.3165 - val_loss: 307.0678\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 13642s 2s/step - loss: 304.0913 - val_loss: 304.2872\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10465s 2s/step - loss: 301.8938 - val_loss: 302.7330\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7909s 1s/step - loss: 300.5873 - val_loss: 301.7510\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 8526s 1s/step - loss: 299.7002 - val_loss: 301.0397\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7230s 1s/step - loss: 299.0066 - val_loss: 300.3937\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 8294s 1s/step - loss: 298.4328 - val_loss: 299.8434\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 8250s 1s/step - loss: 297.9426 - val_loss: 299.4925\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8909s 2s/step - loss: 297.5149 - val_loss: 299.0509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcce5600f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [324.98798001675704,\n",
       "  308.31648811596017,\n",
       "  304.0912586973243,\n",
       "  301.8937511361293,\n",
       "  300.5873382457078,\n",
       "  299.7002091802725,\n",
       "  299.0065637804771,\n",
       "  298.4328209674489,\n",
       "  297.9425933249689,\n",
       "  297.5149222372472],\n",
       " 'val_loss': [313.2678370875624,\n",
       "  307.0677993880396,\n",
       "  304.2871500202989,\n",
       "  302.73299462630064,\n",
       "  301.75103527639715,\n",
       "  301.03966337007205,\n",
       "  300.39373488234764,\n",
       "  299.8433922314147,\n",
       "  299.4924658236212,\n",
       "  299.0508612894959],\n",
       " 'testP4K_loss': [696.3224985280676,\n",
       "  686.2635380876054,\n",
       "  682.7061903721445,\n",
       "  680.7840038270242,\n",
       "  679.5804400619129,\n",
       "  678.422916433854,\n",
       "  677.2276842855165,\n",
       "  676.2437589546028,\n",
       "  675.5998048438668,\n",
       "  674.6606088692057]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+(T-TNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_31_MLR_RH_NSto220'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 10546s 2s/step - loss: 333.2154 - val_loss: 318.1506\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10194s 2s/step - loss: 310.9067 - val_loss: 308.3093\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 9862s 2s/step - loss: 304.7300 - val_loss: 304.2289\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 11121s 2s/step - loss: 301.8291 - val_loss: 302.1135\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10748s 2s/step - loss: 300.2339 - val_loss: 300.8775\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10117s 2s/step - loss: 299.1923 - val_loss: 299.9938\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10174s 2s/step - loss: 298.3948 - val_loss: 299.3052\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 9887s 2s/step - loss: 297.7428 - val_loss: 298.7333\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10299s 2s/step - loss: 297.1934 - val_loss: 298.2455\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 10431s 2s/step - loss: 296.7218 - val_loss: 297.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9680546668>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.21542532209264,\n",
       "  310.90667960419796,\n",
       "  304.72998016670067,\n",
       "  301.8290961309116,\n",
       "  300.2339088743316,\n",
       "  299.19231390816446,\n",
       "  298.39477655642935,\n",
       "  297.74276377828113,\n",
       "  297.19338681379486,\n",
       "  296.7217506637878],\n",
       " 'val_loss': [318.15060213358146,\n",
       "  308.30926291885646,\n",
       "  304.22893252623965,\n",
       "  302.1134895894682,\n",
       "  300.8775425180225,\n",
       "  299.9937965616662,\n",
       "  299.30517368746393,\n",
       "  298.73325744039545,\n",
       "  298.2454878341621,\n",
       "  297.8241368286815],\n",
       " 'testP4K_loss': [723.0185877851295,\n",
       "  698.1272752300885,\n",
       "  685.258197891926,\n",
       "  679.2784368682783,\n",
       "  676.55788616959,\n",
       "  674.8558076801512,\n",
       "  673.4969380158018,\n",
       "  672.4355979235329,\n",
       "  671.423601997825,\n",
       "  670.7109736936912]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_19_MLR_RH_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 7449s 1s/step - loss: 332.8352 - val_loss: 318.1170\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 5519s 958ms/step - loss: 311.1496 - val_loss: 308.7574\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5822s 1s/step - loss: 305.3264 - val_loss: 304.9336\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6582s 1s/step - loss: 302.6262 - val_loss: 302.9958\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7103s 1s/step - loss: 301.1792 - val_loss: 301.9337\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 14946s 3s/step - loss: 300.2849 - val_loss: 301.1922\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 4869s 846ms/step - loss: 299.6383 - val_loss: 300.6844\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5781s 1s/step - loss: 299.1321 - val_loss: 300.2569\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6748s 1s/step - loss: 298.7180 - val_loss: 299.8978\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 4745s 824ms/step - loss: 298.3688 - val_loss: 299.6018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232cfd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [332.83523499690364,\n",
       "  311.1495506735721,\n",
       "  305.32637336838263,\n",
       "  302.6261628686316,\n",
       "  301.1792370861946,\n",
       "  300.28492045017515,\n",
       "  299.6382695135298,\n",
       "  299.1321185072753,\n",
       "  298.7179960372403,\n",
       "  298.3687616935474],\n",
       " 'val_loss': [318.11703873779254,\n",
       "  308.75738331521546,\n",
       "  304.9335873032069,\n",
       "  302.9957635315034,\n",
       "  301.9337132302216,\n",
       "  301.1921791930977,\n",
       "  300.68438835990605,\n",
       "  300.2568701773645,\n",
       "  299.89779036501074,\n",
       "  299.60179900476817],\n",
       " 'testP4K_loss': [732.1974313007879,\n",
       "  706.4723622909272,\n",
       "  691.7300854846619,\n",
       "  684.7900059335971,\n",
       "  681.8280336922402,\n",
       "  679.9574934180021,\n",
       "  678.7282812103871,\n",
       "  677.7188479585591,\n",
       "  676.8431889352383,\n",
       "  676.1222872974321]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsDELQ NN version with test loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(120, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               15480     \n",
      "=================================================================\n",
      "Total params: 122,872\n",
      "Trainable params: 122,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_09_NN7L_RH_TfromNS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5759/5759 [==============================] - 2906s 505ms/step - loss: 202.5537 - val_loss: 189.6939\n",
      "Epoch 2/20\n",
      "5759/5759 [==============================] - 2024s 352ms/step - loss: 185.1673 - val_loss: 182.6220\n",
      "Epoch 3/20\n",
      "5759/5759 [==============================] - 1676s 291ms/step - loss: 180.0439 - val_loss: 180.6006\n",
      "Epoch 4/20\n",
      "5759/5759 [==============================] - 1666s 289ms/step - loss: 177.2701 - val_loss: 176.7251\n",
      "Epoch 5/20\n",
      "5759/5759 [==============================] - 1665s 289ms/step - loss: 175.5439 - val_loss: 175.3660\n",
      "Epoch 6/20\n",
      "5759/5759 [==============================] - 1634s 284ms/step - loss: 174.3043 - val_loss: 174.8005\n",
      "Epoch 7/20\n",
      "5759/5759 [==============================] - 1668s 290ms/step - loss: 173.3022 - val_loss: 173.3208\n",
      "Epoch 8/20\n",
      "5759/5759 [==============================] - 1678s 291ms/step - loss: 172.4542 - val_loss: 172.6152\n",
      "Epoch 9/20\n",
      "5759/5759 [==============================] - 2503s 435ms/step - loss: 171.7443 - val_loss: 172.0109\n",
      "Epoch 10/20\n",
      "5758/5759 [============================>.] - ETA: 0s - loss: 171.1936"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+TfromNS+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_TfromNS_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 6478s 1s/step - loss: 334.0426 - val_loss: 318.8078\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 7037s 1s/step - loss: 310.9031 - val_loss: 307.7163\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5910s 1s/step - loss: 304.0670 - val_loss: 303.4669\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7744s 1s/step - loss: 301.0845 - val_loss: 301.4636\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 7767s 1s/step - loss: 299.5355 - val_loss: 300.3262\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7525s 1s/step - loss: 298.5930 - val_loss: 299.6363\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7967s 1s/step - loss: 297.9281 - val_loss: 299.1007\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 6677s 1s/step - loss: 297.4247 - val_loss: 298.6999\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 6912s 1s/step - loss: 297.0260 - val_loss: 298.3802\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 8484s 1s/step - loss: 296.6993 - val_loss: 298.1298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad9232ceb8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [334.04261498667336,\n",
       "  310.90313761506275,\n",
       "  304.06700017431956,\n",
       "  301.08452700392866,\n",
       "  299.5355056150648,\n",
       "  298.59301007193477,\n",
       "  297.92813114855807,\n",
       "  297.42471469800324,\n",
       "  297.02595976008,\n",
       "  296.69932817398495],\n",
       " 'val_loss': [318.8077631269736,\n",
       "  307.7162972562862,\n",
       "  303.4669281734807,\n",
       "  301.4636359146121,\n",
       "  300.3261635153653,\n",
       "  299.6362964928039,\n",
       "  299.10066872735194,\n",
       "  298.6998974468755,\n",
       "  298.3802061189176,\n",
       "  298.1298349087428],\n",
       " 'testP4K_loss': [717.7692014099695,\n",
       "  692.5093093676612,\n",
       "  681.1892812258153,\n",
       "  677.2928423535407,\n",
       "  675.2491797104337,\n",
       "  673.9023133940009,\n",
       "  672.9248305401367,\n",
       "  672.2034651915096,\n",
       "  671.5856063464887,\n",
       "  671.1150249686433]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+BCONS+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_03_23_MLR_RH_BCONS_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 8133s 1s/step - loss: 330.7609 - val_loss: 316.7712\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6660s 1s/step - loss: 310.4906 - val_loss: 307.8904\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 7116s 1s/step - loss: 304.7100 - val_loss: 303.9924\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 7062s 1s/step - loss: 301.7960 - val_loss: 301.8254\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 8137s 1s/step - loss: 300.0964 - val_loss: 300.5073\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 7596s 1s/step - loss: 298.9904 - val_loss: 299.6046\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 7545s 1s/step - loss: 298.1876 - val_loss: 298.9532\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 7470s 1s/step - loss: 297.5743 - val_loss: 298.4452\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7456s 1s/step - loss: 297.0949 - val_loss: 298.0184\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 7469s 1s/step - loss: 296.7098 - val_loss: 297.6911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad92882ba8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [330.76085305019353,\n",
       "  310.4906432967692,\n",
       "  304.7099609745938,\n",
       "  301.7960417360497,\n",
       "  300.09642175858914,\n",
       "  298.9904117309337,\n",
       "  298.18756228044725,\n",
       "  297.57428517435176,\n",
       "  297.0949075244785,\n",
       "  296.709771785746],\n",
       " 'val_loss': [316.77116454788614,\n",
       "  307.8904337055013,\n",
       "  303.99240362525046,\n",
       "  301.8254467632818,\n",
       "  300.5073347083597,\n",
       "  299.6046408204614,\n",
       "  298.95319436894175,\n",
       "  298.4452129081611,\n",
       "  298.01840061586466,\n",
       "  297.69113439902117],\n",
       " 'testP4K_loss': [702.3246120228104,\n",
       "  686.3533080012243,\n",
       "  680.0862056377094,\n",
       "  676.7398991455286,\n",
       "  674.6481522557676,\n",
       "  673.1816021796275,\n",
       "  672.0055764137222,\n",
       "  671.111931773641,\n",
       "  670.3688568089174,\n",
       "  669.7110506886809]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsDELQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_01_MLR_RH_NSto220_LHF_nsDELQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 11407s 2s/step - loss: 333.0526 - val_loss: 317.6788\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 10810s 2s/step - loss: 310.2102 - val_loss: 307.3595\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 10216s 2s/step - loss: 303.6873 - val_loss: 302.9961\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 10683s 2s/step - loss: 300.5542 - val_loss: 300.6824\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 10550s 2s/step - loss: 298.8112 - val_loss: 299.3256\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 10292s 2s/step - loss: 297.7034 - val_loss: 298.4254\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 10560s 2s/step - loss: 296.9044 - val_loss: 297.7670\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 11866s 2s/step - loss: 296.3028 - val_loss: 297.2482\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 10787s 2s/step - loss: 295.8395 - val_loss: 296.8615\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 11359s 2s/step - loss: 295.4770 - val_loss: 296.5718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96803fe2e8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05256303355685,\n",
       "  310.2102426647167,\n",
       "  303.6872879666863,\n",
       "  300.55415829901773,\n",
       "  298.81120031363105,\n",
       "  297.7034077711415,\n",
       "  296.90441165903536,\n",
       "  296.30281556532356,\n",
       "  295.83954845970965,\n",
       "  295.4770164907051],\n",
       " 'val_loss': [317.6788116925534,\n",
       "  307.3594649500833,\n",
       "  302.9960944944573,\n",
       "  300.68239808442047,\n",
       "  299.32556045328016,\n",
       "  298.42535546180176,\n",
       "  297.7669507749966,\n",
       "  297.24817319492274,\n",
       "  296.86145162218895,\n",
       "  296.5717960327293],\n",
       " 'testP4K_loss': [721.7310292205681,\n",
       "  696.0703459624854,\n",
       "  682.7236189375567,\n",
       "  676.256974551415,\n",
       "  672.9861661186186,\n",
       "  670.9493864272823,\n",
       "  669.4111307171855,\n",
       "  668.1374724690269,\n",
       "  667.1579007533959,\n",
       "  666.4447404165944]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RH+NSto220+LHF_nsQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "dense_out = Dense(120, activation='linear')(inp)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               7800      \n",
      "=================================================================\n",
      "Total params: 7,800\n",
      "Trainable params: 7,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/DFS-L/DATA/pritchard/tbeucler/SPCAM/HDF5_DATA/'\n",
    "save_name = '2021_04_03_MLR_RH_NSto220_LHF_nsQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = AdditionalValidationSets([(test_gen_CI,'testP4K')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5759/5759 [==============================] - 9714s 2s/step - loss: 333.0516 - val_loss: 317.9425\n",
      "Epoch 2/10\n",
      "5759/5759 [==============================] - 6439s 1s/step - loss: 310.6932 - val_loss: 308.0465\n",
      "Epoch 3/10\n",
      "5759/5759 [==============================] - 5183s 900ms/step - loss: 304.5171 - val_loss: 304.0613\n",
      "Epoch 4/10\n",
      "5759/5759 [==============================] - 6201s 1s/step - loss: 301.7091 - val_loss: 302.0904\n",
      "Epoch 5/10\n",
      "5759/5759 [==============================] - 6448s 1s/step - loss: 300.2337 - val_loss: 301.0068\n",
      "Epoch 6/10\n",
      "5759/5759 [==============================] - 6364s 1s/step - loss: 299.3400 - val_loss: 300.3276\n",
      "Epoch 7/10\n",
      "5759/5759 [==============================] - 5246s 911ms/step - loss: 298.7033 - val_loss: 299.8050\n",
      "Epoch 8/10\n",
      "5759/5759 [==============================] - 5084s 883ms/step - loss: 298.2143 - val_loss: 299.3951\n",
      "Epoch 9/10\n",
      "5759/5759 [==============================] - 7377s 1s/step - loss: 297.8223 - val_loss: 299.0667\n",
      "Epoch 10/10\n",
      "5759/5759 [==============================] - 6087s 1s/step - loss: 297.4986 - val_loss: 298.8082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96800d5a20>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_CI, epochs=Nep, validation_data=valid_gen_CI,\\\n",
    "                    callbacks=[earlyStopping, mcp_save_pos, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rec = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [333.05156581512364,\n",
       "  310.6931948111193,\n",
       "  304.51710277887906,\n",
       "  301.70909371684877,\n",
       "  300.2336531922436,\n",
       "  299.34000020094226,\n",
       "  298.7033277413232,\n",
       "  298.2142985748487,\n",
       "  297.8223111576081,\n",
       "  297.49855535381016],\n",
       " 'val_loss': [317.9424894715131,\n",
       "  308.0464616504419,\n",
       "  304.0613429893301,\n",
       "  302.090381744452,\n",
       "  301.0067826105784,\n",
       "  300.32760883262154,\n",
       "  299.8049709262554,\n",
       "  299.39514361780095,\n",
       "  299.06670788960923,\n",
       "  298.80823058409953],\n",
       " 'testP4K_loss': [722.2397246819291,\n",
       "  697.4373292892026,\n",
       "  684.9307260671301,\n",
       "  679.3653844453859,\n",
       "  676.8490013480899,\n",
       "  675.4289473119096,\n",
       "  674.3540836741433,\n",
       "  673.5583135512015,\n",
       "  672.867135846849,\n",
       "  672.3943842652513]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPKL = '/export/home/tbeucler/CBRAIN-CAM/notebooks/tbeucler_devlog/PKL_DATA'\n",
    "\n",
    "hf = open(pathPKL+save_name+'_hist.pkl','wb')\n",
    "\n",
    "F_data = {'hist':hist_rec}\n",
    "\n",
    "pickle.dump(F_data,hf)\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "579.521px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
